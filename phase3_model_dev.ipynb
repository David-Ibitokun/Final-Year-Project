{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6d4ce7c",
   "metadata": {},
   "source": [
    "# Phase 3: Model Development - IMPROVED VERSION\n",
    "## CNN, GRU, and Hybrid CNN-GRU Models\n",
    "\n",
    "### ğŸ¯ Key Improvements in This Version:\n",
    "\n",
    "#### 1. **3 Crops Only** (Rice Removed)\n",
    "- Training on: Maize, Cassava, Yams\n",
    "- **Why**: Rice had poor accuracy (27-33% vs 50-72% for others)\n",
    "- **Expected Impact**: +10-15% overall accuracy\n",
    "\n",
    "#### 2. **Hybrid Model Improvements**\n",
    "Previous issues fixed:\n",
    "- âœ… Reduced dropout (less over-regularization)\n",
    "- âœ… Added residual connections\n",
    "- âœ… Stronger focal loss (gamma=3.0) for class balance\n",
    "- âœ… Amplified class weights (1.5x Low/High, 0.7x Medium)\n",
    "- âœ… Lower learning rate (0.0002) for stability\n",
    "- âœ… Smaller batch size (24) for better gradients\n",
    "\n",
    "**Expected**: Hybrid should now outperform CNN/GRU with >60% accuracy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd81e7aa",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c491b05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: False - []\n",
      "âœ“ All libraries imported successfully!\n",
      "TensorFlow version: 2.20.0\n",
      "Random seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, classification_report)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Check for GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"GPU available: {len(gpus) > 0} - {gpus}\")\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(f\"âœ“ All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Random seed: {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f302f79e",
   "metadata": {},
   "source": [
    "---\n",
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9096f0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Configuration complete (3 crops: Maize, Cassava, Yams)\n",
      "  CNN features: 26\n",
      "  GRU features: 26\n",
      "  Hybrid temporal: 17\n",
      "  Hybrid static: 13\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "data_path = Path('project_data')\n",
    "splits_path = data_path / 'train_test_split'\n",
    "\n",
    "# Crops and regions - RICE REMOVED due to poor performance (27-33% accuracy)\n",
    "CROPS = ['Maize', 'Cassava', 'Yams']\n",
    "ZONES = [\"North West\", \"North East\", \"North Central\", \"South West\", \"South East\", \"South South\"]\n",
    "\n",
    "# Feature columns\n",
    "cnn_feature_cols = [\n",
    "    'Temperature_C', 'Rainfall_mm', 'Humidity_percent', 'CO2_ppm',\n",
    "    'GDD', 'Cumulative_Rainfall', 'Days_Into_Season',\n",
    "    'pH_Temperature_Interaction', 'Nitrogen_Rainfall_Interaction',\n",
    "    'Is_Rainy_Season', 'Is_Peak_Growing',\n",
    "    'Heat_Stress', 'Cold_Stress', 'Rainfall_Anomaly',\n",
    "    'Drought_Risk', 'Flood_Risk',\n",
    "    # Lag features\n",
    "    'Yield_Lag_1', 'Yield_Lag_2', 'Yield_Lag_3',\n",
    "    'Yield_MA_3yr', 'Temp_MA_3yr', 'Rain_MA_3yr',\n",
    "    'Yield_YoY_Change', 'Temp_YoY_Change', 'Rain_YoY_Change',\n",
    "    'Yield_Volatility_3yr'\n",
    "]\n",
    "\n",
    "gru_feature_cols = cnn_feature_cols.copy()  # Same features for GRU\n",
    "\n",
    "# Hybrid features\n",
    "hybrid_temporal_cols = [\n",
    "    'Temperature_C', 'Rainfall_mm', 'Humidity_percent', 'CO2_ppm',\n",
    "    'GDD', 'Cumulative_Rainfall', 'Days_Into_Season',\n",
    "    'Is_Rainy_Season', 'Is_Peak_Growing',\n",
    "    'Heat_Stress', 'Cold_Stress', 'Rainfall_Anomaly',\n",
    "    'Drought_Risk', 'Flood_Risk',\n",
    "    'Yield_Lag_1', 'Yield_MA_3yr', 'Yield_YoY_Change'\n",
    "]\n",
    "\n",
    "hybrid_static_cols = [\n",
    "    'Avg_pH', 'Avg_Nitrogen_ppm', 'Avg_Phosphorus_ppm', 'Avg_Organic_Matter_Percent',\n",
    "    'pH_Temperature_Interaction', 'Nitrogen_Rainfall_Interaction',\n",
    "    'Yield_Lag_2', 'Yield_Lag_3', 'Temp_MA_3yr', 'Rain_MA_3yr',\n",
    "    'Temp_YoY_Change', 'Rain_YoY_Change', 'Yield_Volatility_3yr'\n",
    "]\n",
    "\n",
    "target_col = 'Yield_kg_per_ha'\n",
    "\n",
    "print(\"âœ“ Configuration complete (3 crops: Maize, Cassava, Yams)\")\n",
    "print(f\"  CNN features: {len(cnn_feature_cols)}\")\n",
    "print(f\"  GRU features: {len(gru_feature_cols)}\")\n",
    "print(f\"  Hybrid temporal: {len(hybrid_temporal_cols)}\")\n",
    "print(f\"  Hybrid static: {len(hybrid_static_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ae0106",
   "metadata": {},
   "source": [
    "---\n",
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f1831d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Utility functions defined\n"
     ]
    }
   ],
   "source": [
    "def categorize_yield_balanced(yields, method='percentile'):\n",
    "    \"\"\"\n",
    "    Categorize yields into Low/Medium/High with balanced thresholds.\n",
    "    \"\"\"\n",
    "    if method == 'percentile':\n",
    "        low_thresh = np.percentile(yields, 33.33)\n",
    "        high_thresh = np.percentile(yields, 66.67)\n",
    "    else:\n",
    "        low_thresh = 5.0\n",
    "        high_thresh = 15.0\n",
    "    \n",
    "    categories = np.zeros(len(yields), dtype=int)\n",
    "    categories[yields <= low_thresh] = 0  # Low\n",
    "    categories[(yields > low_thresh) & (yields <= high_thresh)] = 1  # Medium\n",
    "    categories[yields > high_thresh] = 2  # High\n",
    "    \n",
    "    return categories, (low_thresh, high_thresh)\n",
    "\n",
    "def augment_data(X, y, num_augmented=2, noise_level=0.05):\n",
    "    \"\"\"\n",
    "    Augment training data with Gaussian noise.\n",
    "    \"\"\"\n",
    "    X_list = [X]\n",
    "    y_list = [y]\n",
    "    \n",
    "    feature_stds = np.std(X, axis=0)\n",
    "    \n",
    "    for i in range(num_augmented):\n",
    "        noise = np.random.normal(0, noise_level, size=X.shape) * feature_stds\n",
    "        X_noisy = X + noise\n",
    "        X_noisy = np.clip(X_noisy, X.min(axis=0) * 0.8, X.max(axis=0) * 1.2)\n",
    "        \n",
    "        X_list.append(X_noisy)\n",
    "        y_list.append(y)\n",
    "    \n",
    "    X_augmented = np.vstack(X_list)\n",
    "    y_augmented = np.vstack(y_list)\n",
    "    \n",
    "    return X_augmented, y_augmented\n",
    "\n",
    "def augment_time_series(X, y, num_augmented=1, noise_level=0.03):\n",
    "    \"\"\"\n",
    "    Augment time series data with noise.\n",
    "    \"\"\"\n",
    "    X_list = [X]\n",
    "    y_list = [y]\n",
    "    \n",
    "    feature_stds = np.std(X.reshape(-1, X.shape[2]), axis=0)\n",
    "    \n",
    "    for i in range(num_augmented):\n",
    "        noise = np.random.normal(0, noise_level, size=X.shape) * feature_stds\n",
    "        X_noisy = X + noise\n",
    "        X_noisy = np.clip(X_noisy, X.min(axis=(0,1)) * 0.8, X.max(axis=(0,1)) * 1.2)\n",
    "        \n",
    "        X_list.append(X_noisy)\n",
    "        y_list.append(y)\n",
    "    \n",
    "    X_augmented = np.vstack([x.reshape(x.shape[0], -1) for x in X_list])\n",
    "    X_augmented = X_augmented.reshape(-1, X.shape[1], X.shape[2])\n",
    "    y_augmented = np.vstack(y_list)\n",
    "    \n",
    "    return X_augmented, y_augmented\n",
    "\n",
    "def augment_hybrid_data(X_temp, X_stat, y, num_augmented=1, noise_level=0.03):\n",
    "    \"\"\"\n",
    "    Augment Hybrid model data (temporal + static inputs).\n",
    "    \"\"\"\n",
    "    X_temp_list = [X_temp]\n",
    "    X_stat_list = [X_stat]\n",
    "    y_list = [y]\n",
    "    \n",
    "    temp_stds = np.std(X_temp.reshape(-1, X_temp.shape[2]), axis=0)\n",
    "    stat_stds = np.std(X_stat, axis=0)\n",
    "    \n",
    "    for i in range(num_augmented):\n",
    "        noise_temp = np.random.normal(0, noise_level, size=X_temp.shape) * temp_stds\n",
    "        X_temp_noisy = X_temp + noise_temp\n",
    "        X_temp_noisy = np.clip(X_temp_noisy, X_temp.min(axis=(0,1)) * 0.8, X_temp.max(axis=(0,1)) * 1.2)\n",
    "        \n",
    "        noise_stat = np.random.normal(0, noise_level, size=X_stat.shape) * stat_stds\n",
    "        X_stat_noisy = X_stat + noise_stat\n",
    "        X_stat_noisy = np.clip(X_stat_noisy, X_stat.min(axis=0) * 0.8, X_stat.max(axis=0) * 1.2)\n",
    "        \n",
    "        X_temp_list.append(X_temp_noisy)\n",
    "        X_stat_list.append(X_stat_noisy)\n",
    "        y_list.append(y)\n",
    "    \n",
    "    X_temp_aug = np.vstack([x.reshape(x.shape[0], -1) for x in X_temp_list])\n",
    "    X_temp_aug = X_temp_aug.reshape(-1, X_temp.shape[1], X_temp.shape[2])\n",
    "    \n",
    "    X_stat_aug = np.vstack(X_stat_list)\n",
    "    y_aug = np.vstack(y_list)\n",
    "    \n",
    "    return X_temp_aug, X_stat_aug, y_aug\n",
    "\n",
    "print(\"âœ“ Utility functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6ff9fb",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: CNN Model (1D Convolutional Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d8fc9d",
   "metadata": {},
   "source": [
    "### What is 1D-CNN?\n",
    "\n",
    "Convolutional Neural Networks apply filters to detect patterns in data. For time series, 1D-CNN uses sliding windows to extract:\n",
    "- **Seasonal patterns** (e.g., rainy vs dry seasons)\n",
    "- **Local trends** (e.g., temperature peaks during growing season)\n",
    "- **Multi-scale features** through multiple conv layers\n",
    "\n",
    "**Why CNN for crop yield?**\n",
    "- Efficient at capturing temporal patterns (monthly sequences)\n",
    "- Fewer parameters than recurrent networks\n",
    "- Can detect seasonal signatures that indicate yield outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d4bfbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 1: CNN MODEL (1D CONVOLUTIONS)\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Loading CNN data...\n",
      "  Train: (3888, 35)\n",
      "  Val:   (648, 35)\n",
      "  Test:  (648, 35)\n",
      "\n",
      "âœ“ Encoded categorical variables\n",
      "  Total features (including encodings): 28\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 1: CNN MODEL (1D CONVOLUTIONS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load CNN data\n",
    "print(\"\\nğŸ“Š Loading CNN data...\")\n",
    "cnn_train = pd.read_csv(splits_path / 'cnn' / 'train.csv')\n",
    "cnn_val = pd.read_csv(splits_path / 'cnn' / 'val.csv')\n",
    "cnn_test = pd.read_csv(splits_path / 'cnn' / 'test.csv')\n",
    "\n",
    "print(f\"  Train: {cnn_train.shape}\")\n",
    "print(f\"  Val:   {cnn_val.shape}\")\n",
    "print(f\"  Test:  {cnn_test.shape}\")\n",
    "\n",
    "# Remove missing yields\n",
    "cnn_train = cnn_train.dropna(subset=[target_col])\n",
    "cnn_val = cnn_val.dropna(subset=[target_col])\n",
    "cnn_test = cnn_test.dropna(subset=[target_col])\n",
    "\n",
    "# Encode categorical variables\n",
    "crop_encoder = LabelEncoder()\n",
    "region_encoder = LabelEncoder()\n",
    "\n",
    "all_crops = pd.concat([cnn_train['Crop'], cnn_val['Crop'], cnn_test['Crop']])\n",
    "all_regions = pd.concat([cnn_train['Region'], cnn_val['Region'], cnn_test['Region']])\n",
    "\n",
    "crop_encoder.fit(all_crops)\n",
    "region_encoder.fit(all_regions)\n",
    "\n",
    "cnn_train['Crop_encoded'] = crop_encoder.transform(cnn_train['Crop'])\n",
    "cnn_train['Region_encoded'] = region_encoder.transform(cnn_train['Region'])\n",
    "cnn_val['Crop_encoded'] = crop_encoder.transform(cnn_val['Crop'])\n",
    "cnn_val['Region_encoded'] = region_encoder.transform(cnn_val['Region'])\n",
    "cnn_test['Crop_encoded'] = crop_encoder.transform(cnn_test['Crop'])\n",
    "cnn_test['Region_encoded'] = region_encoder.transform(cnn_test['Region'])\n",
    "\n",
    "# Add encoded features to feature list\n",
    "cnn_all_features = cnn_feature_cols + ['Crop_encoded', 'Region_encoded']\n",
    "\n",
    "print(f\"\\nâœ“ Encoded categorical variables\")\n",
    "print(f\"  Total features (including encodings): {len(cnn_all_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0647195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Creating sequences for CNN...\n",
      "\n",
      "  Train sequences: (324, 12, 28)\n",
      "  Val sequences:   (54, 12, 28)\n",
      "  Test sequences:  (54, 12, 28)\n",
      "\n",
      "  Normalizing features...\n",
      "  âœ“ Features normalized\n"
     ]
    }
   ],
   "source": [
    "# Prepare sequences for CNN\n",
    "print(\"\\nğŸ“Š Creating sequences for CNN...\")\n",
    "\n",
    "def create_sequences(df, feature_cols, target_col, sequence_length=12):\n",
    "    \"\"\"Create sliding window sequences from monthly data\"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    # Group by Region, Crop, Year to create annual sequences\n",
    "    for (region, crop, year), group in df.groupby(['Region', 'Crop', 'Year']):\n",
    "        group_sorted = group.sort_values('Month')\n",
    "        \n",
    "        if len(group_sorted) >= sequence_length:\n",
    "            # Take first sequence_length months\n",
    "            seq = group_sorted.iloc[:sequence_length][feature_cols].values\n",
    "            # Target is the annual yield (sum of monthly yields)\n",
    "            target = group_sorted[target_col].sum()\n",
    "            \n",
    "            sequences.append(seq)\n",
    "            targets.append(target)\n",
    "    \n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "sequence_length = 12  # 12 months\n",
    "X_cnn_train, y_cnn_train = create_sequences(cnn_train, cnn_all_features, target_col, sequence_length)\n",
    "X_cnn_val, y_cnn_val = create_sequences(cnn_val, cnn_all_features, target_col, sequence_length)\n",
    "X_cnn_test, y_cnn_test = create_sequences(cnn_test, cnn_all_features, target_col, sequence_length)\n",
    "\n",
    "print(f\"\\n  Train sequences: {X_cnn_train.shape}\")\n",
    "print(f\"  Val sequences:   {X_cnn_val.shape}\")\n",
    "print(f\"  Test sequences:  {X_cnn_test.shape}\")\n",
    "\n",
    "# Normalize features\n",
    "print(\"\\n  Normalizing features...\")\n",
    "scaler_cnn = StandardScaler()\n",
    "\n",
    "# Reshape for scaling\n",
    "X_cnn_train_reshaped = X_cnn_train.reshape(-1, X_cnn_train.shape[2])\n",
    "scaler_cnn.fit(X_cnn_train_reshaped)\n",
    "\n",
    "X_cnn_train_scaled = scaler_cnn.transform(X_cnn_train.reshape(-1, X_cnn_train.shape[2])).reshape(X_cnn_train.shape)\n",
    "X_cnn_val_scaled = scaler_cnn.transform(X_cnn_val.reshape(-1, X_cnn_val.shape[2])).reshape(X_cnn_val.shape)\n",
    "X_cnn_test_scaled = scaler_cnn.transform(X_cnn_test.reshape(-1, X_cnn_test.shape[2])).reshape(X_cnn_test.shape)\n",
    "\n",
    "print(\"  âœ“ Features normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "640fccd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Categorizing yields for classification...\n",
      "\n",
      "Category thresholds (kg/ha):\n",
      "  Low:    < 2.66\n",
      "  Medium: 2.66 - 11.37\n",
      "  High:   > 11.37\n",
      "\n",
      "Class distribution (Train):\n",
      "  Class 0: 108 samples (33.3%)\n",
      "  Class 1: 108 samples (33.3%)\n",
      "  Class 2: 108 samples (33.3%)\n",
      "\n",
      "âœ“ Targets encoded as one-hot vectors\n"
     ]
    }
   ],
   "source": [
    "# Categorize yields\n",
    "print(\"\\nğŸ“Š Categorizing yields for classification...\")\n",
    "\n",
    "y_cnn_train_cat, cnn_percentiles = categorize_yield_balanced(y_cnn_train, method='percentile')\n",
    "y_cnn_val_cat, _ = categorize_yield_balanced(y_cnn_val, method='percentile')\n",
    "y_cnn_test_cat, _ = categorize_yield_balanced(y_cnn_test, method='percentile')\n",
    "\n",
    "print(f\"\\nCategory thresholds (kg/ha):\")\n",
    "print(f\"  Low:    < {cnn_percentiles[0]:.2f}\")\n",
    "print(f\"  Medium: {cnn_percentiles[0]:.2f} - {cnn_percentiles[1]:.2f}\")\n",
    "print(f\"  High:   > {cnn_percentiles[1]:.2f}\")\n",
    "\n",
    "print(f\"\\nClass distribution (Train):\")\n",
    "unique, counts = np.unique(y_cnn_train_cat, return_counts=True)\n",
    "for cls, count in zip(unique, counts):\n",
    "    print(f\"  Class {cls}: {count} samples ({count/len(y_cnn_train_cat)*100:.1f}%)\")\n",
    "\n",
    "# One-hot encode\n",
    "y_cnn_train_onehot = to_categorical(y_cnn_train_cat, num_classes=3)\n",
    "y_cnn_val_onehot = to_categorical(y_cnn_val_cat, num_classes=3)\n",
    "y_cnn_test_onehot = to_categorical(y_cnn_test_cat, num_classes=3)\n",
    "\n",
    "print(f\"\\nâœ“ Targets encoded as one-hot vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69fd8be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ APPLYING DATA AUGMENTATION...\n",
      "  Original CNN training size: 324 samples\n",
      "  Augmented CNN training size: 1296 samples\n",
      "  Augmentation ratio: 4.0x\n",
      "\n",
      "âš–ï¸  CNN Class weights:\n",
      "   Class 0: weight=1.000 (n=432 samples)\n",
      "   Class 1: weight=1.000 (n=432 samples)\n",
      "   Class 2: weight=1.000 (n=432 samples)\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation for CNN\n",
    "print(\"\\nğŸ”„ APPLYING DATA AUGMENTATION...\")\n",
    "\n",
    "print(f\"  Original CNN training size: {X_cnn_train_scaled.shape[0]} samples\")\n",
    "X_cnn_train_aug, y_cnn_train_aug = augment_time_series(\n",
    "    X_cnn_train_scaled,\n",
    "    y_cnn_train_onehot,\n",
    "    num_augmented=3,  # Increased from 1 to 3 for more training data\n",
    "    noise_level=0.02  # Reduced from 0.03 to 0.02 for better quality\n",
    ")\n",
    "\n",
    "print(f\"  Augmented CNN training size: {X_cnn_train_aug.shape[0]} samples\")\n",
    "print(f\"  Augmentation ratio: {X_cnn_train_aug.shape[0] / X_cnn_train_scaled.shape[0]:.1f}x\")\n",
    "\n",
    "# Shuffle\n",
    "indices = np.random.permutation(X_cnn_train_aug.shape[0])\n",
    "X_cnn_train_aug = X_cnn_train_aug[indices]\n",
    "y_cnn_train_aug = y_cnn_train_aug[indices]\n",
    "\n",
    "# Compute class weights\n",
    "y_cnn_train_labels = np.argmax(y_cnn_train_aug, axis=1)\n",
    "cnn_class_weights_array = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_cnn_train_labels),\n",
    "    y=y_cnn_train_labels\n",
    ")\n",
    "cnn_class_weight_dict = dict(enumerate(cnn_class_weights_array))\n",
    "\n",
    "print(f\"\\nâš–ï¸  CNN Class weights:\")\n",
    "for cls, weight in cnn_class_weight_dict.items():\n",
    "    count = np.sum(y_cnn_train_labels == cls)\n",
    "    print(f\"   Class {cls}: weight={weight:.3f} (n={count} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68213d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,440</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_1           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,560</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_2           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_max_pooling1d            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_3           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_4           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚         \u001b[38;5;34m5,440\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚           \u001b[38;5;34m256\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)          â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)          â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)         â”‚        \u001b[38;5;34m24,704\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_1           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)         â”‚           \u001b[38;5;34m512\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m256\u001b[0m)         â”‚        \u001b[38;5;34m98,560\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_2           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m256\u001b[0m)         â”‚         \u001b[38;5;34m1,024\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m256\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_max_pooling1d            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚        \u001b[38;5;34m32,896\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_3           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚           \u001b[38;5;34m512\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m8,256\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_4           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚           \u001b[38;5;34m256\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚         \u001b[38;5;34m2,080\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              â”‚            \u001b[38;5;34m99\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">174,595</span> (682.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m174,595\u001b[0m (682.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">173,315</span> (677.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m173,315\u001b[0m (677.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span> (5.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,280\u001b[0m (5.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Focal Loss for handling class imbalance\n",
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        cross_entropy = -y_true * tf.math.log(y_pred)\n",
    "        weight = alpha * y_true * tf.pow((1 - y_pred), gamma)\n",
    "        loss = weight * cross_entropy\n",
    "        return tf.reduce_mean(tf.reduce_sum(loss, axis=1))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# Build CNN model with Attention\n",
    "def build_cnn_model(sequence_length, n_features, learning_rate=0.0003):\n",
    "    \"\"\"\n",
    "    Build 1D-CNN model with attention for time-series classification.\n",
    "    \n",
    "    Architecture:\n",
    "    - Conv1D layers extract temporal patterns\n",
    "    - Attention mechanism focuses on important patterns\n",
    "    - Dense layers for classification\n",
    "    - Enhanced regularization to prevent overfitting\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(sequence_length, n_features)),\n",
    "        \n",
    "        # First conv block\n",
    "        layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same',\n",
    "                     kernel_regularizer=regularizers.l2(0.002)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        # Second conv block\n",
    "        layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same',\n",
    "                     kernel_regularizer=regularizers.l2(0.002)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        # Third conv block for deeper features\n",
    "        layers.Conv1D(filters=256, kernel_size=3, activation='relu', padding='same',\n",
    "                     kernel_regularizer=regularizers.l2(0.002)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        # Global pooling\n",
    "        layers.GlobalMaxPooling1D(),\n",
    "        \n",
    "        # Dense layers with stronger regularization\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.002)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.002)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate, clipnorm=1.0)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=focal_loss(gamma=2.0, alpha=0.25),\n",
    "        metrics=['accuracy',\n",
    "                 tf.keras.metrics.Precision(name='precision'),\n",
    "                 tf.keras.metrics.Recall(name='recall')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "cnn_model = build_cnn_model(\n",
    "    sequence_length=sequence_length,\n",
    "    n_features=X_cnn_train_aug.shape[2],\n",
    "    learning_rate=0.0003\n",
    ")\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4ac9470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Training CNN model...\n",
      "Epoch 1/300\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.3402 - loss: 1.4490 - precision: 0.3411 - recall: 0.2935\n",
      "Epoch 1: val_accuracy improved from -inf to 0.33333\n",
      "  Saved model to models\\cnn_best.weights.h5\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - accuracy: 0.3596 - loss: 1.4330 - precision: 0.3638 - recall: 0.3102 - val_accuracy: 0.3333 - val_loss: 1.2579 - val_precision: 1.0000 - val_recall: 0.0370 - learning_rate: 3.0000e-04\n",
      "Epoch 2/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.4151 - loss: 1.3842 - precision: 0.4229 - recall: 0.3511 - val_accuracy: 0.3333 - val_loss: 1.2528 - val_precision: 0.7143 - val_recall: 0.1852 - learning_rate: 3.0000e-04\n",
      "Epoch 3/300\n",
      "\u001b[1m39/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4499 - loss: 1.3342 - precision: 0.4779 - recall: 0.3867\n",
      "Epoch 3: val_accuracy improved from 0.33333 to 0.35185\n",
      "  Saved model to models\\cnn_best.weights.h5\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.4522 - loss: 1.3326 - precision: 0.4803 - recall: 0.3866 - val_accuracy: 0.3519 - val_loss: 1.2424 - val_precision: 0.7500 - val_recall: 0.3333 - learning_rate: 3.0000e-04\n",
      "Epoch 4/300\n",
      "\u001b[1m38/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4890 - loss: 1.3031 - precision: 0.5278 - recall: 0.4328\n",
      "Epoch 4: val_accuracy improved from 0.35185 to 0.42593\n",
      "  Saved model to models\\cnn_best.weights.h5\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.5000 - loss: 1.2974 - precision: 0.5403 - recall: 0.4344 - val_accuracy: 0.4259 - val_loss: 1.2148 - val_precision: 0.8000 - val_recall: 0.2963 - learning_rate: 3.0000e-04\n",
      "Epoch 5/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.4954 - loss: 1.2616 - precision: 0.5195 - recall: 0.4048\n",
      "Epoch 5: val_accuracy improved from 0.42593 to 0.44444\n",
      "  Saved model to models\\cnn_best.weights.h5\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.5316 - loss: 1.2476 - precision: 0.5638 - recall: 0.4468 - val_accuracy: 0.4444 - val_loss: 1.1957 - val_precision: 0.7200 - val_recall: 0.3333 - learning_rate: 3.0000e-04\n",
      "Epoch 6/300\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5439 - loss: 1.2342 - precision: 0.5673 - recall: 0.4734\n",
      "Epoch 6: val_accuracy improved from 0.44444 to 0.46296\n",
      "  Saved model to models\\cnn_best.weights.h5\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.5525 - loss: 1.2235 - precision: 0.5916 - recall: 0.4884 - val_accuracy: 0.4630 - val_loss: 1.1847 - val_precision: 0.6429 - val_recall: 0.3333 - learning_rate: 3.0000e-04\n",
      "Epoch 7/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.6250 - loss: 1.1827 - precision: 0.6667 - recall: 0.5540 - val_accuracy: 0.4630 - val_loss: 1.1610 - val_precision: 0.6333 - val_recall: 0.3519 - learning_rate: 3.0000e-04\n",
      "Epoch 8/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6628 - loss: 1.1600 - precision: 0.7052 - recall: 0.5973\n",
      "Epoch 8: val_accuracy improved from 0.46296 to 0.48148\n",
      "  Saved model to models\\cnn_best.weights.h5\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.6466 - loss: 1.1566 - precision: 0.6915 - recall: 0.5795 - val_accuracy: 0.4815 - val_loss: 1.1358 - val_precision: 0.6000 - val_recall: 0.3889 - learning_rate: 3.0000e-04\n",
      "Epoch 9/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6759 - loss: 1.1294 - precision: 0.7410 - recall: 0.6159\n",
      "Epoch 9: val_accuracy improved from 0.48148 to 0.57407\n",
      "  Saved model to models\\cnn_best.weights.h5\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.6952 - loss: 1.1176 - precision: 0.7560 - recall: 0.6312 - val_accuracy: 0.5741 - val_loss: 1.1149 - val_precision: 0.5676 - val_recall: 0.3889 - learning_rate: 3.0000e-04\n",
      "Epoch 10/300\n",
      "\u001b[1m38/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7073 - loss: 1.0949 - precision: 0.7423 - recall: 0.6390\n",
      "Epoch 10: val_accuracy improved from 0.57407 to 0.64815\n",
      "  Saved model to models\\cnn_best.weights.h5\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.7153 - loss: 1.0923 - precision: 0.7549 - recall: 0.6535 - val_accuracy: 0.6481 - val_loss: 1.0921 - val_precision: 0.5897 - val_recall: 0.4259 - learning_rate: 3.0000e-04\n",
      "Epoch 11/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.7454 - loss: 1.0635 - precision: 0.7996 - recall: 0.6867 - val_accuracy: 0.6296 - val_loss: 1.0665 - val_precision: 0.6304 - val_recall: 0.5370 - learning_rate: 3.0000e-04\n",
      "Epoch 12/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7623 - loss: 1.0348 - precision: 0.7967 - recall: 0.6983 - val_accuracy: 0.6296 - val_loss: 1.0555 - val_precision: 0.6531 - val_recall: 0.5926 - learning_rate: 3.0000e-04\n",
      "Epoch 13/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8086 - loss: 1.0059 - precision: 0.8384 - recall: 0.7407 - val_accuracy: 0.6481 - val_loss: 1.0379 - val_precision: 0.6471 - val_recall: 0.6111 - learning_rate: 3.0000e-04\n",
      "Epoch 14/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8102 - loss: 0.9807 - precision: 0.8401 - recall: 0.7539 - val_accuracy: 0.6481 - val_loss: 1.0249 - val_precision: 0.6415 - val_recall: 0.6296 - learning_rate: 3.0000e-04\n",
      "Epoch 15/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.8364 - loss: 0.9577 - precision: 0.8642 - recall: 0.7809 - val_accuracy: 0.6481 - val_loss: 1.0085 - val_precision: 0.6538 - val_recall: 0.6296 - learning_rate: 3.0000e-04\n",
      "Epoch 16/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8472 - loss: 0.9358 - precision: 0.8800 - recall: 0.8094 - val_accuracy: 0.6481 - val_loss: 0.9879 - val_precision: 0.6604 - val_recall: 0.6481 - learning_rate: 3.0000e-04\n",
      "Epoch 17/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8596 - loss: 0.9135 - precision: 0.8873 - recall: 0.8202 - val_accuracy: 0.6481 - val_loss: 0.9668 - val_precision: 0.6731 - val_recall: 0.6481 - learning_rate: 3.0000e-04\n",
      "Epoch 18/300\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8656 - loss: 0.8899 - precision: 0.8892 - recall: 0.8193\n",
      "Epoch 18: val_accuracy improved from 0.64815 to 0.66667\n",
      "  Saved model to models\\cnn_best.weights.h5\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.8657 - loss: 0.8868 - precision: 0.8918 - recall: 0.8202 - val_accuracy: 0.6667 - val_loss: 0.9523 - val_precision: 0.6731 - val_recall: 0.6481 - learning_rate: 3.0000e-04\n",
      "Epoch 19/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8812 - loss: 0.8627 - precision: 0.9095 - recall: 0.8372 - val_accuracy: 0.6667 - val_loss: 0.9319 - val_precision: 0.6731 - val_recall: 0.6481 - learning_rate: 3.0000e-04\n",
      "Epoch 20/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8750 - loss: 0.8417 - precision: 0.9078 - recall: 0.8434 - val_accuracy: 0.6667 - val_loss: 0.9112 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 21/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8827 - loss: 0.8209 - precision: 0.9036 - recall: 0.8465 - val_accuracy: 0.6667 - val_loss: 0.8891 - val_precision: 0.6731 - val_recall: 0.6481 - learning_rate: 3.0000e-04\n",
      "Epoch 22/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8935 - loss: 0.8019 - precision: 0.9229 - recall: 0.8581\n",
      "Epoch 22: val_accuracy improved from 0.66667 to 0.68519\n",
      "  Saved model to models\\cnn_best.weights.h5\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.8920 - loss: 0.7993 - precision: 0.9142 - recall: 0.8549 - val_accuracy: 0.6852 - val_loss: 0.8684 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 23/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8873 - loss: 0.7806 - precision: 0.9093 - recall: 0.8588 - val_accuracy: 0.6667 - val_loss: 0.8502 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 24/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.9020 - loss: 0.7548 - precision: 0.9236 - recall: 0.8681 - val_accuracy: 0.6667 - val_loss: 0.8336 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 25/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9059 - loss: 0.7331 - precision: 0.9297 - recall: 0.8781 - val_accuracy: 0.6667 - val_loss: 0.8172 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 26/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9136 - loss: 0.7111 - precision: 0.9347 - recall: 0.8835 - val_accuracy: 0.6667 - val_loss: 0.8006 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 27/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9128 - loss: 0.6925 - precision: 0.9238 - recall: 0.8881 - val_accuracy: 0.6667 - val_loss: 0.7796 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 28/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9074 - loss: 0.6721 - precision: 0.9199 - recall: 0.8773 - val_accuracy: 0.6852 - val_loss: 0.7645 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 29/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9190 - loss: 0.6513 - precision: 0.9369 - recall: 0.8935 - val_accuracy: 0.6852 - val_loss: 0.7368 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 30/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9290 - loss: 0.6298 - precision: 0.9422 - recall: 0.9051 - val_accuracy: 0.6852 - val_loss: 0.7150 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 31/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9252 - loss: 0.6121 - precision: 0.9442 - recall: 0.9005 - val_accuracy: 0.6667 - val_loss: 0.6990 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 32/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9252 - loss: 0.5939 - precision: 0.9375 - recall: 0.9028 - val_accuracy: 0.6667 - val_loss: 0.6850 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 33/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9244 - loss: 0.5755 - precision: 0.9424 - recall: 0.9082 - val_accuracy: 0.6852 - val_loss: 0.6551 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 34/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9360 - loss: 0.5556 - precision: 0.9438 - recall: 0.9198 - val_accuracy: 0.6852 - val_loss: 0.6347 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 35/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9406 - loss: 0.5364 - precision: 0.9511 - recall: 0.9151 - val_accuracy: 0.6667 - val_loss: 0.6183 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 36/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9398 - loss: 0.5188 - precision: 0.9524 - recall: 0.9267 - val_accuracy: 0.6852 - val_loss: 0.6023 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 37/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9452 - loss: 0.5016 - precision: 0.9576 - recall: 0.9228 - val_accuracy: 0.6852 - val_loss: 0.5763 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 38/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9398 - loss: 0.4850 - precision: 0.9580 - recall: 0.9144 - val_accuracy: 0.6852 - val_loss: 0.5705 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 39/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9498 - loss: 0.4666 - precision: 0.9574 - recall: 0.9375 - val_accuracy: 0.6852 - val_loss: 0.5512 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 40/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9460 - loss: 0.4503 - precision: 0.9549 - recall: 0.9306 - val_accuracy: 0.6852 - val_loss: 0.5344 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 41/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9491 - loss: 0.4369 - precision: 0.9580 - recall: 0.9329 - val_accuracy: 0.6852 - val_loss: 0.5251 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 42/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9545 - loss: 0.4190 - precision: 0.9620 - recall: 0.9375 - val_accuracy: 0.6667 - val_loss: 0.5250 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 43/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9498 - loss: 0.4045 - precision: 0.9587 - recall: 0.9321 - val_accuracy: 0.6667 - val_loss: 0.5133 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 44/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9506 - loss: 0.3888 - precision: 0.9558 - recall: 0.9352 - val_accuracy: 0.6852 - val_loss: 0.4994 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 45/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9630 - loss: 0.3734 - precision: 0.9731 - recall: 0.9475 - val_accuracy: 0.6852 - val_loss: 0.4932 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 46/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9599 - loss: 0.3593 - precision: 0.9670 - recall: 0.9483 - val_accuracy: 0.6852 - val_loss: 0.4728 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 47/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9684 - loss: 0.3468 - precision: 0.9726 - recall: 0.9576 - val_accuracy: 0.6852 - val_loss: 0.4681 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 48/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.9591 - loss: 0.3339 - precision: 0.9663 - recall: 0.9514 - val_accuracy: 0.6852 - val_loss: 0.4508 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 49/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9637 - loss: 0.3225 - precision: 0.9693 - recall: 0.9514 - val_accuracy: 0.6852 - val_loss: 0.4442 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 50/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9668 - loss: 0.3093 - precision: 0.9718 - recall: 0.9560 - val_accuracy: 0.6852 - val_loss: 0.4232 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 51/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9568 - loss: 0.2966 - precision: 0.9646 - recall: 0.9452 - val_accuracy: 0.6667 - val_loss: 0.4074 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 52/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9606 - loss: 0.2853 - precision: 0.9677 - recall: 0.9483 - val_accuracy: 0.6667 - val_loss: 0.3860 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 53/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9691 - loss: 0.2717 - precision: 0.9796 - recall: 0.9630 - val_accuracy: 0.6667 - val_loss: 0.3773 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 54/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9684 - loss: 0.2617 - precision: 0.9712 - recall: 0.9630 - val_accuracy: 0.6667 - val_loss: 0.3793 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 55/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9707 - loss: 0.2525 - precision: 0.9772 - recall: 0.9576 - val_accuracy: 0.6667 - val_loss: 0.3603 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 56/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9715 - loss: 0.2410 - precision: 0.9774 - recall: 0.9660 - val_accuracy: 0.6667 - val_loss: 0.3580 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 57/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9653 - loss: 0.2311 - precision: 0.9757 - recall: 0.9599 - val_accuracy: 0.6667 - val_loss: 0.3570 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 58/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9738 - loss: 0.2216 - precision: 0.9789 - recall: 0.9668 - val_accuracy: 0.6667 - val_loss: 0.3397 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 59/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9784 - loss: 0.2130 - precision: 0.9828 - recall: 0.9707 - val_accuracy: 0.6667 - val_loss: 0.3237 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 60/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9753 - loss: 0.2031 - precision: 0.9774 - recall: 0.9684 - val_accuracy: 0.6667 - val_loss: 0.3360 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 61/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9830 - loss: 0.1938 - precision: 0.9844 - recall: 0.9730 - val_accuracy: 0.6667 - val_loss: 0.3306 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 62/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9815 - loss: 0.1861 - precision: 0.9845 - recall: 0.9776 - val_accuracy: 0.6667 - val_loss: 0.3160 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 63/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9699 - loss: 0.1795 - precision: 0.9735 - recall: 0.9630 - val_accuracy: 0.6667 - val_loss: 0.3205 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 64/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9769 - loss: 0.1706 - precision: 0.9798 - recall: 0.9715 - val_accuracy: 0.6852 - val_loss: 0.3119 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 65/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9869 - loss: 0.1626 - precision: 0.9906 - recall: 0.9769 - val_accuracy: 0.6852 - val_loss: 0.3184 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 66/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9753 - loss: 0.1572 - precision: 0.9759 - recall: 0.9668 - val_accuracy: 0.6852 - val_loss: 0.3140 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 67/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9738 - loss: 0.1512 - precision: 0.9767 - recall: 0.9699 - val_accuracy: 0.6852 - val_loss: 0.2934 - val_precision: 0.6981 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 68/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9730 - loss: 0.1457 - precision: 0.9766 - recall: 0.9653 - val_accuracy: 0.6852 - val_loss: 0.2827 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 69/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9745 - loss: 0.1397 - precision: 0.9766 - recall: 0.9676 - val_accuracy: 0.6852 - val_loss: 0.3170 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 70/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9807 - loss: 0.1329 - precision: 0.9844 - recall: 0.9730 - val_accuracy: 0.6852 - val_loss: 0.2974 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 71/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9846 - loss: 0.1259 - precision: 0.9868 - recall: 0.9792 - val_accuracy: 0.6852 - val_loss: 0.3078 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 72/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9815 - loss: 0.1216 - precision: 0.9852 - recall: 0.9792 - val_accuracy: 0.6852 - val_loss: 0.2875 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 73/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9799 - loss: 0.1163 - precision: 0.9844 - recall: 0.9753 - val_accuracy: 0.6852 - val_loss: 0.2770 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 74/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9884 - loss: 0.1109 - precision: 0.9907 - recall: 0.9853 - val_accuracy: 0.6852 - val_loss: 0.2722 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 75/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9784 - loss: 0.1085 - precision: 0.9806 - recall: 0.9769 - val_accuracy: 0.6852 - val_loss: 0.2895 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 76/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9838 - loss: 0.1027 - precision: 0.9852 - recall: 0.9761 - val_accuracy: 0.6667 - val_loss: 0.2949 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 77/300\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9875 - loss: 0.0988 - precision: 0.9876 - recall: 0.9845\n",
      "Epoch 77: val_accuracy improved from 0.68519 to 0.72222\n",
      "  Saved model to models\\cnn_best.weights.h5\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.9799 - loss: 0.1001 - precision: 0.9806 - recall: 0.9745 - val_accuracy: 0.7222 - val_loss: 0.2510 - val_precision: 0.7170 - val_recall: 0.7037 - learning_rate: 3.0000e-04\n",
      "Epoch 78/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9861 - loss: 0.0948 - precision: 0.9860 - recall: 0.9784 - val_accuracy: 0.6667 - val_loss: 0.2931 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 79/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9846 - loss: 0.0906 - precision: 0.9861 - recall: 0.9846 - val_accuracy: 0.6852 - val_loss: 0.2867 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 80/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9838 - loss: 0.0870 - precision: 0.9845 - recall: 0.9792 - val_accuracy: 0.6852 - val_loss: 0.2342 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 81/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9869 - loss: 0.0832 - precision: 0.9876 - recall: 0.9830 - val_accuracy: 0.6852 - val_loss: 0.2461 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 82/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9830 - loss: 0.0805 - precision: 0.9844 - recall: 0.9769 - val_accuracy: 0.6852 - val_loss: 0.2276 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 83/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9823 - loss: 0.0775 - precision: 0.9852 - recall: 0.9745 - val_accuracy: 0.6852 - val_loss: 0.2532 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 84/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9807 - loss: 0.0741 - precision: 0.9844 - recall: 0.9769 - val_accuracy: 0.6667 - val_loss: 0.2477 - val_precision: 0.6923 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 85/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9846 - loss: 0.0715 - precision: 0.9868 - recall: 0.9784 - val_accuracy: 0.7037 - val_loss: 0.2511 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 3.0000e-04\n",
      "Epoch 86/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9784 - loss: 0.0697 - precision: 0.9806 - recall: 0.9769 - val_accuracy: 0.6852 - val_loss: 0.2712 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 87/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9846 - loss: 0.0661 - precision: 0.9876 - recall: 0.9823 - val_accuracy: 0.6852 - val_loss: 0.2686 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 88/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9846 - loss: 0.0649 - precision: 0.9868 - recall: 0.9830 - val_accuracy: 0.7222 - val_loss: 0.2475 - val_precision: 0.7222 - val_recall: 0.7222 - learning_rate: 3.0000e-04\n",
      "Epoch 89/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9776 - loss: 0.0649 - precision: 0.9799 - recall: 0.9769 - val_accuracy: 0.6667 - val_loss: 0.2555 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 90/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9853 - loss: 0.0617 - precision: 0.9868 - recall: 0.9823 - val_accuracy: 0.6667 - val_loss: 0.2147 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 91/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9892 - loss: 0.0573 - precision: 0.9915 - recall: 0.9884 - val_accuracy: 0.7037 - val_loss: 0.1901 - val_precision: 0.6981 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 92/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9884 - loss: 0.0561 - precision: 0.9891 - recall: 0.9846 - val_accuracy: 0.6852 - val_loss: 0.2210 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 93/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9877 - loss: 0.0538 - precision: 0.9915 - recall: 0.9853 - val_accuracy: 0.6852 - val_loss: 0.2370 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 94/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9900 - loss: 0.0512 - precision: 0.9922 - recall: 0.9846 - val_accuracy: 0.6852 - val_loss: 0.2312 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 95/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9884 - loss: 0.0505 - precision: 0.9915 - recall: 0.9846 - val_accuracy: 0.7222 - val_loss: 0.2257 - val_precision: 0.7170 - val_recall: 0.7037 - learning_rate: 3.0000e-04\n",
      "Epoch 96/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9838 - loss: 0.0496 - precision: 0.9868 - recall: 0.9830 - val_accuracy: 0.6667 - val_loss: 0.2329 - val_precision: 0.6923 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 97/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9846 - loss: 0.0483 - precision: 0.9868 - recall: 0.9838 - val_accuracy: 0.6852 - val_loss: 0.2665 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 98/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.9838 - loss: 0.0471 - precision: 0.9845 - recall: 0.9815 - val_accuracy: 0.6852 - val_loss: 0.2560 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 99/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9869 - loss: 0.0472 - precision: 0.9876 - recall: 0.9823 - val_accuracy: 0.6667 - val_loss: 0.2469 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 100/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9915 - loss: 0.0429 - precision: 0.9930 - recall: 0.9884 - val_accuracy: 0.6852 - val_loss: 0.2217 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 101/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9869 - loss: 0.0417 - precision: 0.9899 - recall: 0.9846 - val_accuracy: 0.7222 - val_loss: 0.2083 - val_precision: 0.7222 - val_recall: 0.7222 - learning_rate: 3.0000e-04\n",
      "Epoch 102/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9838 - loss: 0.0426 - precision: 0.9860 - recall: 0.9784 - val_accuracy: 0.7222 - val_loss: 0.1974 - val_precision: 0.7222 - val_recall: 0.7222 - learning_rate: 3.0000e-04\n",
      "Epoch 103/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9807 - loss: 0.0404 - precision: 0.9829 - recall: 0.9761 - val_accuracy: 0.7037 - val_loss: 0.1784 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 3.0000e-04\n",
      "Epoch 104/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9869 - loss: 0.0390 - precision: 0.9876 - recall: 0.9846 - val_accuracy: 0.6667 - val_loss: 0.2560 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 105/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9892 - loss: 0.0386 - precision: 0.9891 - recall: 0.9846 - val_accuracy: 0.7037 - val_loss: 0.2064 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 3.0000e-04\n",
      "Epoch 106/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9877 - loss: 0.0379 - precision: 0.9892 - recall: 0.9853 - val_accuracy: 0.7222 - val_loss: 0.2152 - val_precision: 0.7222 - val_recall: 0.7222 - learning_rate: 3.0000e-04\n",
      "Epoch 107/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9799 - loss: 0.0377 - precision: 0.9837 - recall: 0.9769 - val_accuracy: 0.7037 - val_loss: 0.2070 - val_precision: 0.7170 - val_recall: 0.7037 - learning_rate: 3.0000e-04\n",
      "Epoch 108/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9869 - loss: 0.0357 - precision: 0.9884 - recall: 0.9861 - val_accuracy: 0.6667 - val_loss: 0.2759 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 109/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.9946 - loss: 0.0330 - precision: 0.9946 - recall: 0.9931 - val_accuracy: 0.6667 - val_loss: 0.2665 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 110/300\n",
      "\u001b[1m39/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9874 - loss: 0.0328 - precision: 0.9874 - recall: 0.9865\n",
      "Epoch 110: val_accuracy improved from 0.72222 to 0.74074\n",
      "  Saved model to models\\cnn_best.weights.h5\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9884 - loss: 0.0327 - precision: 0.9892 - recall: 0.9861 - val_accuracy: 0.7407 - val_loss: 0.1883 - val_precision: 0.7407 - val_recall: 0.7407 - learning_rate: 3.0000e-04\n",
      "Epoch 111/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9884 - loss: 0.0325 - precision: 0.9891 - recall: 0.9846 - val_accuracy: 0.6852 - val_loss: 0.2476 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 112/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9907 - loss: 0.0301 - precision: 0.9930 - recall: 0.9907 - val_accuracy: 0.6667 - val_loss: 0.2404 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 113/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9977 - loss: 0.0281 - precision: 0.9977 - recall: 0.9969 - val_accuracy: 0.6667 - val_loss: 0.2444 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 3.0000e-04\n",
      "Epoch 114/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9923 - loss: 0.0288 - precision: 0.9930 - recall: 0.9915 - val_accuracy: 0.6852 - val_loss: 0.2254 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 115/300\n",
      "\u001b[1m39/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9815 - loss: 0.0292 - precision: 0.9828 - recall: 0.9767\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9861 - loss: 0.0278 - precision: 0.9876 - recall: 0.9830 - val_accuracy: 0.6481 - val_loss: 0.2418 - val_precision: 0.6481 - val_recall: 0.6481 - learning_rate: 3.0000e-04\n",
      "Epoch 116/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9931 - loss: 0.0262 - precision: 0.9954 - recall: 0.9923 - val_accuracy: 0.6852 - val_loss: 0.2103 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 1.5000e-04\n",
      "Epoch 117/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9946 - loss: 0.0252 - precision: 0.9946 - recall: 0.9938 - val_accuracy: 0.6667 - val_loss: 0.2195 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 1.5000e-04\n",
      "Epoch 118/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9946 - loss: 0.0250 - precision: 0.9954 - recall: 0.9923 - val_accuracy: 0.6852 - val_loss: 0.2264 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 1.5000e-04\n",
      "Epoch 119/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9985 - loss: 0.0240 - precision: 0.9985 - recall: 0.9961 - val_accuracy: 0.6852 - val_loss: 0.2365 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 1.5000e-04\n",
      "Epoch 120/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.9992 - loss: 0.0234 - precision: 0.9992 - recall: 0.9961 - val_accuracy: 0.6667 - val_loss: 0.2531 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 1.5000e-04\n",
      "Epoch 121/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9954 - loss: 0.0239 - precision: 0.9961 - recall: 0.9931 - val_accuracy: 0.7037 - val_loss: 0.2236 - val_precision: 0.6981 - val_recall: 0.6852 - learning_rate: 1.5000e-04\n",
      "Epoch 122/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9961 - loss: 0.0223 - precision: 0.9985 - recall: 0.9961 - val_accuracy: 0.6852 - val_loss: 0.2202 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 1.5000e-04\n",
      "Epoch 123/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9954 - loss: 0.0226 - precision: 0.9961 - recall: 0.9946 - val_accuracy: 0.6852 - val_loss: 0.2266 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 1.5000e-04\n",
      "Epoch 124/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0212 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.7037 - val_loss: 0.2233 - val_precision: 0.6981 - val_recall: 0.6852 - learning_rate: 1.5000e-04\n",
      "Epoch 125/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9985 - loss: 0.0209 - precision: 0.9985 - recall: 0.9969 - val_accuracy: 0.6852 - val_loss: 0.2319 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 1.5000e-04\n",
      "Epoch 126/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9907 - loss: 0.0218 - precision: 0.9915 - recall: 0.9900 - val_accuracy: 0.7037 - val_loss: 0.2342 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 1.5000e-04\n",
      "Epoch 127/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9957 - loss: 0.0204 - precision: 0.9960 - recall: 0.9957\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 7.500000356230885e-05.\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9946 - loss: 0.0209 - precision: 0.9961 - recall: 0.9946 - val_accuracy: 0.7037 - val_loss: 0.2423 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 1.5000e-04\n",
      "Epoch 128/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9969 - loss: 0.0205 - precision: 0.9977 - recall: 0.9946 - val_accuracy: 0.6852 - val_loss: 0.2564 - val_precision: 0.6981 - val_recall: 0.6852 - learning_rate: 7.5000e-05\n",
      "Epoch 129/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9969 - loss: 0.0199 - precision: 0.9969 - recall: 0.9969 - val_accuracy: 0.6852 - val_loss: 0.2460 - val_precision: 0.6981 - val_recall: 0.6852 - learning_rate: 7.5000e-05\n",
      "Epoch 130/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9992 - loss: 0.0195 - precision: 0.9992 - recall: 0.9985 - val_accuracy: 0.6852 - val_loss: 0.2431 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 7.5000e-05\n",
      "Epoch 131/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9985 - loss: 0.0191 - precision: 0.9992 - recall: 0.9985 - val_accuracy: 0.7037 - val_loss: 0.2394 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 7.5000e-05\n",
      "Epoch 132/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9992 - loss: 0.0190 - precision: 0.9992 - recall: 0.9985 - val_accuracy: 0.6852 - val_loss: 0.2425 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 7.5000e-05\n",
      "Epoch 133/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9977 - loss: 0.0190 - precision: 0.9985 - recall: 0.9977 - val_accuracy: 0.6852 - val_loss: 0.2491 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 7.5000e-05\n",
      "Epoch 134/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9969 - loss: 0.0189 - precision: 0.9969 - recall: 0.9954 - val_accuracy: 0.6852 - val_loss: 0.2420 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 7.5000e-05\n",
      "Epoch 135/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9969 - loss: 0.0184 - precision: 0.9969 - recall: 0.9961 - val_accuracy: 0.7037 - val_loss: 0.2300 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 7.5000e-05\n",
      "Epoch 136/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9985 - loss: 0.0181 - precision: 0.9985 - recall: 0.9985 - val_accuracy: 0.6667 - val_loss: 0.2606 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 7.5000e-05\n",
      "Epoch 137/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9985 - loss: 0.0178 - precision: 0.9992 - recall: 0.9977 - val_accuracy: 0.6852 - val_loss: 0.2463 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 7.5000e-05\n",
      "Epoch 138/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9977 - loss: 0.0177 - precision: 0.9985 - recall: 0.9977 - val_accuracy: 0.6852 - val_loss: 0.2232 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 7.5000e-05\n",
      "Epoch 138: early stopping\n",
      "Restoring model weights from the end of the best epoch: 103.\n",
      "\n",
      "âœ“ CNN model training complete!\n"
     ]
    }
   ],
   "source": [
    "# Train CNN model\n",
    "print(\"\\nğŸš€ Training CNN model...\")\n",
    "\n",
    "# Ensure models directory exists and clean up any locked files\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "Path('models').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Remove existing checkpoint if it exists and might be locked\n",
    "cnn_checkpoint_path = Path('models') / 'cnn_best.weights.h5'\n",
    "if cnn_checkpoint_path.exists():\n",
    "    try:\n",
    "        os.remove(cnn_checkpoint_path)\n",
    "        gc.collect()  # Force garbage collection\n",
    "        time.sleep(2.0)  # Longer delay for Windows file system\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not remove checkpoint file: {e}\")\n",
    "\n",
    "# Custom callback to handle file locking issues\n",
    "class SafeModelCheckpoint(callbacks.Callback):\n",
    "    def __init__(self, filepath, monitor='val_accuracy'):\n",
    "        super().__init__()\n",
    "        self.filepath = filepath\n",
    "        self.monitor = monitor\n",
    "        self.best = -float('inf')\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            return\n",
    "            \n",
    "        if current > self.best:\n",
    "            print(f\"\\nEpoch {epoch + 1}: {self.monitor} improved from {self.best:.5f} to {current:.5f}\")\n",
    "            self.best = current\n",
    "            # Close any existing file handles\n",
    "            gc.collect()\n",
    "            time.sleep(0.1)\n",
    "            # Save with retry logic\n",
    "            for attempt in range(3):\n",
    "                try:\n",
    "                    # Remove old file first\n",
    "                    if os.path.exists(self.filepath):\n",
    "                        os.remove(self.filepath)\n",
    "                        time.sleep(0.1)\n",
    "                    self.model.save_weights(self.filepath)\n",
    "                    print(f\"  Saved model to {self.filepath}\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt < 2:\n",
    "                        print(f\"  Save attempt {attempt + 1} failed, retrying...\")\n",
    "                        gc.collect()\n",
    "                        time.sleep(0.5)\n",
    "                    else:\n",
    "                        print(f\"  Warning: Could not save checkpoint: {e}\")\n",
    "\n",
    "cnn_callbacks = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=35,  # Increased patience for 300 epochs\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=12,  # Increased patience\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    SafeModelCheckpoint(\n",
    "        filepath=str(cnn_checkpoint_path),\n",
    "        monitor='val_accuracy'\n",
    "    )\n",
    "]\n",
    "\n",
    "history_cnn = cnn_model.fit(\n",
    "    X_cnn_train_aug, y_cnn_train_aug,\n",
    "    validation_data=(X_cnn_val_scaled, y_cnn_val_onehot),\n",
    "    epochs=300,  # Increased from 150 to 300\n",
    "    batch_size=32,\n",
    "    class_weight=cnn_class_weight_dict,\n",
    "    callbacks=cnn_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ CNN model training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9f59ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Evaluating CNN model...\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "================================================================================\n",
      "CNN MODEL RESULTS\n",
      "================================================================================\n",
      "\n",
      "Train Set:\n",
      "  Accuracy:  0.9969\n",
      "  Precision: 0.9969\n",
      "  Recall:    0.9969\n",
      "  F1-Score:  0.9969\n",
      "\n",
      "Validation Set:\n",
      "  Accuracy:  0.7037\n",
      "  Precision: 0.7333\n",
      "  Recall:    0.7037\n",
      "  F1-Score:  0.6667\n",
      "\n",
      "Test Set:\n",
      "  Accuracy:  0.7407\n",
      "  Precision: 0.7831\n",
      "  Recall:    0.7407\n",
      "  F1-Score:  0.7285\n",
      "\n",
      "\n",
      "Test Set Confusion Matrix:\n",
      "[[17  1  0]\n",
      " [ 0 16  2]\n",
      " [ 0 11  7]]\n",
      "\n",
      "\n",
      "Test Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low       1.00      0.94      0.97        18\n",
      "      Medium       0.57      0.89      0.70        18\n",
      "        High       0.78      0.39      0.52        18\n",
      "\n",
      "    accuracy                           0.74        54\n",
      "   macro avg       0.78      0.74      0.73        54\n",
      "weighted avg       0.78      0.74      0.73        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate CNN model\n",
    "print(\"\\nğŸ“Š Evaluating CNN model...\")\n",
    "\n",
    "y_cnn_pred_train_probs = cnn_model.predict(X_cnn_train_scaled)\n",
    "y_cnn_pred_val_probs = cnn_model.predict(X_cnn_val_scaled)\n",
    "y_cnn_pred_test_probs = cnn_model.predict(X_cnn_test_scaled)\n",
    "\n",
    "y_cnn_pred_train = np.argmax(y_cnn_pred_train_probs, axis=1)\n",
    "y_cnn_pred_val = np.argmax(y_cnn_pred_val_probs, axis=1)\n",
    "y_cnn_pred_test = np.argmax(y_cnn_pred_test_probs, axis=1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CNN MODEL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for split_name, y_true, y_pred in [\n",
    "    ('Train', y_cnn_train_cat, y_cnn_pred_train),\n",
    "    ('Validation', y_cnn_val_cat, y_cnn_pred_val),\n",
    "    ('Test', y_cnn_test_cat, y_cnn_pred_test)\n",
    "]:\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"\\n{split_name} Set:\")\n",
    "    print(f\"  Accuracy:  {acc:.4f}\")\n",
    "    print(f\"  Precision: {prec:.4f}\")\n",
    "    print(f\"  Recall:    {rec:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "\n",
    "print(\"\\n\\nTest Set Confusion Matrix:\")\n",
    "print(confusion_matrix(y_cnn_test_cat, y_cnn_pred_test))\n",
    "\n",
    "print(\"\\n\\nTest Set Classification Report:\")\n",
    "print(classification_report(y_cnn_test_cat, y_cnn_pred_test, \n",
    "                          target_names=['Low', 'Medium', 'High']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b0b024",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: GRU Model (Gated Recurrent Unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81959e86",
   "metadata": {},
   "source": [
    "### What is GRU?\n",
    "\n",
    "GRU is a recurrent neural network (RNN) variant that maintains hidden state across time steps. Compared to LSTM:\n",
    "- **Fewer parameters** (2 gates vs 3 in LSTM)\n",
    "- **Faster training**\n",
    "- **Better for smaller datasets**\n",
    "- **Less prone to overfitting**\n",
    "\n",
    "**Why GRU for crop yield?**\n",
    "- Captures sequential dependencies (how previous months affect current growth)\n",
    "- Lighter than LSTM, better suited for 576 training samples\n",
    "- Bidirectional GRU sees both past and future context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0d7c9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 2: GRU MODEL\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Loading GRU data...\n",
      "  Total GRU features: 28\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2: GRU MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load GRU data (same as CNN)\n",
    "print(\"\\nğŸ“Š Loading GRU data...\")\n",
    "gru_train = pd.read_csv(splits_path / 'gru' / 'train.csv')\n",
    "gru_val = pd.read_csv(splits_path / 'gru' / 'val.csv')\n",
    "gru_test = pd.read_csv(splits_path / 'gru' / 'test.csv')\n",
    "\n",
    "# Remove missing yields\n",
    "gru_train = gru_train.dropna(subset=[target_col])\n",
    "gru_val = gru_val.dropna(subset=[target_col])\n",
    "gru_test = gru_test.dropna(subset=[target_col])\n",
    "\n",
    "# Encode categorical\n",
    "gru_train['Crop_encoded'] = crop_encoder.transform(gru_train['Crop'])\n",
    "gru_train['Region_encoded'] = region_encoder.transform(gru_train['Region'])\n",
    "gru_val['Crop_encoded'] = crop_encoder.transform(gru_val['Crop'])\n",
    "gru_val['Region_encoded'] = region_encoder.transform(gru_val['Region'])\n",
    "gru_test['Crop_encoded'] = crop_encoder.transform(gru_test['Crop'])\n",
    "gru_test['Region_encoded'] = region_encoder.transform(gru_test['Region'])\n",
    "\n",
    "gru_all_features = gru_feature_cols + ['Crop_encoded', 'Region_encoded']\n",
    "\n",
    "print(f\"  Total GRU features: {len(gru_all_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f96d5686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Creating sequences for GRU...\n",
      "\n",
      "  Train sequences: (324, 12, 28)\n",
      "  Val sequences:   (54, 12, 28)\n",
      "  Test sequences:  (54, 12, 28)\n",
      "  âœ“ Features normalized\n"
     ]
    }
   ],
   "source": [
    "# Create sequences for GRU\n",
    "print(\"\\nğŸ“Š Creating sequences for GRU...\")\n",
    "\n",
    "X_gru_train, y_gru_train = create_sequences(gru_train, gru_all_features, target_col, sequence_length)\n",
    "X_gru_val, y_gru_val = create_sequences(gru_val, gru_all_features, target_col, sequence_length)\n",
    "X_gru_test, y_gru_test = create_sequences(gru_test, gru_all_features, target_col, sequence_length)\n",
    "\n",
    "print(f\"\\n  Train sequences: {X_gru_train.shape}\")\n",
    "print(f\"  Val sequences:   {X_gru_val.shape}\")\n",
    "print(f\"  Test sequences:  {X_gru_test.shape}\")\n",
    "\n",
    "# Normalize\n",
    "scaler_gru = StandardScaler()\n",
    "X_gru_train_reshaped = X_gru_train.reshape(-1, X_gru_train.shape[2])\n",
    "scaler_gru.fit(X_gru_train_reshaped)\n",
    "\n",
    "X_gru_train_scaled = scaler_gru.transform(X_gru_train.reshape(-1, X_gru_train.shape[2])).reshape(X_gru_train.shape)\n",
    "X_gru_val_scaled = scaler_gru.transform(X_gru_val.reshape(-1, X_gru_val.shape[2])).reshape(X_gru_val.shape)\n",
    "X_gru_test_scaled = scaler_gru.transform(X_gru_test.reshape(-1, X_gru_test.shape[2])).reshape(X_gru_test.shape)\n",
    "\n",
    "print(\"  âœ“ Features normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e8ce819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Categorizing yields for GRU...\n",
      "\n",
      "Class distribution (GRU Train):\n",
      "  Class 0: 108 samples (33.3%)\n",
      "  Class 1: 108 samples (33.3%)\n",
      "  Class 2: 108 samples (33.3%)\n"
     ]
    }
   ],
   "source": [
    "# Categorize yields\n",
    "print(\"\\nğŸ“Š Categorizing yields for GRU...\")\n",
    "\n",
    "y_gru_train_cat, gru_percentiles = categorize_yield_balanced(y_gru_train, method='percentile')\n",
    "y_gru_val_cat, _ = categorize_yield_balanced(y_gru_val, method='percentile')\n",
    "y_gru_test_cat, _ = categorize_yield_balanced(y_gru_test, method='percentile')\n",
    "\n",
    "print(f\"\\nClass distribution (GRU Train):\")\n",
    "unique, counts = np.unique(y_gru_train_cat, return_counts=True)\n",
    "for cls, count in zip(unique, counts):\n",
    "    print(f\"  Class {cls}: {count} samples ({count/len(y_gru_train_cat)*100:.1f}%)\")\n",
    "\n",
    "y_gru_train_onehot = to_categorical(y_gru_train_cat, num_classes=3)\n",
    "y_gru_val_onehot = to_categorical(y_gru_val_cat, num_classes=3)\n",
    "y_gru_test_onehot = to_categorical(y_gru_test_cat, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3443627b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ APPLYING DATA AUGMENTATION (GRU)...\n",
      "  Augmented GRU training size: 1296 samples\n",
      "\n",
      "âš–ï¸  GRU Class weights:\n",
      "   Class 0: weight=1.000 (n=432 samples)\n",
      "   Class 1: weight=1.000 (n=432 samples)\n",
      "   Class 2: weight=1.000 (n=432 samples)\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation for GRU\n",
    "print(\"\\nğŸ”„ APPLYING DATA AUGMENTATION (GRU)...\")\n",
    "\n",
    "X_gru_train_aug, y_gru_train_aug = augment_time_series(\n",
    "    X_gru_train_scaled,\n",
    "    y_gru_train_onehot,\n",
    "    num_augmented=3,  # Increased from 1 to 3\n",
    "    noise_level=0.02  # Reduced noise for quality\n",
    ")\n",
    "\n",
    "print(f\"  Augmented GRU training size: {X_gru_train_aug.shape[0]} samples\")\n",
    "\n",
    "# Shuffle\n",
    "indices = np.random.permutation(X_gru_train_aug.shape[0])\n",
    "X_gru_train_aug = X_gru_train_aug[indices]\n",
    "y_gru_train_aug = y_gru_train_aug[indices]\n",
    "\n",
    "# Class weights\n",
    "y_gru_train_labels = np.argmax(y_gru_train_aug, axis=1)\n",
    "gru_class_weights_array = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_gru_train_labels),\n",
    "    y=y_gru_train_labels\n",
    ")\n",
    "gru_class_weight_dict = dict(enumerate(gru_class_weights_array))\n",
    "\n",
    "print(f\"\\nâš–ï¸  GRU Class weights:\")\n",
    "for cls, weight in gru_class_weight_dict.items():\n",
    "    count = np.sum(y_gru_train_labels == cls)\n",
    "    print(f\"   Class {cls}: weight={weight:.3f} (n={count} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bad9815c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">72,576</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_5           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)        â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">99,072</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_6           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">31,104</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_7           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_8           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m192\u001b[0m)        â”‚        \u001b[38;5;34m72,576\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_5           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m192\u001b[0m)        â”‚           \u001b[38;5;34m768\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m192\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚        \u001b[38;5;34m99,072\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_6           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚           \u001b[38;5;34m512\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚        \u001b[38;5;34m31,104\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_7           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚           \u001b[38;5;34m256\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m4,160\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_8           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚           \u001b[38;5;34m256\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚         \u001b[38;5;34m2,080\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              â”‚            \u001b[38;5;34m99\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">210,883</span> (823.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m210,883\u001b[0m (823.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">209,987</span> (820.26 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m209,987\u001b[0m (820.26 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build GRU model with Attention\n",
    "def build_gru_model(sequence_length, n_features, learning_rate=0.0003):\n",
    "    \"\"\"\n",
    "    Build Bidirectional GRU model with attention.\n",
    "    \n",
    "    Architecture:\n",
    "    - Bidirectional GRU layers capture forward and backward patterns\n",
    "    - Attention mechanism for focusing on key time steps\n",
    "    - Enhanced regularization to prevent overfitting\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(sequence_length, n_features)),\n",
    "        \n",
    "        # First Bidirectional GRU layer\n",
    "        layers.Bidirectional(layers.GRU(96, return_sequences=True,\n",
    "                                       kernel_regularizer=regularizers.l2(0.003),\n",
    "                                       recurrent_regularizer=regularizers.l2(0.003),\n",
    "                                       dropout=0.3,\n",
    "                                       recurrent_dropout=0.3)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        # Second Bidirectional GRU layer\n",
    "        layers.Bidirectional(layers.GRU(64, return_sequences=True,\n",
    "                                       kernel_regularizer=regularizers.l2(0.003),\n",
    "                                       recurrent_regularizer=regularizers.l2(0.003),\n",
    "                                       dropout=0.3,\n",
    "                                       recurrent_dropout=0.3)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        # Third GRU layer\n",
    "        layers.Bidirectional(layers.GRU(32, return_sequences=False,\n",
    "                                       kernel_regularizer=regularizers.l2(0.002),\n",
    "                                       recurrent_regularizer=regularizers.l2(0.002),\n",
    "                                       dropout=0.2,\n",
    "                                       recurrent_dropout=0.2)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        # Dense layers\n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.002)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate, clipnorm=1.0)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=focal_loss(gamma=2.0, alpha=0.25),\n",
    "        metrics=['accuracy',\n",
    "                 tf.keras.metrics.Precision(name='precision'),\n",
    "                 tf.keras.metrics.Recall(name='recall')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "gru_model = build_gru_model(\n",
    "    sequence_length=sequence_length,\n",
    "    n_features=X_gru_train_aug.shape[2],\n",
    "    learning_rate=0.0003\n",
    ")\n",
    "gru_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44a8c1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Training GRU model...\n",
      "Epoch 1/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3526 - loss: 3.3977 - precision: 0.3487 - recall: 0.2889\n",
      "Epoch 1: val_accuracy improved from -inf to 0.66667\n",
      "  Saved model to models\\gru_best.weights.h5\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 211ms/step - accuracy: 0.3719 - loss: 3.3163 - precision: 0.3717 - recall: 0.3086 - val_accuracy: 0.6667 - val_loss: 3.0045 - val_precision: 1.0000 - val_recall: 0.0556 - learning_rate: 3.0000e-04\n",
      "Epoch 2/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.3695 - loss: 3.1172 - precision: 0.3760 - recall: 0.2997\n",
      "Epoch 2: val_accuracy improved from 0.66667 to 0.70370\n",
      "  Saved model to models\\gru_best.weights.h5\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 117ms/step - accuracy: 0.4012 - loss: 3.0505 - precision: 0.4118 - recall: 0.3295 - val_accuracy: 0.7037 - val_loss: 2.7639 - val_precision: 0.9474 - val_recall: 0.3333 - learning_rate: 3.0000e-04\n",
      "Epoch 3/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 103ms/step - accuracy: 0.4707 - loss: 2.7887 - precision: 0.4901 - recall: 0.4012 - val_accuracy: 0.6481 - val_loss: 2.5586 - val_precision: 0.8235 - val_recall: 0.5185 - learning_rate: 3.0000e-04\n",
      "Epoch 4/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 78ms/step - accuracy: 0.4954 - loss: 2.5872 - precision: 0.5222 - recall: 0.4167 - val_accuracy: 0.6296 - val_loss: 2.3843 - val_precision: 0.6512 - val_recall: 0.5185 - learning_rate: 3.0000e-04\n",
      "Epoch 5/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - accuracy: 0.5386 - loss: 2.3921 - precision: 0.5593 - recall: 0.4583 - val_accuracy: 0.5370 - val_loss: 2.2395 - val_precision: 0.5532 - val_recall: 0.4815 - learning_rate: 3.0000e-04\n",
      "Epoch 6/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 75ms/step - accuracy: 0.5779 - loss: 2.2147 - precision: 0.6144 - recall: 0.5077 - val_accuracy: 0.5370 - val_loss: 2.1152 - val_precision: 0.5102 - val_recall: 0.4630 - learning_rate: 3.0000e-04\n",
      "Epoch 7/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.5856 - loss: 2.0604 - precision: 0.6338 - recall: 0.5262 - val_accuracy: 0.4815 - val_loss: 2.0024 - val_precision: 0.5000 - val_recall: 0.4630 - learning_rate: 3.0000e-04\n",
      "Epoch 8/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 104ms/step - accuracy: 0.6590 - loss: 1.9218 - precision: 0.6837 - recall: 0.5872 - val_accuracy: 0.5185 - val_loss: 1.8936 - val_precision: 0.5294 - val_recall: 0.5000 - learning_rate: 3.0000e-04\n",
      "Epoch 9/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - accuracy: 0.6397 - loss: 1.8124 - precision: 0.6673 - recall: 0.5633 - val_accuracy: 0.5556 - val_loss: 1.7839 - val_precision: 0.5490 - val_recall: 0.5185 - learning_rate: 3.0000e-04\n",
      "Epoch 10/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.6458 - loss: 1.7109 - precision: 0.6854 - recall: 0.5833 - val_accuracy: 0.5000 - val_loss: 1.6936 - val_precision: 0.5192 - val_recall: 0.5000 - learning_rate: 3.0000e-04\n",
      "Epoch 11/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 89ms/step - accuracy: 0.6744 - loss: 1.6107 - precision: 0.7008 - recall: 0.6019 - val_accuracy: 0.5185 - val_loss: 1.5991 - val_precision: 0.5200 - val_recall: 0.4815 - learning_rate: 3.0000e-04\n",
      "Epoch 12/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.6582 - loss: 1.5321 - precision: 0.7012 - recall: 0.6011 - val_accuracy: 0.4815 - val_loss: 1.5401 - val_precision: 0.4717 - val_recall: 0.4630 - learning_rate: 3.0000e-04\n",
      "Epoch 13/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 77ms/step - accuracy: 0.6790 - loss: 1.4465 - precision: 0.7103 - recall: 0.6111 - val_accuracy: 0.5000 - val_loss: 1.4592 - val_precision: 0.4800 - val_recall: 0.4444 - learning_rate: 3.0000e-04\n",
      "Epoch 14/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.6952 - loss: 1.3752 - precision: 0.7236 - recall: 0.6242 - val_accuracy: 0.5000 - val_loss: 1.4012 - val_precision: 0.4902 - val_recall: 0.4630 - learning_rate: 3.0000e-04\n",
      "Epoch 15/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - accuracy: 0.6767 - loss: 1.3158 - precision: 0.7123 - recall: 0.6188 - val_accuracy: 0.4815 - val_loss: 1.3361 - val_precision: 0.4906 - val_recall: 0.4815 - learning_rate: 3.0000e-04\n",
      "Epoch 16/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - accuracy: 0.6960 - loss: 1.2466 - precision: 0.7237 - recall: 0.6327 - val_accuracy: 0.5000 - val_loss: 1.2833 - val_precision: 0.4808 - val_recall: 0.4630 - learning_rate: 3.0000e-04\n",
      "Epoch 17/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 71ms/step - accuracy: 0.6937 - loss: 1.1952 - precision: 0.7225 - recall: 0.6327 - val_accuracy: 0.5000 - val_loss: 1.2267 - val_precision: 0.4906 - val_recall: 0.4815 - learning_rate: 3.0000e-04\n",
      "Epoch 18/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 77ms/step - accuracy: 0.7299 - loss: 1.1352 - precision: 0.7646 - recall: 0.6667 - val_accuracy: 0.4815 - val_loss: 1.1799 - val_precision: 0.4808 - val_recall: 0.4630 - learning_rate: 3.0000e-04\n",
      "Epoch 19/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 78ms/step - accuracy: 0.7222 - loss: 1.0905 - precision: 0.7420 - recall: 0.6481 - val_accuracy: 0.5000 - val_loss: 1.1128 - val_precision: 0.5094 - val_recall: 0.5000 - learning_rate: 3.0000e-04\n",
      "Epoch 20/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - accuracy: 0.7114 - loss: 1.0449 - precision: 0.7436 - recall: 0.6489 - val_accuracy: 0.5000 - val_loss: 1.0850 - val_precision: 0.5000 - val_recall: 0.4815 - learning_rate: 3.0000e-04\n",
      "Epoch 21/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - accuracy: 0.7292 - loss: 0.9935 - precision: 0.7520 - recall: 0.6551 - val_accuracy: 0.4815 - val_loss: 1.0428 - val_precision: 0.4808 - val_recall: 0.4630 - learning_rate: 3.0000e-04\n",
      "Epoch 22/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.7168 - loss: 0.9573 - precision: 0.7472 - recall: 0.6775 - val_accuracy: 0.5185 - val_loss: 0.9851 - val_precision: 0.5094 - val_recall: 0.5000 - learning_rate: 3.0000e-04\n",
      "Epoch 23/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 86ms/step - accuracy: 0.7454 - loss: 0.9129 - precision: 0.7709 - recall: 0.6906 - val_accuracy: 0.5000 - val_loss: 0.9496 - val_precision: 0.5000 - val_recall: 0.5000 - learning_rate: 3.0000e-04\n",
      "Epoch 24/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 94ms/step - accuracy: 0.7384 - loss: 0.8757 - precision: 0.7646 - recall: 0.6890 - val_accuracy: 0.5000 - val_loss: 0.8968 - val_precision: 0.5094 - val_recall: 0.5000 - learning_rate: 3.0000e-04\n",
      "Epoch 25/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - accuracy: 0.7755 - loss: 0.8390 - precision: 0.7885 - recall: 0.7076 - val_accuracy: 0.5185 - val_loss: 0.8600 - val_precision: 0.5094 - val_recall: 0.5000 - learning_rate: 3.0000e-04\n",
      "Epoch 26/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - accuracy: 0.7593 - loss: 0.8059 - precision: 0.7820 - recall: 0.6921 - val_accuracy: 0.5000 - val_loss: 0.8258 - val_precision: 0.5094 - val_recall: 0.5000 - learning_rate: 3.0000e-04\n",
      "Epoch 27/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - accuracy: 0.7569 - loss: 0.7721 - precision: 0.7849 - recall: 0.7037 - val_accuracy: 0.5000 - val_loss: 0.7940 - val_precision: 0.5000 - val_recall: 0.5000 - learning_rate: 3.0000e-04\n",
      "Epoch 28/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 70ms/step - accuracy: 0.7531 - loss: 0.7457 - precision: 0.7833 - recall: 0.7114 - val_accuracy: 0.5000 - val_loss: 0.7634 - val_precision: 0.5000 - val_recall: 0.5000 - learning_rate: 3.0000e-04\n",
      "Epoch 29/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - accuracy: 0.7778 - loss: 0.7109 - precision: 0.8101 - recall: 0.7207 - val_accuracy: 0.5370 - val_loss: 0.7396 - val_precision: 0.5370 - val_recall: 0.5370 - learning_rate: 3.0000e-04\n",
      "Epoch 30/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 74ms/step - accuracy: 0.7523 - loss: 0.6879 - precision: 0.7691 - recall: 0.6914 - val_accuracy: 0.5185 - val_loss: 0.7067 - val_precision: 0.5185 - val_recall: 0.5185 - learning_rate: 3.0000e-04\n",
      "Epoch 31/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.7600 - loss: 0.6593 - precision: 0.7864 - recall: 0.7045 - val_accuracy: 0.5000 - val_loss: 0.6837 - val_precision: 0.5000 - val_recall: 0.5000 - learning_rate: 3.0000e-04\n",
      "Epoch 32/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 85ms/step - accuracy: 0.7855 - loss: 0.6305 - precision: 0.8133 - recall: 0.7361 - val_accuracy: 0.4630 - val_loss: 0.6629 - val_precision: 0.4615 - val_recall: 0.4444 - learning_rate: 3.0000e-04\n",
      "Epoch 33/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - accuracy: 0.7762 - loss: 0.6086 - precision: 0.8032 - recall: 0.7276 - val_accuracy: 0.4630 - val_loss: 0.6432 - val_precision: 0.4630 - val_recall: 0.4630 - learning_rate: 3.0000e-04\n",
      "Epoch 34/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - accuracy: 0.7832 - loss: 0.5846 - precision: 0.8074 - recall: 0.7245 - val_accuracy: 0.5000 - val_loss: 0.6083 - val_precision: 0.4906 - val_recall: 0.4815 - learning_rate: 3.0000e-04\n",
      "Epoch 35/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step - accuracy: 0.7793 - loss: 0.5618 - precision: 0.8095 - recall: 0.7346 - val_accuracy: 0.4444 - val_loss: 0.6040 - val_precision: 0.4340 - val_recall: 0.4259 - learning_rate: 3.0000e-04\n",
      "Epoch 36/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step - accuracy: 0.7840 - loss: 0.5343 - precision: 0.8038 - recall: 0.7269 - val_accuracy: 0.4259 - val_loss: 0.5865 - val_precision: 0.4340 - val_recall: 0.4259 - learning_rate: 3.0000e-04\n",
      "Epoch 37/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - accuracy: 0.7909 - loss: 0.5204 - precision: 0.8134 - recall: 0.7330 - val_accuracy: 0.4444 - val_loss: 0.5590 - val_precision: 0.4340 - val_recall: 0.4259 - learning_rate: 3.0000e-04\n",
      "Epoch 38/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.7701 - loss: 0.5033 - precision: 0.7944 - recall: 0.7215 - val_accuracy: 0.4815 - val_loss: 0.5327 - val_precision: 0.4706 - val_recall: 0.4444 - learning_rate: 3.0000e-04\n",
      "Epoch 39/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 89ms/step - accuracy: 0.7755 - loss: 0.4803 - precision: 0.8024 - recall: 0.7207 - val_accuracy: 0.5185 - val_loss: 0.5168 - val_precision: 0.5192 - val_recall: 0.5000 - learning_rate: 3.0000e-04\n",
      "Epoch 40/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - accuracy: 0.7724 - loss: 0.4639 - precision: 0.8084 - recall: 0.7292 - val_accuracy: 0.4630 - val_loss: 0.4888 - val_precision: 0.4510 - val_recall: 0.4259 - learning_rate: 3.0000e-04\n",
      "Epoch 41/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 82ms/step - accuracy: 0.8063 - loss: 0.4496 - precision: 0.8246 - recall: 0.7508 - val_accuracy: 0.4444 - val_loss: 0.4863 - val_precision: 0.4400 - val_recall: 0.4074 - learning_rate: 3.0000e-04\n",
      "Epoch 42/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step - accuracy: 0.7832 - loss: 0.4326 - precision: 0.8145 - recall: 0.7353 - val_accuracy: 0.4630 - val_loss: 0.4550 - val_precision: 0.4808 - val_recall: 0.4630 - learning_rate: 3.0000e-04\n",
      "Epoch 43/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - accuracy: 0.8032 - loss: 0.4116 - precision: 0.8296 - recall: 0.7515 - val_accuracy: 0.5000 - val_loss: 0.4413 - val_precision: 0.5200 - val_recall: 0.4815 - learning_rate: 3.0000e-04\n",
      "Epoch 44/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 77ms/step - accuracy: 0.7924 - loss: 0.3992 - precision: 0.8222 - recall: 0.7315 - val_accuracy: 0.4444 - val_loss: 0.4307 - val_precision: 0.4340 - val_recall: 0.4259 - learning_rate: 3.0000e-04\n",
      "Epoch 45/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - accuracy: 0.8032 - loss: 0.3824 - precision: 0.8303 - recall: 0.7623 - val_accuracy: 0.4630 - val_loss: 0.4143 - val_precision: 0.4423 - val_recall: 0.4259 - learning_rate: 3.0000e-04\n",
      "Epoch 46/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.8171 - loss: 0.3674 - precision: 0.8390 - recall: 0.7639 - val_accuracy: 0.5556 - val_loss: 0.3851 - val_precision: 0.5385 - val_recall: 0.5185 - learning_rate: 3.0000e-04\n",
      "Epoch 47/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - accuracy: 0.8148 - loss: 0.3530 - precision: 0.8405 - recall: 0.7685 - val_accuracy: 0.4630 - val_loss: 0.3821 - val_precision: 0.4706 - val_recall: 0.4444 - learning_rate: 3.0000e-04\n",
      "Epoch 48/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - accuracy: 0.8140 - loss: 0.3393 - precision: 0.8419 - recall: 0.7724 - val_accuracy: 0.4630 - val_loss: 0.3840 - val_precision: 0.4528 - val_recall: 0.4444 - learning_rate: 3.0000e-04\n",
      "Epoch 49/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.7994 - loss: 0.3285 - precision: 0.8198 - recall: 0.7546 - val_accuracy: 0.5370 - val_loss: 0.3520 - val_precision: 0.5192 - val_recall: 0.5000 - learning_rate: 3.0000e-04\n",
      "Epoch 50/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 92ms/step - accuracy: 0.8025 - loss: 0.3183 - precision: 0.8224 - recall: 0.7539 - val_accuracy: 0.4815 - val_loss: 0.3453 - val_precision: 0.5098 - val_recall: 0.4815 - learning_rate: 3.0000e-04\n",
      "Epoch 51/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step - accuracy: 0.8133 - loss: 0.3042 - precision: 0.8388 - recall: 0.7708 - val_accuracy: 0.4630 - val_loss: 0.3403 - val_precision: 0.4615 - val_recall: 0.4444 - learning_rate: 3.0000e-04\n",
      "Epoch 52/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.8094 - loss: 0.2959 - precision: 0.8288 - recall: 0.7546 - val_accuracy: 0.5000 - val_loss: 0.3288 - val_precision: 0.5000 - val_recall: 0.4815 - learning_rate: 3.0000e-04\n",
      "Epoch 53/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 77ms/step - accuracy: 0.8117 - loss: 0.2835 - precision: 0.8346 - recall: 0.7747 - val_accuracy: 0.5000 - val_loss: 0.3199 - val_precision: 0.4902 - val_recall: 0.4630 - learning_rate: 3.0000e-04\n",
      "Epoch 54/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - accuracy: 0.8125 - loss: 0.2717 - precision: 0.8463 - recall: 0.7816 - val_accuracy: 0.5370 - val_loss: 0.3140 - val_precision: 0.5192 - val_recall: 0.5000 - learning_rate: 3.0000e-04\n",
      "Epoch 55/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - accuracy: 0.8171 - loss: 0.2644 - precision: 0.8370 - recall: 0.7608 - val_accuracy: 0.4815 - val_loss: 0.3089 - val_precision: 0.4902 - val_recall: 0.4630 - learning_rate: 3.0000e-04\n",
      "Epoch 56/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - accuracy: 0.8302 - loss: 0.2539 - precision: 0.8576 - recall: 0.7809 - val_accuracy: 0.5370 - val_loss: 0.2806 - val_precision: 0.5370 - val_recall: 0.5370 - learning_rate: 3.0000e-04\n",
      "Epoch 57/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - accuracy: 0.8356 - loss: 0.2452 - precision: 0.8526 - recall: 0.7901 - val_accuracy: 0.5000 - val_loss: 0.2793 - val_precision: 0.5294 - val_recall: 0.5000 - learning_rate: 3.0000e-04\n",
      "Epoch 58/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 93ms/step - accuracy: 0.8318 - loss: 0.2348 - precision: 0.8505 - recall: 0.7855 - val_accuracy: 0.5370 - val_loss: 0.2724 - val_precision: 0.5283 - val_recall: 0.5185 - learning_rate: 3.0000e-04\n",
      "Epoch 59/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 78ms/step - accuracy: 0.8326 - loss: 0.2263 - precision: 0.8600 - recall: 0.7963 - val_accuracy: 0.5741 - val_loss: 0.2595 - val_precision: 0.5660 - val_recall: 0.5556 - learning_rate: 3.0000e-04\n",
      "Epoch 60/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 86ms/step - accuracy: 0.8241 - loss: 0.2187 - precision: 0.8485 - recall: 0.7778 - val_accuracy: 0.5556 - val_loss: 0.2553 - val_precision: 0.5556 - val_recall: 0.5556 - learning_rate: 3.0000e-04\n",
      "Epoch 61/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - accuracy: 0.8164 - loss: 0.2119 - precision: 0.8355 - recall: 0.7801 - val_accuracy: 0.5741 - val_loss: 0.2401 - val_precision: 0.5882 - val_recall: 0.5556 - learning_rate: 3.0000e-04\n",
      "Epoch 62/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 88ms/step - accuracy: 0.8233 - loss: 0.2035 - precision: 0.8482 - recall: 0.7932 - val_accuracy: 0.5185 - val_loss: 0.2405 - val_precision: 0.5306 - val_recall: 0.4815 - learning_rate: 3.0000e-04\n",
      "Epoch 63/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - accuracy: 0.8279 - loss: 0.1991 - precision: 0.8557 - recall: 0.7917 - val_accuracy: 0.5185 - val_loss: 0.2503 - val_precision: 0.5192 - val_recall: 0.5000 - learning_rate: 3.0000e-04\n",
      "Epoch 64/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 89ms/step - accuracy: 0.8187 - loss: 0.1896 - precision: 0.8425 - recall: 0.7801 - val_accuracy: 0.5185 - val_loss: 0.2331 - val_precision: 0.5192 - val_recall: 0.5000 - learning_rate: 3.0000e-04\n",
      "Epoch 65/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 85ms/step - accuracy: 0.8140 - loss: 0.1866 - precision: 0.8297 - recall: 0.7670 - val_accuracy: 0.5926 - val_loss: 0.2144 - val_precision: 0.6038 - val_recall: 0.5926 - learning_rate: 3.0000e-04\n",
      "Epoch 66/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - accuracy: 0.8387 - loss: 0.1792 - precision: 0.8605 - recall: 0.7948 - val_accuracy: 0.6111 - val_loss: 0.2089 - val_precision: 0.6226 - val_recall: 0.6111 - learning_rate: 3.0000e-04\n",
      "Epoch 67/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 85ms/step - accuracy: 0.8194 - loss: 0.1712 - precision: 0.8517 - recall: 0.7708 - val_accuracy: 0.5185 - val_loss: 0.2009 - val_precision: 0.5000 - val_recall: 0.4630 - learning_rate: 3.0000e-04\n",
      "Epoch 68/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.8457 - loss: 0.1641 - precision: 0.8733 - recall: 0.8140 - val_accuracy: 0.5185 - val_loss: 0.2017 - val_precision: 0.5094 - val_recall: 0.5000 - learning_rate: 3.0000e-04\n",
      "Epoch 69/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 89ms/step - accuracy: 0.8310 - loss: 0.1608 - precision: 0.8569 - recall: 0.7901 - val_accuracy: 0.5000 - val_loss: 0.1959 - val_precision: 0.5200 - val_recall: 0.4815 - learning_rate: 3.0000e-04\n",
      "Epoch 70/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 98ms/step - accuracy: 0.8241 - loss: 0.1561 - precision: 0.8498 - recall: 0.7816 - val_accuracy: 0.5741 - val_loss: 0.1906 - val_precision: 0.5660 - val_recall: 0.5556 - learning_rate: 3.0000e-04\n",
      "Epoch 71/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 84ms/step - accuracy: 0.8403 - loss: 0.1513 - precision: 0.8652 - recall: 0.8025 - val_accuracy: 0.5185 - val_loss: 0.1924 - val_precision: 0.4902 - val_recall: 0.4630 - learning_rate: 3.0000e-04\n",
      "Epoch 72/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 71ms/step - accuracy: 0.8318 - loss: 0.1445 - precision: 0.8554 - recall: 0.8032 - val_accuracy: 0.5000 - val_loss: 0.1813 - val_precision: 0.5192 - val_recall: 0.5000 - learning_rate: 3.0000e-04\n",
      "Epoch 73/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.8418 - loss: 0.1412 - precision: 0.8599 - recall: 0.8002 - val_accuracy: 0.5370 - val_loss: 0.1868 - val_precision: 0.5472 - val_recall: 0.5370 - learning_rate: 3.0000e-04\n",
      "Epoch 74/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 78ms/step - accuracy: 0.8488 - loss: 0.1370 - precision: 0.8705 - recall: 0.8140 - val_accuracy: 0.5556 - val_loss: 0.1781 - val_precision: 0.5385 - val_recall: 0.5185 - learning_rate: 3.0000e-04\n",
      "Epoch 75/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.8434 - loss: 0.1328 - precision: 0.8685 - recall: 0.8002 - val_accuracy: 0.4444 - val_loss: 0.1889 - val_precision: 0.4314 - val_recall: 0.4074 - learning_rate: 3.0000e-04\n",
      "Epoch 76/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - accuracy: 0.8519 - loss: 0.1266 - precision: 0.8720 - recall: 0.8094 - val_accuracy: 0.5185 - val_loss: 0.1911 - val_precision: 0.5200 - val_recall: 0.4815 - learning_rate: 3.0000e-04\n",
      "Epoch 77/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step - accuracy: 0.8356 - loss: 0.1244 - precision: 0.8637 - recall: 0.7824 - val_accuracy: 0.5370 - val_loss: 0.1576 - val_precision: 0.5200 - val_recall: 0.4815 - learning_rate: 3.0000e-04\n",
      "Epoch 78/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - accuracy: 0.8410 - loss: 0.1199 - precision: 0.8666 - recall: 0.8071 - val_accuracy: 0.6111 - val_loss: 0.1482 - val_precision: 0.6038 - val_recall: 0.5926 - learning_rate: 3.0000e-04\n",
      "Epoch 79/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - accuracy: 0.8573 - loss: 0.1134 - precision: 0.8785 - recall: 0.8148 - val_accuracy: 0.4815 - val_loss: 0.1568 - val_precision: 0.4600 - val_recall: 0.4259 - learning_rate: 3.0000e-04\n",
      "Epoch 80/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 91ms/step - accuracy: 0.8526 - loss: 0.1093 - precision: 0.8715 - recall: 0.8218 - val_accuracy: 0.6296 - val_loss: 0.1430 - val_precision: 0.6154 - val_recall: 0.5926 - learning_rate: 3.0000e-04\n",
      "Epoch 81/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 74ms/step - accuracy: 0.8449 - loss: 0.1077 - precision: 0.8629 - recall: 0.8156 - val_accuracy: 0.4444 - val_loss: 0.1483 - val_precision: 0.4340 - val_recall: 0.4259 - learning_rate: 3.0000e-04\n",
      "Epoch 82/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - accuracy: 0.8457 - loss: 0.1049 - precision: 0.8683 - recall: 0.8140 - val_accuracy: 0.6296 - val_loss: 0.1254 - val_precision: 0.6538 - val_recall: 0.6296 - learning_rate: 3.0000e-04\n",
      "Epoch 83/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step - accuracy: 0.8418 - loss: 0.1035 - precision: 0.8582 - recall: 0.8079 - val_accuracy: 0.5370 - val_loss: 0.1435 - val_precision: 0.5370 - val_recall: 0.5370 - learning_rate: 3.0000e-04\n",
      "Epoch 84/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 85ms/step - accuracy: 0.8580 - loss: 0.0975 - precision: 0.8773 - recall: 0.8272 - val_accuracy: 0.6111 - val_loss: 0.1331 - val_precision: 0.6154 - val_recall: 0.5926 - learning_rate: 3.0000e-04\n",
      "Epoch 85/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 96ms/step - accuracy: 0.8434 - loss: 0.0964 - precision: 0.8656 - recall: 0.8102 - val_accuracy: 0.6111 - val_loss: 0.1285 - val_precision: 0.6038 - val_recall: 0.5926 - learning_rate: 3.0000e-04\n",
      "Epoch 86/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - accuracy: 0.8549 - loss: 0.0920 - precision: 0.8791 - recall: 0.8194 - val_accuracy: 0.5556 - val_loss: 0.1333 - val_precision: 0.5660 - val_recall: 0.5556 - learning_rate: 3.0000e-04\n",
      "Epoch 87/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - accuracy: 0.8611 - loss: 0.0874 - precision: 0.8749 - recall: 0.8256 - val_accuracy: 0.6111 - val_loss: 0.1411 - val_precision: 0.6000 - val_recall: 0.5556 - learning_rate: 3.0000e-04\n",
      "Epoch 88/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.8380 - loss: 0.0901 - precision: 0.8550 - recall: 0.8056 - val_accuracy: 0.5741 - val_loss: 0.1394 - val_precision: 0.5660 - val_recall: 0.5556 - learning_rate: 3.0000e-04\n",
      "Epoch 89/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 93ms/step - accuracy: 0.8511 - loss: 0.0852 - precision: 0.8719 - recall: 0.8248 - val_accuracy: 0.5556 - val_loss: 0.1203 - val_precision: 0.5686 - val_recall: 0.5370 - learning_rate: 3.0000e-04\n",
      "Epoch 90/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 76ms/step - accuracy: 0.8696 - loss: 0.0804 - precision: 0.8908 - recall: 0.8434 - val_accuracy: 0.6111 - val_loss: 0.1174 - val_precision: 0.6000 - val_recall: 0.5556 - learning_rate: 3.0000e-04\n",
      "Epoch 91/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 71ms/step - accuracy: 0.8526 - loss: 0.0819 - precision: 0.8682 - recall: 0.8233 - val_accuracy: 0.5556 - val_loss: 0.1088 - val_precision: 0.5472 - val_recall: 0.5370 - learning_rate: 3.0000e-04\n",
      "Epoch 92/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 86ms/step - accuracy: 0.8549 - loss: 0.0807 - precision: 0.8746 - recall: 0.8179 - val_accuracy: 0.5926 - val_loss: 0.1066 - val_precision: 0.6000 - val_recall: 0.5556 - learning_rate: 3.0000e-04\n",
      "Epoch 93/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - accuracy: 0.8526 - loss: 0.0762 - precision: 0.8715 - recall: 0.8218 - val_accuracy: 0.5926 - val_loss: 0.1009 - val_precision: 0.5926 - val_recall: 0.5926 - learning_rate: 3.0000e-04\n",
      "Epoch 94/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 87ms/step - accuracy: 0.8364 - loss: 0.0771 - precision: 0.8612 - recall: 0.8140 - val_accuracy: 0.5000 - val_loss: 0.1162 - val_precision: 0.4902 - val_recall: 0.4630 - learning_rate: 3.0000e-04\n",
      "Epoch 95/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 74ms/step - accuracy: 0.8495 - loss: 0.0737 - precision: 0.8732 - recall: 0.8233 - val_accuracy: 0.7037 - val_loss: 0.0985 - val_precision: 0.6981 - val_recall: 0.6852 - learning_rate: 3.0000e-04\n",
      "Epoch 96/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.8426 - loss: 0.0719 - precision: 0.8649 - recall: 0.8148 - val_accuracy: 0.5741 - val_loss: 0.1096 - val_precision: 0.5769 - val_recall: 0.5556 - learning_rate: 3.0000e-04\n",
      "Epoch 97/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.8495 - loss: 0.0702 - precision: 0.8684 - recall: 0.8194 - val_accuracy: 0.6296 - val_loss: 0.0979 - val_precision: 0.6226 - val_recall: 0.6111 - learning_rate: 3.0000e-04\n",
      "Epoch 98/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.8580 - loss: 0.0661 - precision: 0.8780 - recall: 0.8272 - val_accuracy: 0.6111 - val_loss: 0.1063 - val_precision: 0.5918 - val_recall: 0.5370 - learning_rate: 3.0000e-04\n",
      "Epoch 99/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.8526 - loss: 0.0667 - precision: 0.8722 - recall: 0.8110 - val_accuracy: 0.6296 - val_loss: 0.0934 - val_precision: 0.6471 - val_recall: 0.6111 - learning_rate: 3.0000e-04\n",
      "Epoch 100/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.8580 - loss: 0.0658 - precision: 0.8762 - recall: 0.8302 - val_accuracy: 0.5926 - val_loss: 0.1004 - val_precision: 0.5926 - val_recall: 0.5926 - learning_rate: 3.0000e-04\n",
      "Epoch 101/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 99ms/step - accuracy: 0.8557 - loss: 0.0649 - precision: 0.8741 - recall: 0.8194 - val_accuracy: 0.6296 - val_loss: 0.0931 - val_precision: 0.6346 - val_recall: 0.6111 - learning_rate: 3.0000e-04\n",
      "Epoch 102/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 67ms/step - accuracy: 0.8534 - loss: 0.0644 - precision: 0.8692 - recall: 0.8202 - val_accuracy: 0.4444 - val_loss: 0.1202 - val_precision: 0.4314 - val_recall: 0.4074 - learning_rate: 3.0000e-04\n",
      "Epoch 103/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.8557 - loss: 0.0619 - precision: 0.8752 - recall: 0.8279 - val_accuracy: 0.5556 - val_loss: 0.0918 - val_precision: 0.5660 - val_recall: 0.5556 - learning_rate: 3.0000e-04\n",
      "Epoch 104/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.8542 - loss: 0.0615 - precision: 0.8778 - recall: 0.8256 - val_accuracy: 0.5741 - val_loss: 0.0994 - val_precision: 0.5577 - val_recall: 0.5370 - learning_rate: 3.0000e-04\n",
      "Epoch 105/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.8611 - loss: 0.0581 - precision: 0.8764 - recall: 0.8264 - val_accuracy: 0.6296 - val_loss: 0.0842 - val_precision: 0.6400 - val_recall: 0.5926 - learning_rate: 3.0000e-04\n",
      "Epoch 106/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8747 - loss: 0.0582 - precision: 0.8887 - recall: 0.8404\n",
      "Epoch 106: val_accuracy improved from 0.70370 to 0.72222\n",
      "  Saved model to models\\gru_best.weights.h5\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 90ms/step - accuracy: 0.8511 - loss: 0.0610 - precision: 0.8660 - recall: 0.8179 - val_accuracy: 0.7222 - val_loss: 0.0715 - val_precision: 0.7308 - val_recall: 0.7037 - learning_rate: 3.0000e-04\n",
      "Epoch 107/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - accuracy: 0.8480 - loss: 0.0574 - precision: 0.8666 - recall: 0.8218 - val_accuracy: 0.5556 - val_loss: 0.0918 - val_precision: 0.5660 - val_recall: 0.5556 - learning_rate: 3.0000e-04\n",
      "Epoch 108/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 65ms/step - accuracy: 0.8534 - loss: 0.0572 - precision: 0.8809 - recall: 0.8218 - val_accuracy: 0.5926 - val_loss: 0.0919 - val_precision: 0.5882 - val_recall: 0.5556 - learning_rate: 3.0000e-04\n",
      "Epoch 109/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 67ms/step - accuracy: 0.8611 - loss: 0.0553 - precision: 0.8753 - recall: 0.8287 - val_accuracy: 0.5926 - val_loss: 0.0937 - val_precision: 0.6038 - val_recall: 0.5926 - learning_rate: 3.0000e-04\n",
      "Epoch 110/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.8642 - loss: 0.0532 - precision: 0.8803 - recall: 0.8341 - val_accuracy: 0.5370 - val_loss: 0.1069 - val_precision: 0.5714 - val_recall: 0.5185 - learning_rate: 3.0000e-04\n",
      "Epoch 111/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.8627 - loss: 0.0533 - precision: 0.8794 - recall: 0.8272 - val_accuracy: 0.5370 - val_loss: 0.1007 - val_precision: 0.5192 - val_recall: 0.5000 - learning_rate: 3.0000e-04\n",
      "Epoch 112/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - accuracy: 0.8665 - loss: 0.0540 - precision: 0.8817 - recall: 0.8341 - val_accuracy: 0.5556 - val_loss: 0.1091 - val_precision: 0.5577 - val_recall: 0.5370 - learning_rate: 3.0000e-04\n",
      "Epoch 113/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.8619 - loss: 0.0535 - precision: 0.8728 - recall: 0.8364 - val_accuracy: 0.6111 - val_loss: 0.0884 - val_precision: 0.6078 - val_recall: 0.5741 - learning_rate: 3.0000e-04\n",
      "Epoch 114/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.8565 - loss: 0.0527 - precision: 0.8736 - recall: 0.8264 - val_accuracy: 0.7037 - val_loss: 0.0730 - val_precision: 0.7308 - val_recall: 0.7037 - learning_rate: 3.0000e-04\n",
      "Epoch 115/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.8681 - loss: 0.0502 - precision: 0.8865 - recall: 0.8380 - val_accuracy: 0.5000 - val_loss: 0.0931 - val_precision: 0.4706 - val_recall: 0.4444 - learning_rate: 3.0000e-04\n",
      "Epoch 116/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 62ms/step - accuracy: 0.8673 - loss: 0.0517 - precision: 0.8805 - recall: 0.8418 - val_accuracy: 0.6667 - val_loss: 0.0851 - val_precision: 0.6604 - val_recall: 0.6481 - learning_rate: 3.0000e-04\n",
      "Epoch 117/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.8611 - loss: 0.0500 - precision: 0.8768 - recall: 0.8241 - val_accuracy: 0.4815 - val_loss: 0.0990 - val_precision: 0.4906 - val_recall: 0.4815 - learning_rate: 3.0000e-04\n",
      "Epoch 118/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8383 - loss: 0.0497 - precision: 0.8669 - recall: 0.8063\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 89ms/step - accuracy: 0.8326 - loss: 0.0509 - precision: 0.8519 - recall: 0.7986 - val_accuracy: 0.7037 - val_loss: 0.0764 - val_precision: 0.6863 - val_recall: 0.6481 - learning_rate: 3.0000e-04\n",
      "Epoch 119/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 70ms/step - accuracy: 0.8603 - loss: 0.0484 - precision: 0.8819 - recall: 0.8295 - val_accuracy: 0.7037 - val_loss: 0.0693 - val_precision: 0.6981 - val_recall: 0.6852 - learning_rate: 1.5000e-04\n",
      "Epoch 120/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.8719 - loss: 0.0460 - precision: 0.8853 - recall: 0.8457 - val_accuracy: 0.6852 - val_loss: 0.0696 - val_precision: 0.7059 - val_recall: 0.6667 - learning_rate: 1.5000e-04\n",
      "Epoch 121/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.8704 - loss: 0.0470 - precision: 0.8898 - recall: 0.8472 - val_accuracy: 0.6852 - val_loss: 0.0691 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 1.5000e-04\n",
      "Epoch 122/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 65ms/step - accuracy: 0.8819 - loss: 0.0458 - precision: 0.8961 - recall: 0.8519 - val_accuracy: 0.6667 - val_loss: 0.0685 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 1.5000e-04\n",
      "Epoch 123/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.8711 - loss: 0.0441 - precision: 0.8902 - recall: 0.8380 - val_accuracy: 0.6481 - val_loss: 0.0713 - val_precision: 0.6538 - val_recall: 0.6296 - learning_rate: 1.5000e-04\n",
      "Epoch 124/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.8719 - loss: 0.0453 - precision: 0.8825 - recall: 0.8403 - val_accuracy: 0.6111 - val_loss: 0.0727 - val_precision: 0.6078 - val_recall: 0.5741 - learning_rate: 1.5000e-04\n",
      "Epoch 125/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - accuracy: 0.8696 - loss: 0.0450 - precision: 0.8819 - recall: 0.8410 - val_accuracy: 0.5741 - val_loss: 0.0845 - val_precision: 0.5849 - val_recall: 0.5741 - learning_rate: 1.5000e-04\n",
      "Epoch 126/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step - accuracy: 0.8735 - loss: 0.0427 - precision: 0.8872 - recall: 0.8434 - val_accuracy: 0.6296 - val_loss: 0.0775 - val_precision: 0.6600 - val_recall: 0.6111 - learning_rate: 1.5000e-04\n",
      "Epoch 127/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.8750 - loss: 0.0417 - precision: 0.8936 - recall: 0.8488 - val_accuracy: 0.6111 - val_loss: 0.0793 - val_precision: 0.6471 - val_recall: 0.6111 - learning_rate: 1.5000e-04\n",
      "Epoch 128/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - accuracy: 0.8673 - loss: 0.0424 - precision: 0.8767 - recall: 0.8341 - val_accuracy: 0.6296 - val_loss: 0.0824 - val_precision: 0.6346 - val_recall: 0.6111 - learning_rate: 1.5000e-04\n",
      "Epoch 129/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - accuracy: 0.8650 - loss: 0.0445 - precision: 0.8811 - recall: 0.8349 - val_accuracy: 0.6667 - val_loss: 0.0883 - val_precision: 0.6667 - val_recall: 0.6296 - learning_rate: 1.5000e-04\n",
      "Epoch 130/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 68ms/step - accuracy: 0.8642 - loss: 0.0443 - precision: 0.8855 - recall: 0.8356 - val_accuracy: 0.6667 - val_loss: 0.0810 - val_precision: 0.6731 - val_recall: 0.6481 - learning_rate: 1.5000e-04\n",
      "Epoch 131/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.8665 - loss: 0.0431 - precision: 0.8826 - recall: 0.8356 - val_accuracy: 0.5926 - val_loss: 0.0973 - val_precision: 0.5849 - val_recall: 0.5741 - learning_rate: 1.5000e-04\n",
      "Epoch 132/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - accuracy: 0.8665 - loss: 0.0431 - precision: 0.8792 - recall: 0.8426 - val_accuracy: 0.6111 - val_loss: 0.0967 - val_precision: 0.6038 - val_recall: 0.5926 - learning_rate: 1.5000e-04\n",
      "Epoch 133/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 73ms/step - accuracy: 0.8673 - loss: 0.0436 - precision: 0.8783 - recall: 0.8356 - val_accuracy: 0.6296 - val_loss: 0.0858 - val_precision: 0.6346 - val_recall: 0.6111 - learning_rate: 1.5000e-04\n",
      "Epoch 134/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8687 - loss: 0.0423 - precision: 0.8820 - recall: 0.8395\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 7.500000356230885e-05.\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - accuracy: 0.8673 - loss: 0.0433 - precision: 0.8818 - recall: 0.8403 - val_accuracy: 0.6852 - val_loss: 0.0885 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 1.5000e-04\n",
      "Epoch 135/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 95ms/step - accuracy: 0.8735 - loss: 0.0411 - precision: 0.8902 - recall: 0.8449 - val_accuracy: 0.6667 - val_loss: 0.0845 - val_precision: 0.6538 - val_recall: 0.6296 - learning_rate: 7.5000e-05\n",
      "Epoch 136/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - accuracy: 0.8704 - loss: 0.0409 - precision: 0.8894 - recall: 0.8441 - val_accuracy: 0.6296 - val_loss: 0.0912 - val_precision: 0.6346 - val_recall: 0.6111 - learning_rate: 7.5000e-05\n",
      "Epoch 137/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.8804 - loss: 0.0395 - precision: 0.8976 - recall: 0.8526 - val_accuracy: 0.6481 - val_loss: 0.0857 - val_precision: 0.6667 - val_recall: 0.6296 - learning_rate: 7.5000e-05\n",
      "Epoch 138/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.8727 - loss: 0.0410 - precision: 0.8822 - recall: 0.8434 - val_accuracy: 0.6481 - val_loss: 0.0870 - val_precision: 0.6538 - val_recall: 0.6296 - learning_rate: 7.5000e-05\n",
      "Epoch 139/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.8727 - loss: 0.0407 - precision: 0.8886 - recall: 0.8495 - val_accuracy: 0.5741 - val_loss: 0.0870 - val_precision: 0.5962 - val_recall: 0.5741 - learning_rate: 7.5000e-05\n",
      "Epoch 140/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - accuracy: 0.8765 - loss: 0.0380 - precision: 0.8895 - recall: 0.8511 - val_accuracy: 0.6296 - val_loss: 0.0882 - val_precision: 0.6200 - val_recall: 0.5741 - learning_rate: 7.5000e-05\n",
      "Epoch 141/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 79ms/step - accuracy: 0.8704 - loss: 0.0393 - precision: 0.8874 - recall: 0.8395 - val_accuracy: 0.6481 - val_loss: 0.0870 - val_precision: 0.6471 - val_recall: 0.6111 - learning_rate: 7.5000e-05\n",
      "Epoch 142/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.8696 - loss: 0.0392 - precision: 0.8880 - recall: 0.8380 - val_accuracy: 0.6481 - val_loss: 0.0835 - val_precision: 0.6471 - val_recall: 0.6111 - learning_rate: 7.5000e-05\n",
      "Epoch 143/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.8789 - loss: 0.0401 - precision: 0.8961 - recall: 0.8519 - val_accuracy: 0.6667 - val_loss: 0.0782 - val_precision: 0.6604 - val_recall: 0.6481 - learning_rate: 7.5000e-05\n",
      "Epoch 144/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - accuracy: 0.8789 - loss: 0.0377 - precision: 0.8903 - recall: 0.8519 - val_accuracy: 0.6667 - val_loss: 0.0815 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 7.5000e-05\n",
      "Epoch 145/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.8650 - loss: 0.0391 - precision: 0.8820 - recall: 0.8418 - val_accuracy: 0.6296 - val_loss: 0.0809 - val_precision: 0.6538 - val_recall: 0.6296 - learning_rate: 7.5000e-05\n",
      "Epoch 146/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8831 - loss: 0.0393 - precision: 0.8982 - recall: 0.8583\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 3.7500001781154424e-05.\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 122ms/step - accuracy: 0.8819 - loss: 0.0397 - precision: 0.8981 - recall: 0.8565 - val_accuracy: 0.6481 - val_loss: 0.0945 - val_precision: 0.6538 - val_recall: 0.6296 - learning_rate: 7.5000e-05\n",
      "Epoch 147/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.8735 - loss: 0.0365 - precision: 0.8906 - recall: 0.8542 - val_accuracy: 0.6667 - val_loss: 0.0878 - val_precision: 0.6604 - val_recall: 0.6481 - learning_rate: 3.7500e-05\n",
      "Epoch 148/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step - accuracy: 0.8889 - loss: 0.0373 - precision: 0.9035 - recall: 0.8596 - val_accuracy: 0.6481 - val_loss: 0.0900 - val_precision: 0.6481 - val_recall: 0.6481 - learning_rate: 3.7500e-05\n",
      "Epoch 149/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 87ms/step - accuracy: 0.8742 - loss: 0.0379 - precision: 0.8864 - recall: 0.8488 - val_accuracy: 0.6111 - val_loss: 0.0948 - val_precision: 0.6078 - val_recall: 0.5741 - learning_rate: 3.7500e-05\n",
      "Epoch 150/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.8611 - loss: 0.0403 - precision: 0.8742 - recall: 0.8364 - val_accuracy: 0.6296 - val_loss: 0.0845 - val_precision: 0.6400 - val_recall: 0.5926 - learning_rate: 3.7500e-05\n",
      "Epoch 151/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - accuracy: 0.8796 - loss: 0.0377 - precision: 0.8912 - recall: 0.8596 - val_accuracy: 0.6667 - val_loss: 0.0800 - val_precision: 0.6604 - val_recall: 0.6481 - learning_rate: 3.7500e-05\n",
      "Epoch 152/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 87ms/step - accuracy: 0.8619 - loss: 0.0387 - precision: 0.8795 - recall: 0.8395 - val_accuracy: 0.6667 - val_loss: 0.0785 - val_precision: 0.6731 - val_recall: 0.6481 - learning_rate: 3.7500e-05\n",
      "Epoch 153/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 67ms/step - accuracy: 0.8866 - loss: 0.0365 - precision: 0.8998 - recall: 0.8665 - val_accuracy: 0.5926 - val_loss: 0.0883 - val_precision: 0.5962 - val_recall: 0.5741 - learning_rate: 3.7500e-05\n",
      "Epoch 154/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - accuracy: 0.8696 - loss: 0.0372 - precision: 0.8856 - recall: 0.8480 - val_accuracy: 0.6111 - val_loss: 0.0819 - val_precision: 0.6078 - val_recall: 0.5741 - learning_rate: 3.7500e-05\n",
      "Epoch 155/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.8719 - loss: 0.0386 - precision: 0.8844 - recall: 0.8503 - val_accuracy: 0.5926 - val_loss: 0.0828 - val_precision: 0.6038 - val_recall: 0.5926 - learning_rate: 3.7500e-05\n",
      "Epoch 156/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step - accuracy: 0.8711 - loss: 0.0387 - precision: 0.8917 - recall: 0.8511 - val_accuracy: 0.6667 - val_loss: 0.0762 - val_precision: 0.6731 - val_recall: 0.6481 - learning_rate: 3.7500e-05\n",
      "Epoch 157/300\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - accuracy: 0.8858 - loss: 0.0371 - precision: 0.9046 - recall: 0.8565 - val_accuracy: 0.6481 - val_loss: 0.0773 - val_precision: 0.6604 - val_recall: 0.6481 - learning_rate: 3.7500e-05\n",
      "Epoch 157: early stopping\n",
      "Restoring model weights from the end of the best epoch: 122.\n",
      "\n",
      "âœ“ GRU model training complete!\n"
     ]
    }
   ],
   "source": [
    "# Train GRU model\n",
    "print(\"\\nğŸš€ Training GRU model...\")\n",
    "\n",
    "# Ensure models directory exists and clean up any locked files\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "Path('models').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Remove existing checkpoint if it exists and might be locked\n",
    "gru_checkpoint_path = Path('models') / 'gru_best.weights.h5'\n",
    "if gru_checkpoint_path.exists():\n",
    "    try:\n",
    "        os.remove(gru_checkpoint_path)\n",
    "        gc.collect()  # Force garbage collection\n",
    "        time.sleep(2.0)  # Longer delay for Windows file system\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not remove checkpoint file: {e}\")\n",
    "\n",
    "# Custom callback to handle file locking issues\n",
    "class SafeModelCheckpoint(callbacks.Callback):\n",
    "    def __init__(self, filepath, monitor='val_accuracy'):\n",
    "        super().__init__()\n",
    "        self.filepath = filepath\n",
    "        self.monitor = monitor\n",
    "        self.best = -float('inf')\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            return\n",
    "            \n",
    "        if current > self.best:\n",
    "            print(f\"\\nEpoch {epoch + 1}: {self.monitor} improved from {self.best:.5f} to {current:.5f}\")\n",
    "            self.best = current\n",
    "            # Close any existing file handles\n",
    "            gc.collect()\n",
    "            time.sleep(0.1)\n",
    "            # Save with retry logic\n",
    "            for attempt in range(3):\n",
    "                try:\n",
    "                    # Remove old file first\n",
    "                    if os.path.exists(self.filepath):\n",
    "                        os.remove(self.filepath)\n",
    "                        time.sleep(0.1)\n",
    "                    self.model.save_weights(self.filepath)\n",
    "                    print(f\"  Saved model to {self.filepath}\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt < 2:\n",
    "                        print(f\"  Save attempt {attempt + 1} failed, retrying...\")\n",
    "                        gc.collect()\n",
    "                        time.sleep(0.5)\n",
    "                    else:\n",
    "                        print(f\"  Warning: Could not save checkpoint: {e}\")\n",
    "\n",
    "gru_callbacks = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=35,  # Increased patience\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=12,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    SafeModelCheckpoint(\n",
    "        filepath=str(gru_checkpoint_path),\n",
    "        monitor='val_accuracy'\n",
    "    )\n",
    "]\n",
    "\n",
    "history_gru = gru_model.fit(\n",
    "    X_gru_train_aug, y_gru_train_aug,\n",
    "    validation_data=(X_gru_val_scaled, y_gru_val_onehot),\n",
    "    epochs=300,  # Increased from 150 to 300\n",
    "    batch_size=32,\n",
    "    class_weight=gru_class_weight_dict,\n",
    "    callbacks=gru_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ GRU model training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9563a98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Evaluating GRU model...\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 335ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\n",
      "================================================================================\n",
      "GRU MODEL RESULTS\n",
      "================================================================================\n",
      "\n",
      "Train Set:\n",
      "  Accuracy:  0.9568\n",
      "  Precision: 0.9567\n",
      "  Recall:    0.9568\n",
      "  F1-Score:  0.9566\n",
      "\n",
      "Validation Set:\n",
      "  Accuracy:  0.6667\n",
      "  Precision: 0.5000\n",
      "  Recall:    0.6667\n",
      "  F1-Score:  0.5556\n",
      "\n",
      "Test Set:\n",
      "  Accuracy:  0.6852\n",
      "  Precision: 0.8381\n",
      "  Recall:    0.6852\n",
      "  F1-Score:  0.5948\n",
      "\n",
      "\n",
      "Test Set Confusion Matrix:\n",
      "[[18  0  0]\n",
      " [ 0 18  0]\n",
      " [ 0 17  1]]\n",
      "\n",
      "\n",
      "Test Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low       1.00      1.00      1.00        18\n",
      "      Medium       0.51      1.00      0.68        18\n",
      "        High       1.00      0.06      0.11        18\n",
      "\n",
      "    accuracy                           0.69        54\n",
      "   macro avg       0.84      0.69      0.59        54\n",
      "weighted avg       0.84      0.69      0.59        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate GRU model\n",
    "print(\"\\nğŸ“Š Evaluating GRU model...\")\n",
    "\n",
    "y_gru_pred_train_probs = gru_model.predict(X_gru_train_scaled)\n",
    "y_gru_pred_val_probs = gru_model.predict(X_gru_val_scaled)\n",
    "y_gru_pred_test_probs = gru_model.predict(X_gru_test_scaled)\n",
    "\n",
    "y_gru_pred_train = np.argmax(y_gru_pred_train_probs, axis=1)\n",
    "y_gru_pred_val = np.argmax(y_gru_pred_val_probs, axis=1)\n",
    "y_gru_pred_test = np.argmax(y_gru_pred_test_probs, axis=1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GRU MODEL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for split_name, y_true, y_pred in [\n",
    "    ('Train', y_gru_train_cat, y_gru_pred_train),\n",
    "    ('Validation', y_gru_val_cat, y_gru_pred_val),\n",
    "    ('Test', y_gru_test_cat, y_gru_pred_test)\n",
    "]:\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"\\n{split_name} Set:\")\n",
    "    print(f\"  Accuracy:  {acc:.4f}\")\n",
    "    print(f\"  Precision: {prec:.4f}\")\n",
    "    print(f\"  Recall:    {rec:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "\n",
    "print(\"\\n\\nTest Set Confusion Matrix:\")\n",
    "print(confusion_matrix(y_gru_test_cat, y_gru_pred_test))\n",
    "\n",
    "print(\"\\n\\nTest Set Classification Report:\")\n",
    "print(classification_report(y_gru_test_cat, y_gru_pred_test,\n",
    "                          target_names=['Low', 'Medium', 'High']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62010648",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Hybrid CNN-GRU Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2438dcdf",
   "metadata": {},
   "source": [
    "### What is Hybrid CNN-GRU?\n",
    "\n",
    "Combines the strengths of both architectures:\n",
    "\n",
    "**CNN Branch (Temporal):**\n",
    "- Extracts local patterns from monthly sequences\n",
    "- Captures seasonal signatures, growth patterns\n",
    "\n",
    "**GRU Branch (Sequential):**\n",
    "- Models temporal dependencies from CNN features\n",
    "- Captures how patterns evolve across growing season\n",
    "\n",
    "**Static Branch:**\n",
    "- Processes soil properties and lag features\n",
    "- Non-temporal but crucial predictors\n",
    "\n",
    "**Fusion:**\n",
    "- Concatenates temporal and static representations\n",
    "- Dense layers learn optimal combination\n",
    "- Most powerful of the three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2965e281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 3: HYBRID CNN-GRU MODEL\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Loading Hybrid data...\n",
      "  Temporal features: 17\n",
      "  Static features: 15\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3: HYBRID CNN-GRU MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load Hybrid data\n",
    "print(\"\\nğŸ“Š Loading Hybrid data...\")\n",
    "hybrid_train = pd.read_csv(splits_path / 'hybrid' / 'train.csv')\n",
    "hybrid_val = pd.read_csv(splits_path / 'hybrid' / 'val.csv')\n",
    "hybrid_test = pd.read_csv(splits_path / 'hybrid' / 'test.csv')\n",
    "\n",
    "# Remove missing yields\n",
    "hybrid_train = hybrid_train.dropna(subset=[target_col])\n",
    "hybrid_val = hybrid_val.dropna(subset=[target_col])\n",
    "hybrid_test = hybrid_test.dropna(subset=[target_col])\n",
    "\n",
    "# Encode categorical\n",
    "hybrid_train['Crop_encoded'] = crop_encoder.transform(hybrid_train['Crop'])\n",
    "hybrid_train['Region_encoded'] = region_encoder.transform(hybrid_train['Region'])\n",
    "hybrid_val['Crop_encoded'] = crop_encoder.transform(hybrid_val['Crop'])\n",
    "hybrid_val['Region_encoded'] = region_encoder.transform(hybrid_val['Region'])\n",
    "hybrid_test['Crop_encoded'] = crop_encoder.transform(hybrid_test['Crop'])\n",
    "hybrid_test['Region_encoded'] = region_encoder.transform(hybrid_test['Region'])\n",
    "\n",
    "# Add encodings to static features\n",
    "hybrid_static_cols_full = hybrid_static_cols + ['Crop_encoded', 'Region_encoded']\n",
    "\n",
    "print(f\"  Temporal features: {len(hybrid_temporal_cols)}\")\n",
    "print(f\"  Static features: {len(hybrid_static_cols_full)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed942a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Creating Hybrid sequences...\n",
      "\n",
      "  Train temporal: (324, 12, 17)\n",
      "  Train static:   (324, 15)\n",
      "  Val temporal:   (54, 12, 17)\n",
      "  Test temporal:  (54, 12, 17)\n"
     ]
    }
   ],
   "source": [
    "# Prepare Hybrid sequences\n",
    "print(\"\\nğŸ“Š Creating Hybrid sequences...\")\n",
    "\n",
    "def create_hybrid_sequences(df, temporal_cols, static_cols, target_col, sequence_length=12):\n",
    "    \"\"\"Create sequences with separate temporal and static components\"\"\"\n",
    "    temporal_sequences = []\n",
    "    static_features = []\n",
    "    targets = []\n",
    "    \n",
    "    for (region, crop, year), group in df.groupby(['Region', 'Crop', 'Year']):\n",
    "        group_sorted = group.sort_values('Month')\n",
    "        \n",
    "        if len(group_sorted) >= sequence_length:\n",
    "            # Temporal sequence (first 12 months)\n",
    "            temporal_seq = group_sorted.iloc[:sequence_length][temporal_cols].values\n",
    "            \n",
    "            # Static features (same across all months, take first)\n",
    "            static_feat = group_sorted.iloc[0][static_cols].values\n",
    "            \n",
    "            # Target (annual yield)\n",
    "            target = group_sorted[target_col].sum()\n",
    "            \n",
    "            temporal_sequences.append(temporal_seq)\n",
    "            static_features.append(static_feat)\n",
    "            targets.append(target)\n",
    "    \n",
    "    return np.array(temporal_sequences), np.array(static_features), np.array(targets)\n",
    "\n",
    "X_hybrid_temp_train, X_hybrid_stat_train, y_hybrid_train = create_hybrid_sequences(\n",
    "    hybrid_train, hybrid_temporal_cols, hybrid_static_cols_full, target_col, sequence_length\n",
    ")\n",
    "X_hybrid_temp_val, X_hybrid_stat_val, y_hybrid_val = create_hybrid_sequences(\n",
    "    hybrid_val, hybrid_temporal_cols, hybrid_static_cols_full, target_col, sequence_length\n",
    ")\n",
    "X_hybrid_temp_test, X_hybrid_stat_test, y_hybrid_test = create_hybrid_sequences(\n",
    "    hybrid_test, hybrid_temporal_cols, hybrid_static_cols_full, target_col, sequence_length\n",
    ")\n",
    "\n",
    "print(f\"\\n  Train temporal: {X_hybrid_temp_train.shape}\")\n",
    "print(f\"  Train static:   {X_hybrid_stat_train.shape}\")\n",
    "print(f\"  Val temporal:   {X_hybrid_temp_val.shape}\")\n",
    "print(f\"  Test temporal:  {X_hybrid_temp_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47d07bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Normalizing Hybrid features...\n",
      "  âœ“ Features normalized\n"
     ]
    }
   ],
   "source": [
    "# Normalize Hybrid features\n",
    "print(\"\\n  Normalizing Hybrid features...\")\n",
    "\n",
    "# Temporal features\n",
    "scaler_hybrid_temp = StandardScaler()\n",
    "X_hybrid_temp_train_reshaped = X_hybrid_temp_train.reshape(-1, X_hybrid_temp_train.shape[2])\n",
    "scaler_hybrid_temp.fit(X_hybrid_temp_train_reshaped)\n",
    "\n",
    "X_hybrid_temp_train_scaled = scaler_hybrid_temp.transform(\n",
    "    X_hybrid_temp_train.reshape(-1, X_hybrid_temp_train.shape[2])\n",
    ").reshape(X_hybrid_temp_train.shape)\n",
    "X_hybrid_temp_val_scaled = scaler_hybrid_temp.transform(\n",
    "    X_hybrid_temp_val.reshape(-1, X_hybrid_temp_val.shape[2])\n",
    ").reshape(X_hybrid_temp_val.shape)\n",
    "X_hybrid_temp_test_scaled = scaler_hybrid_temp.transform(\n",
    "    X_hybrid_temp_test.reshape(-1, X_hybrid_temp_test.shape[2])\n",
    ").reshape(X_hybrid_temp_test.shape)\n",
    "\n",
    "# Static features\n",
    "scaler_hybrid_stat = StandardScaler()\n",
    "X_hybrid_stat_train_scaled = scaler_hybrid_stat.fit_transform(X_hybrid_stat_train)\n",
    "X_hybrid_stat_val_scaled = scaler_hybrid_stat.transform(X_hybrid_stat_val)\n",
    "X_hybrid_stat_test_scaled = scaler_hybrid_stat.transform(X_hybrid_stat_test)\n",
    "\n",
    "print(\"  âœ“ Features normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "876de97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Categorizing yields for Hybrid...\n",
      "\n",
      "Class distribution (Hybrid Train):\n",
      "  Class 0: 108 samples (33.3%)\n",
      "  Class 1: 108 samples (33.3%)\n",
      "  Class 2: 108 samples (33.3%)\n"
     ]
    }
   ],
   "source": [
    "# Categorize yields for Hybrid\n",
    "print(\"\\nğŸ“Š Categorizing yields for Hybrid...\")\n",
    "\n",
    "y_hybrid_train_cat, hybrid_percentiles = categorize_yield_balanced(y_hybrid_train, method='percentile')\n",
    "y_hybrid_val_cat, _ = categorize_yield_balanced(y_hybrid_val, method='percentile')\n",
    "y_hybrid_test_cat, _ = categorize_yield_balanced(y_hybrid_test, method='percentile')\n",
    "\n",
    "print(f\"\\nClass distribution (Hybrid Train):\")\n",
    "unique, counts = np.unique(y_hybrid_train_cat, return_counts=True)\n",
    "for cls, count in zip(unique, counts):\n",
    "    print(f\"  Class {cls}: {count} samples ({count/len(y_hybrid_train_cat)*100:.1f}%)\")\n",
    "\n",
    "y_hybrid_train_onehot = to_categorical(y_hybrid_train_cat, num_classes=3)\n",
    "y_hybrid_val_onehot = to_categorical(y_hybrid_val_cat, num_classes=3)\n",
    "y_hybrid_test_onehot = to_categorical(y_hybrid_test_cat, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb434406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ APPLYING DATA AUGMENTATION (Hybrid)...\n",
      "  Augmented Hybrid training size: 1296 samples\n",
      "\n",
      "âš–ï¸  Hybrid Class weights:\n",
      "   Class 0: weight=1.000 (n=432 samples)\n",
      "   Class 1: weight=1.000 (n=432 samples)\n",
      "   Class 2: weight=1.000 (n=432 samples)\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation for Hybrid\n",
    "print(\"\\nğŸ”„ APPLYING DATA AUGMENTATION (Hybrid)...\")\n",
    "\n",
    "X_hybrid_temp_aug, X_hybrid_stat_aug, y_hybrid_train_aug = augment_hybrid_data(\n",
    "    X_hybrid_temp_train_scaled,\n",
    "    X_hybrid_stat_train_scaled,\n",
    "    y_hybrid_train_onehot,\n",
    "    num_augmented=3,  # Increased from 1 to 3\n",
    "    noise_level=0.02  # Reduced noise\n",
    ")\n",
    "\n",
    "print(f\"  Augmented Hybrid training size: {X_hybrid_temp_aug.shape[0]} samples\")\n",
    "\n",
    "# Shuffle\n",
    "indices = np.random.permutation(X_hybrid_temp_aug.shape[0])\n",
    "X_hybrid_temp_aug = X_hybrid_temp_aug[indices]\n",
    "X_hybrid_stat_aug = X_hybrid_stat_aug[indices]\n",
    "y_hybrid_train_aug = y_hybrid_train_aug[indices]\n",
    "\n",
    "# Class weights\n",
    "y_hybrid_train_labels = np.argmax(y_hybrid_train_aug, axis=1)\n",
    "hybrid_class_weights_array = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_hybrid_train_labels),\n",
    "    y=y_hybrid_train_labels\n",
    ")\n",
    "hybrid_class_weight_dict = dict(enumerate(hybrid_class_weights_array))\n",
    "\n",
    "print(f\"\\nâš–ï¸  Hybrid Class weights:\")\n",
    "for cls, weight in hybrid_class_weight_dict.items():\n",
    "    count = np.sum(y_hybrid_train_labels == cls)\n",
    "    print(f\"   Class {cls}: weight={weight:.3f} (n={count} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e919e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ temporal_input      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,328</span> â”‚ temporal_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚ conv1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d_2     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_11          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ max_pooling1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> â”‚ dropout_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚ conv1d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ static_input        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_12          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> â”‚ static_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,560</span> â”‚ dropout_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚ conv1d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_15          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_13          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)        â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,384</span> â”‚ dropout_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_3     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)    â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">203,904</span> â”‚ dropout_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> â”‚ dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> â”‚ bidirectional_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_16          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_14          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,208</span> â”‚ dropout_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_4     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">99,072</span> â”‚ dropout_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚ dense_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚ bidirectional_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_17          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ concatenate         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       â”‚                   â”‚            â”‚ dropout_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">37,056</span> â”‚ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> â”‚ dense_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_18          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)        â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,528</span> â”‚ dropout_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> â”‚ dense_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_19          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)        â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,528</span> â”‚ dropout_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dropout_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ dense_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)        â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,656</span> â”‚ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> â”‚ dense_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_20          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">147</span> â”‚ dropout_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ temporal_input      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m17\u001b[0m)    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)    â”‚      \u001b[38;5;34m3,328\u001b[0m â”‚ temporal_input[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)    â”‚        \u001b[38;5;34m256\u001b[0m â”‚ conv1d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d_2     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_11          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚          \u001b[38;5;34m0\u001b[0m â”‚ max_pooling1d_2[\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mDropout\u001b[0m)           â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚     \u001b[38;5;34m24,704\u001b[0m â”‚ dropout_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚        \u001b[38;5;34m512\u001b[0m â”‚ conv1d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ static_input        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)        â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_12          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mDropout\u001b[0m)           â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_7 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚      \u001b[38;5;34m2,048\u001b[0m â”‚ static_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)    â”‚     \u001b[38;5;34m98,560\u001b[0m â”‚ dropout_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚        \u001b[38;5;34m512\u001b[0m â”‚ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)    â”‚      \u001b[38;5;34m1,024\u001b[0m â”‚ conv1d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_15          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mDropout\u001b[0m)           â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_13          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mDropout\u001b[0m)           â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_8 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)        â”‚     \u001b[38;5;34m12,384\u001b[0m â”‚ dropout_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_3     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m192\u001b[0m)    â”‚    \u001b[38;5;34m203,904\u001b[0m â”‚ dropout_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\n",
       "â”‚ (\u001b[38;5;33mBidirectional\u001b[0m)     â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)        â”‚        \u001b[38;5;34m384\u001b[0m â”‚ dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m192\u001b[0m)    â”‚        \u001b[38;5;34m768\u001b[0m â”‚ bidirectional_3[\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_16          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)        â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mDropout\u001b[0m)           â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_14          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m192\u001b[0m)    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mDropout\u001b[0m)           â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_9 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚      \u001b[38;5;34m6,208\u001b[0m â”‚ dropout_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_4     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚     \u001b[38;5;34m99,072\u001b[0m â”‚ dropout_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\n",
       "â”‚ (\u001b[38;5;33mBidirectional\u001b[0m)     â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚        \u001b[38;5;34m256\u001b[0m â”‚ dense_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚        \u001b[38;5;34m512\u001b[0m â”‚ bidirectional_4[\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_17          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mDropout\u001b[0m)           â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ concatenate         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mConcatenate\u001b[0m)       â”‚                   â”‚            â”‚ dropout_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_10 (\u001b[38;5;33mDense\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       â”‚     \u001b[38;5;34m37,056\u001b[0m â”‚ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       â”‚        \u001b[38;5;34m768\u001b[0m â”‚ dense_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_18          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mDropout\u001b[0m)           â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_11 (\u001b[38;5;33mDense\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)        â”‚     \u001b[38;5;34m18,528\u001b[0m â”‚ dropout_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)        â”‚        \u001b[38;5;34m384\u001b[0m â”‚ dense_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_19          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)        â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mDropout\u001b[0m)           â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_12 (\u001b[38;5;33mDense\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)        â”‚     \u001b[38;5;34m18,528\u001b[0m â”‚ dropout_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add (\u001b[38;5;33mAdd\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)        â”‚          \u001b[38;5;34m0\u001b[0m â”‚ dropout_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ dense_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_13 (\u001b[38;5;33mDense\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)        â”‚      \u001b[38;5;34m4,656\u001b[0m â”‚ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)        â”‚        \u001b[38;5;34m192\u001b[0m â”‚ dense_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_20          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)        â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mDropout\u001b[0m)           â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_14 (\u001b[38;5;33mDense\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         â”‚        \u001b[38;5;34m147\u001b[0m â”‚ dropout_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">534,691</span> (2.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m534,691\u001b[0m (2.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">531,907</span> (2.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m531,907\u001b[0m (2.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,784</span> (10.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,784\u001b[0m (10.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build Hybrid CNN-GRU model with IMPROVED architecture to fix class imbalance\n",
    "def build_hybrid_cnn_gru_model(sequence_length, n_temporal_features, n_static_features, learning_rate=0.0002):\n",
    "    \"\"\"\n",
    "    Build IMPROVED Hybrid CNN-GRU model to fix underperformance issues:\n",
    "    \n",
    "    FIXES IMPLEMENTED:\n",
    "    1. Reduced dropout to prevent over-regularization (was causing Medium bias)\n",
    "    2. Added residual connections for better gradient flow\n",
    "    3. Increased model capacity in fusion layers\n",
    "    4. Lower learning rate for more stable training\n",
    "    5. Stronger focal loss (gamma=3.0) to focus on hard examples (Low/High yields)\n",
    "    \n",
    "    Temporal Branch:\n",
    "    - CNN extracts patterns from monthly sequences\n",
    "    - GRU models temporal dependencies with residual connections\n",
    "    \n",
    "    Static Branch:\n",
    "    - Dense layers for soil and lag features\n",
    "    - Reduced dropout to retain more discriminative information\n",
    "    \n",
    "    Fusion:\n",
    "    - Enhanced capacity with residual connections\n",
    "    - Better integration of temporal + static features\n",
    "    \"\"\"\n",
    "    # Temporal input (CNN â†’ GRU)\n",
    "    temporal_input = layers.Input(shape=(sequence_length, n_temporal_features), name='temporal_input')\n",
    "    \n",
    "    # CNN layers with residual connections\n",
    "    x = layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same',\n",
    "                     kernel_regularizer=regularizers.l2(0.001))(temporal_input)  # Reduced regularization\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    cnn_skip1 = x  # Store for residual\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = layers.Dropout(0.4)(x)  # INCREASED to reduce overfitting\n",
    "    \n",
    "    x = layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same',\n",
    "                     kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)  # INCREASED to reduce overfitting\n",
    "    \n",
    "    x = layers.Conv1D(filters=256, kernel_size=3, activation='relu', padding='same',\n",
    "                     kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)  # Reduced from 0.5\n",
    "    \n",
    "    # Bidirectional GRU with anti-overfitting regularization\n",
    "    x = layers.Bidirectional(layers.GRU(96, return_sequences=True,\n",
    "                                       kernel_regularizer=regularizers.l2(0.01),  # INCREASED from 0.001\n",
    "                                       recurrent_regularizer=regularizers.l2(0.01),  # INCREASED from 0.001\n",
    "                                       dropout=0.3,  # INCREASED from 0.2\n",
    "                                       recurrent_dropout=0.3))(x)  # INCREASED from 0.2\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)  # INCREASED from 0.3\n",
    "    \n",
    "    x = layers.Bidirectional(layers.GRU(64, return_sequences=False,\n",
    "                                       kernel_regularizer=regularizers.l2(0.01),  # INCREASED from 0.001\n",
    "                                       recurrent_regularizer=regularizers.l2(0.01),  # INCREASED from 0.001\n",
    "                                       dropout=0.3,  # INCREASED from 0.2\n",
    "                                       recurrent_dropout=0.3))(x)  # INCREASED from 0.2\n",
    "    temporal_out = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Static input branch with anti-overfitting regularization\n",
    "    static_input = layers.Input(shape=(n_static_features,), name='static_input')\n",
    "    y = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(static_input)  # INCREASED from 0.001\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Dropout(0.4)(y)  # INCREASED from 0.3\n",
    "    static_skip1 = y  # Store for residual\n",
    "    \n",
    "    y = layers.Dense(96, activation='relu', kernel_regularizer=regularizers.l2(0.01))(y)  # INCREASED from 0.001\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Dropout(0.35)(y)  # INCREASED from 0.25\n",
    "    \n",
    "    y = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))(y)  # INCREASED from 0.001\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    static_out = layers.Dropout(0.3)(y)  # INCREASED from 0.2\n",
    "    \n",
    "    # Enhanced fusion layer with anti-overfitting regularization\n",
    "    merged = layers.concatenate([temporal_out, static_out])\n",
    "    \n",
    "    z = layers.Dense(192, activation='relu', kernel_regularizer=regularizers.l2(0.01))(merged)  # INCREASED from 0.001\n",
    "    z = layers.BatchNormalization()(z)\n",
    "    z = layers.Dropout(0.45)(z)  # INCREASED from 0.35\n",
    "    fusion_skip = z  # Store for residual\n",
    "    \n",
    "    z = layers.Dense(96, activation='relu', kernel_regularizer=regularizers.l2(0.01))(z)  # INCREASED from 0.001\n",
    "    z = layers.BatchNormalization()(z)\n",
    "    z = layers.Dropout(0.4)(z)  # INCREASED from 0.3\n",
    "    \n",
    "    # Residual connection\n",
    "    z_reshaped = layers.Dense(96, kernel_regularizer=regularizers.l2(0.01))(fusion_skip)  # Match dimensions\n",
    "    z = layers.Add()([z, z_reshaped])  # Add residual\n",
    "    \n",
    "    z = layers.Dense(48, activation='relu', kernel_regularizer=regularizers.l2(0.01))(z)  # INCREASED from 0.001\n",
    "    z = layers.BatchNormalization()(z)\n",
    "    z = layers.Dropout(0.35)(z)  # INCREASED from 0.25\n",
    "    \n",
    "    # Output layer\n",
    "    output = layers.Dense(3, activation='softmax')(z)\n",
    "    \n",
    "    model = models.Model(inputs=[temporal_input, static_input], outputs=output)\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate, clipnorm=1.0)\n",
    "    \n",
    "    # CRITICAL FIX: Stronger focal loss to combat class imbalance\n",
    "    # gamma=3.0 focuses more on hard-to-classify examples (Low/High yields)\n",
    "    # alpha=0.3 increased to give more weight to minority classes\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=focal_loss(gamma=3.0, alpha=0.3),  # Increased from gamma=2.0, alpha=0.25\n",
    "        metrics=['accuracy',\n",
    "                 tf.keras.metrics.Precision(name='precision'),\n",
    "                 tf.keras.metrics.Recall(name='recall')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "hybrid_model = build_hybrid_cnn_gru_model(\n",
    "    sequence_length=sequence_length,\n",
    "    n_temporal_features=X_hybrid_temp_aug.shape[2],\n",
    "    n_static_features=X_hybrid_stat_aug.shape[1],\n",
    "    learning_rate=0.0002  # Reduced from 0.0003 for more stable training\n",
    ")\n",
    "hybrid_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "083d407f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Training IMPROVED Hybrid CNN-GRU model...\n",
      "ANTI-OVERFITTING IMPROVEMENTS:\n",
      "  â€¢ INCREASED L2 regularization (0.001 â†’ 0.01) to prevent memorization\n",
      "  â€¢ INCREASED dropout (0.2-0.35 â†’ 0.3-0.45) for better generalization\n",
      "  â€¢ Added residual connections for gradient flow\n",
      "  â€¢ Stronger focal loss (gamma=3.0) for hard examples\n",
      "  â€¢ Lower learning rate (0.0002) for stability\n",
      "  â€¢ Enhanced class weights for Low/High yield detection\n",
      "\n",
      "Amplified Class Weights (to combat Medium bias):\n",
      "  Class 0 (Low): 1.5000\n",
      "  Class 1 (Medium): 0.7000\n",
      "  Class 2 (High): 1.5000\n",
      "Epoch 1/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.3141 - loss: 20.0645 - precision: 0.3145 - recall: 0.2617\n",
      "Epoch 1: val_accuracy improved from -inf to 0.33333\n",
      "  Saved model to models\\hybrid_best.weights.h5\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 168ms/step - accuracy: 0.3426 - loss: 19.6651 - precision: 0.3475 - recall: 0.2909 - val_accuracy: 0.3333 - val_loss: 18.5987 - val_precision: 0.3333 - val_recall: 0.1111 - learning_rate: 2.0000e-04\n",
      "Epoch 2/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 63ms/step - accuracy: 0.4028 - loss: 18.1285 - precision: 0.4135 - recall: 0.3356 - val_accuracy: 0.3333 - val_loss: 17.2197 - val_precision: 0.3778 - val_recall: 0.3148 - learning_rate: 2.0000e-04\n",
      "Epoch 3/300\n",
      "\u001b[1m53/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.4624 - loss: 17.0679 - precision: 0.4658 - recall: 0.3786\n",
      "Epoch 3: val_accuracy improved from 0.33333 to 0.48148\n",
      "  Saved model to models\\hybrid_best.weights.h5\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 72ms/step - accuracy: 0.4715 - loss: 16.7463 - precision: 0.4885 - recall: 0.4082 - val_accuracy: 0.4815 - val_loss: 15.9286 - val_precision: 0.4524 - val_recall: 0.3519 - learning_rate: 2.0000e-04\n",
      "Epoch 4/300\n",
      "\u001b[1m53/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.4762 - loss: 15.8109 - precision: 0.4913 - recall: 0.4211\n",
      "Epoch 4: val_accuracy improved from 0.48148 to 0.64815\n",
      "  Saved model to models\\hybrid_best.weights.h5\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 86ms/step - accuracy: 0.5039 - loss: 15.5138 - precision: 0.5207 - recall: 0.4468 - val_accuracy: 0.6481 - val_loss: 14.7613 - val_precision: 0.6471 - val_recall: 0.6111 - learning_rate: 2.0000e-04\n",
      "Epoch 5/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - accuracy: 0.5756 - loss: 14.3282 - precision: 0.5936 - recall: 0.5139 - val_accuracy: 0.5185 - val_loss: 13.6291 - val_precision: 0.5102 - val_recall: 0.4630 - learning_rate: 2.0000e-04\n",
      "Epoch 6/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.6019 - loss: 13.2219 - precision: 0.6373 - recall: 0.5478 - val_accuracy: 0.5741 - val_loss: 12.5889 - val_precision: 0.5600 - val_recall: 0.5185 - learning_rate: 2.0000e-04\n",
      "Epoch 7/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 50ms/step - accuracy: 0.6605 - loss: 12.2065 - precision: 0.6845 - recall: 0.6111 - val_accuracy: 0.5556 - val_loss: 11.6573 - val_precision: 0.5294 - val_recall: 0.5000 - learning_rate: 2.0000e-04\n",
      "Epoch 8/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - accuracy: 0.6921 - loss: 11.2735 - precision: 0.7248 - recall: 0.6543 - val_accuracy: 0.5741 - val_loss: 10.7777 - val_precision: 0.5600 - val_recall: 0.5185 - learning_rate: 2.0000e-04\n",
      "Epoch 9/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 61ms/step - accuracy: 0.7284 - loss: 10.4031 - precision: 0.7544 - recall: 0.6875 - val_accuracy: 0.6296 - val_loss: 9.9708 - val_precision: 0.6154 - val_recall: 0.5926 - learning_rate: 2.0000e-04\n",
      "Epoch 10/300\n",
      "\u001b[1m53/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.7333 - loss: 9.7957 - precision: 0.7514 - recall: 0.6979\n",
      "Epoch 10: val_accuracy improved from 0.64815 to 0.66667\n",
      "  Saved model to models\\hybrid_best.weights.h5\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 68ms/step - accuracy: 0.7585 - loss: 9.5973 - precision: 0.7769 - recall: 0.7199 - val_accuracy: 0.6667 - val_loss: 9.2140 - val_precision: 0.6604 - val_recall: 0.6481 - learning_rate: 2.0000e-04\n",
      "Epoch 11/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.7565 - loss: 9.0456 - precision: 0.7747 - recall: 0.7168\n",
      "Epoch 11: val_accuracy improved from 0.66667 to 0.68519\n",
      "  Saved model to models\\hybrid_best.weights.h5\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 94ms/step - accuracy: 0.7716 - loss: 8.8642 - precision: 0.7890 - recall: 0.7330 - val_accuracy: 0.6852 - val_loss: 8.5170 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 12/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7702 - loss: 8.3399 - precision: 0.7857 - recall: 0.7426\n",
      "Epoch 12: val_accuracy improved from 0.68519 to 0.70370\n",
      "  Saved model to models\\hybrid_best.weights.h5\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 79ms/step - accuracy: 0.8002 - loss: 8.1748 - precision: 0.8120 - recall: 0.7701 - val_accuracy: 0.7037 - val_loss: 7.8571 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 2.0000e-04\n",
      "Epoch 13/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - accuracy: 0.8194 - loss: 7.5339 - precision: 0.8337 - recall: 0.7971 - val_accuracy: 0.7037 - val_loss: 7.2698 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 2.0000e-04\n",
      "Epoch 14/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 63ms/step - accuracy: 0.8140 - loss: 6.9489 - precision: 0.8335 - recall: 0.7878 - val_accuracy: 0.7037 - val_loss: 6.7168 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 2.0000e-04\n",
      "Epoch 15/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step - accuracy: 0.8279 - loss: 6.4087 - precision: 0.8457 - recall: 0.7994 - val_accuracy: 0.7037 - val_loss: 6.1914 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 2.0000e-04\n",
      "Epoch 16/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 48ms/step - accuracy: 0.8333 - loss: 5.9170 - precision: 0.8552 - recall: 0.8156 - val_accuracy: 0.6667 - val_loss: 5.7182 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 17/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - accuracy: 0.8526 - loss: 5.4504 - precision: 0.8677 - recall: 0.8302 - val_accuracy: 0.6667 - val_loss: 5.2956 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 18/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - accuracy: 0.8603 - loss: 5.0262 - precision: 0.8741 - recall: 0.8356 - val_accuracy: 0.6667 - val_loss: 4.8876 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 19/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 63ms/step - accuracy: 0.8773 - loss: 4.6301 - precision: 0.8930 - recall: 0.8565 - val_accuracy: 0.6667 - val_loss: 4.5079 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 20/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.8634 - loss: 4.2717 - precision: 0.8722 - recall: 0.8480 - val_accuracy: 0.6667 - val_loss: 4.1695 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 21/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.8796 - loss: 3.9325 - precision: 0.8883 - recall: 0.8588 - val_accuracy: 0.6667 - val_loss: 3.8468 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 22/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step - accuracy: 0.8789 - loss: 3.6286 - precision: 0.8862 - recall: 0.8596 - val_accuracy: 0.6111 - val_loss: 3.5682 - val_precision: 0.6111 - val_recall: 0.6111 - learning_rate: 2.0000e-04\n",
      "Epoch 23/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 60ms/step - accuracy: 0.8897 - loss: 3.3409 - precision: 0.9007 - recall: 0.8750 - val_accuracy: 0.6667 - val_loss: 3.2908 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 24/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.8750 - loss: 3.0816 - precision: 0.8882 - recall: 0.8580 - val_accuracy: 0.6111 - val_loss: 3.0414 - val_precision: 0.6111 - val_recall: 0.6111 - learning_rate: 2.0000e-04\n",
      "Epoch 25/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 66ms/step - accuracy: 0.9012 - loss: 2.8392 - precision: 0.9100 - recall: 0.8812 - val_accuracy: 0.6667 - val_loss: 2.8198 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 26/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.8997 - loss: 2.6163 - precision: 0.9131 - recall: 0.8843 - val_accuracy: 0.6667 - val_loss: 2.5895 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 27/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 54ms/step - accuracy: 0.8981 - loss: 2.4103 - precision: 0.9088 - recall: 0.8843 - val_accuracy: 0.6667 - val_loss: 2.4105 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 28/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - accuracy: 0.8889 - loss: 2.2267 - precision: 0.8984 - recall: 0.8735 - val_accuracy: 0.6667 - val_loss: 2.2092 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 29/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.9113 - loss: 2.0471 - precision: 0.9199 - recall: 0.8951 - val_accuracy: 0.6667 - val_loss: 2.0456 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 30/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 49ms/step - accuracy: 0.9012 - loss: 1.8934 - precision: 0.9098 - recall: 0.8873 - val_accuracy: 0.6667 - val_loss: 1.9051 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 31/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 54ms/step - accuracy: 0.9012 - loss: 1.7453 - precision: 0.9119 - recall: 0.8943 - val_accuracy: 0.6667 - val_loss: 1.7694 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 32/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 54ms/step - accuracy: 0.9167 - loss: 1.6081 - precision: 0.9201 - recall: 0.9059 - val_accuracy: 0.6667 - val_loss: 1.6369 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 33/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.9051 - loss: 1.4843 - precision: 0.9214 - recall: 0.8951 - val_accuracy: 0.6667 - val_loss: 1.5145 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 34/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.9097 - loss: 1.3719 - precision: 0.9167 - recall: 0.8997 - val_accuracy: 0.6667 - val_loss: 1.4050 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 35/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.9035 - loss: 1.2660 - precision: 0.9100 - recall: 0.8889 - val_accuracy: 0.6667 - val_loss: 1.2740 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 36/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.9174 - loss: 1.1700 - precision: 0.9237 - recall: 0.9066 - val_accuracy: 0.6667 - val_loss: 1.1857 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 37/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.9136 - loss: 1.0849 - precision: 0.9222 - recall: 0.9059 - val_accuracy: 0.6667 - val_loss: 1.0834 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 38/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.9144 - loss: 1.0048 - precision: 0.9200 - recall: 0.9051 - val_accuracy: 0.6667 - val_loss: 1.0358 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 39/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.9128 - loss: 0.9307 - precision: 0.9201 - recall: 0.9059 - val_accuracy: 0.6667 - val_loss: 0.9562 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 40/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.9151 - loss: 0.8635 - precision: 0.9209 - recall: 0.9074 - val_accuracy: 0.6667 - val_loss: 0.9481 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 41/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.9167 - loss: 0.8019 - precision: 0.9219 - recall: 0.9113 - val_accuracy: 0.6667 - val_loss: 0.8584 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 42/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.9213 - loss: 0.7410 - precision: 0.9281 - recall: 0.9159 - val_accuracy: 0.6667 - val_loss: 0.7682 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 43/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - accuracy: 0.9128 - loss: 0.6947 - precision: 0.9224 - recall: 0.9074 - val_accuracy: 0.6667 - val_loss: 0.7654 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 44/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - accuracy: 0.9205 - loss: 0.6430 - precision: 0.9262 - recall: 0.9097 - val_accuracy: 0.6667 - val_loss: 0.6986 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 45/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 68ms/step - accuracy: 0.9213 - loss: 0.6001 - precision: 0.9287 - recall: 0.9144 - val_accuracy: 0.6667 - val_loss: 0.6339 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 46/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - accuracy: 0.9174 - loss: 0.5617 - precision: 0.9205 - recall: 0.9113 - val_accuracy: 0.6667 - val_loss: 0.6073 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 47/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - accuracy: 0.9221 - loss: 0.5229 - precision: 0.9275 - recall: 0.9182 - val_accuracy: 0.6667 - val_loss: 0.5776 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 48/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9252 - loss: 0.4898 - precision: 0.9311 - recall: 0.9182 - val_accuracy: 0.6667 - val_loss: 0.5904 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 49/300\n",
      "\u001b[1m53/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9180 - loss: 0.4708 - precision: 0.9236 - recall: 0.9104\n",
      "Epoch 49: val_accuracy improved from 0.70370 to 0.72222\n",
      "  Saved model to models\\hybrid_best.weights.h5\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 64ms/step - accuracy: 0.9321 - loss: 0.4587 - precision: 0.9357 - recall: 0.9213 - val_accuracy: 0.7222 - val_loss: 0.4832 - val_precision: 0.7222 - val_recall: 0.7222 - learning_rate: 2.0000e-04\n",
      "Epoch 50/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - accuracy: 0.9259 - loss: 0.4287 - precision: 0.9319 - recall: 0.9182 - val_accuracy: 0.6667 - val_loss: 0.4848 - val_precision: 0.6604 - val_recall: 0.6481 - learning_rate: 2.0000e-04\n",
      "Epoch 51/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.9329 - loss: 0.4007 - precision: 0.9412 - recall: 0.9259 - val_accuracy: 0.6667 - val_loss: 0.4529 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 52/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.9360 - loss: 0.3777 - precision: 0.9369 - recall: 0.9282 - val_accuracy: 0.6667 - val_loss: 0.4296 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 53/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - accuracy: 0.9313 - loss: 0.3557 - precision: 0.9375 - recall: 0.9259 - val_accuracy: 0.6667 - val_loss: 0.4789 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 54/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.9321 - loss: 0.3366 - precision: 0.9368 - recall: 0.9267 - val_accuracy: 0.6667 - val_loss: 0.4708 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 55/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.9298 - loss: 0.3169 - precision: 0.9336 - recall: 0.9221 - val_accuracy: 0.6667 - val_loss: 0.4221 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 56/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.9290 - loss: 0.3006 - precision: 0.9375 - recall: 0.9252 - val_accuracy: 0.6667 - val_loss: 0.3625 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 57/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.9344 - loss: 0.2821 - precision: 0.9395 - recall: 0.9221 - val_accuracy: 0.6667 - val_loss: 0.3575 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 58/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - accuracy: 0.9421 - loss: 0.2669 - precision: 0.9469 - recall: 0.9360 - val_accuracy: 0.6667 - val_loss: 0.3278 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 59/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.9429 - loss: 0.2526 - precision: 0.9457 - recall: 0.9406 - val_accuracy: 0.6667 - val_loss: 0.3344 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 60/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - accuracy: 0.9406 - loss: 0.2391 - precision: 0.9417 - recall: 0.9352 - val_accuracy: 0.6667 - val_loss: 0.3489 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 61/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - accuracy: 0.9390 - loss: 0.2285 - precision: 0.9419 - recall: 0.9375 - val_accuracy: 0.6667 - val_loss: 0.2914 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 62/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 0.9414 - loss: 0.2158 - precision: 0.9462 - recall: 0.9367 - val_accuracy: 0.6667 - val_loss: 0.2897 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 63/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.9483 - loss: 0.2049 - precision: 0.9532 - recall: 0.9429 - val_accuracy: 0.6667 - val_loss: 0.2924 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 64/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.9414 - loss: 0.1967 - precision: 0.9441 - recall: 0.9383 - val_accuracy: 0.6481 - val_loss: 0.4048 - val_precision: 0.6604 - val_recall: 0.6481 - learning_rate: 2.0000e-04\n",
      "Epoch 65/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.9336 - loss: 0.1911 - precision: 0.9361 - recall: 0.9267 - val_accuracy: 0.6667 - val_loss: 0.3476 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 66/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.9568 - loss: 0.1796 - precision: 0.9589 - recall: 0.9537 - val_accuracy: 0.6852 - val_loss: 0.2485 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 67/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.9367 - loss: 0.1701 - precision: 0.9408 - recall: 0.9321 - val_accuracy: 0.6667 - val_loss: 0.3179 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 68/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.9506 - loss: 0.1614 - precision: 0.9534 - recall: 0.9475 - val_accuracy: 0.6667 - val_loss: 0.3317 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 69/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - accuracy: 0.9537 - loss: 0.1564 - precision: 0.9571 - recall: 0.9475 - val_accuracy: 0.6852 - val_loss: 0.2924 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 70/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - accuracy: 0.9583 - loss: 0.1489 - precision: 0.9611 - recall: 0.9537 - val_accuracy: 0.6667 - val_loss: 0.3732 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 71/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.9282 - loss: 0.1465 - precision: 0.9356 - recall: 0.9198 - val_accuracy: 0.7037 - val_loss: 0.2373 - val_precision: 0.6981 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 72/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.9560 - loss: 0.1384 - precision: 0.9603 - recall: 0.9522 - val_accuracy: 0.6852 - val_loss: 0.2735 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 73/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.9545 - loss: 0.1332 - precision: 0.9594 - recall: 0.9475 - val_accuracy: 0.6667 - val_loss: 0.3299 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 74/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.9444 - loss: 0.1297 - precision: 0.9501 - recall: 0.9398 - val_accuracy: 0.6852 - val_loss: 0.1752 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 75/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.9576 - loss: 0.1222 - precision: 0.9618 - recall: 0.9522 - val_accuracy: 0.7222 - val_loss: 0.1725 - val_precision: 0.7222 - val_recall: 0.7222 - learning_rate: 2.0000e-04\n",
      "Epoch 76/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.9537 - loss: 0.1195 - precision: 0.9558 - recall: 0.9522 - val_accuracy: 0.6852 - val_loss: 0.2655 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 77/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.9583 - loss: 0.1118 - precision: 0.9612 - recall: 0.9568 - val_accuracy: 0.6667 - val_loss: 0.2977 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 78/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.9599 - loss: 0.1086 - precision: 0.9643 - recall: 0.9576 - val_accuracy: 0.6667 - val_loss: 0.3029 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 79/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.9568 - loss: 0.1051 - precision: 0.9596 - recall: 0.9529 - val_accuracy: 0.6852 - val_loss: 0.2013 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 80/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.9560 - loss: 0.1024 - precision: 0.9573 - recall: 0.9514 - val_accuracy: 0.6852 - val_loss: 0.2070 - val_precision: 0.6923 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 81/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.9298 - loss: 0.1058 - precision: 0.9359 - recall: 0.9244 - val_accuracy: 0.6852 - val_loss: 0.4319 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 82/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.9568 - loss: 0.0969 - precision: 0.9604 - recall: 0.9537 - val_accuracy: 0.6852 - val_loss: 0.2533 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 83/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - accuracy: 0.9668 - loss: 0.0888 - precision: 0.9705 - recall: 0.9660 - val_accuracy: 0.6852 - val_loss: 0.2220 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 84/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.9630 - loss: 0.0872 - precision: 0.9643 - recall: 0.9599 - val_accuracy: 0.6852 - val_loss: 0.1686 - val_precision: 0.6981 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 85/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.9529 - loss: 0.0843 - precision: 0.9549 - recall: 0.9475 - val_accuracy: 0.6667 - val_loss: 0.2022 - val_precision: 0.6604 - val_recall: 0.6481 - learning_rate: 2.0000e-04\n",
      "Epoch 86/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.9522 - loss: 0.0858 - precision: 0.9548 - recall: 0.9444 - val_accuracy: 0.6852 - val_loss: 0.1940 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 87/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.9606 - loss: 0.0792 - precision: 0.9635 - recall: 0.9568 - val_accuracy: 0.7222 - val_loss: 0.1667 - val_precision: 0.7222 - val_recall: 0.7222 - learning_rate: 2.0000e-04\n",
      "Epoch 88/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.9645 - loss: 0.0755 - precision: 0.9682 - recall: 0.9622 - val_accuracy: 0.6852 - val_loss: 0.1607 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 89/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.9591 - loss: 0.0757 - precision: 0.9626 - recall: 0.9529 - val_accuracy: 0.7037 - val_loss: 0.1910 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 2.0000e-04\n",
      "Epoch 90/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.9506 - loss: 0.0784 - precision: 0.9556 - recall: 0.9468 - val_accuracy: 0.6667 - val_loss: 0.5779 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 91/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - accuracy: 0.9514 - loss: 0.0749 - precision: 0.9548 - recall: 0.9452 - val_accuracy: 0.7037 - val_loss: 0.2189 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 2.0000e-04\n",
      "Epoch 92/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.9552 - loss: 0.0702 - precision: 0.9581 - recall: 0.9537 - val_accuracy: 0.6852 - val_loss: 0.2797 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 93/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.9622 - loss: 0.0672 - precision: 0.9641 - recall: 0.9545 - val_accuracy: 0.7222 - val_loss: 0.1269 - val_precision: 0.7222 - val_recall: 0.7222 - learning_rate: 2.0000e-04\n",
      "Epoch 94/300\n",
      "\u001b[1m53/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9554 - loss: 0.0662 - precision: 0.9606 - recall: 0.9508\n",
      "Epoch 94: val_accuracy improved from 0.72222 to 0.75926\n",
      "  Saved model to models\\hybrid_best.weights.h5\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 59ms/step - accuracy: 0.9560 - loss: 0.0657 - precision: 0.9610 - recall: 0.9498 - val_accuracy: 0.7593 - val_loss: 0.1084 - val_precision: 0.7593 - val_recall: 0.7593 - learning_rate: 2.0000e-04\n",
      "Epoch 95/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - accuracy: 0.9684 - loss: 0.0622 - precision: 0.9697 - recall: 0.9645 - val_accuracy: 0.6111 - val_loss: 0.3441 - val_precision: 0.6111 - val_recall: 0.6111 - learning_rate: 2.0000e-04\n",
      "Epoch 96/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.9506 - loss: 0.0660 - precision: 0.9577 - recall: 0.9429 - val_accuracy: 0.7037 - val_loss: 0.2204 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 2.0000e-04\n",
      "Epoch 97/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.9668 - loss: 0.0611 - precision: 0.9689 - recall: 0.9630 - val_accuracy: 0.6852 - val_loss: 0.2230 - val_precision: 0.6792 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 98/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.9676 - loss: 0.0567 - precision: 0.9675 - recall: 0.9645 - val_accuracy: 0.6852 - val_loss: 0.1283 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 99/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.9699 - loss: 0.0552 - precision: 0.9720 - recall: 0.9660 - val_accuracy: 0.7037 - val_loss: 0.0711 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 2.0000e-04\n",
      "Epoch 100/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.9591 - loss: 0.0638 - precision: 0.9650 - recall: 0.9583 - val_accuracy: 0.7037 - val_loss: 0.2749 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 2.0000e-04\n",
      "Epoch 101/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.9560 - loss: 0.0604 - precision: 0.9609 - recall: 0.9491 - val_accuracy: 0.7037 - val_loss: 0.3049 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 2.0000e-04\n",
      "Epoch 102/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.9707 - loss: 0.0540 - precision: 0.9774 - recall: 0.9668 - val_accuracy: 0.7037 - val_loss: 0.2437 - val_precision: 0.6981 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 103/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.9707 - loss: 0.0522 - precision: 0.9728 - recall: 0.9676 - val_accuracy: 0.7037 - val_loss: 0.1564 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 2.0000e-04\n",
      "Epoch 104/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.9637 - loss: 0.0503 - precision: 0.9643 - recall: 0.9599 - val_accuracy: 0.6111 - val_loss: 0.1750 - val_precision: 0.6667 - val_recall: 0.5926 - learning_rate: 2.0000e-04\n",
      "Epoch 105/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.9437 - loss: 0.0556 - precision: 0.9509 - recall: 0.9406 - val_accuracy: 0.6667 - val_loss: 0.3634 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 106/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.9707 - loss: 0.0496 - precision: 0.9713 - recall: 0.9668 - val_accuracy: 0.6852 - val_loss: 0.2530 - val_precision: 0.6981 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 107/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - accuracy: 0.9645 - loss: 0.0491 - precision: 0.9652 - recall: 0.9622 - val_accuracy: 0.6852 - val_loss: 0.1806 - val_precision: 0.6981 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 108/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.9506 - loss: 0.0553 - precision: 0.9540 - recall: 0.9444 - val_accuracy: 0.6667 - val_loss: 0.3007 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 109/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.9691 - loss: 0.0473 - precision: 0.9705 - recall: 0.9660 - val_accuracy: 0.6852 - val_loss: 0.1360 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 110/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.9745 - loss: 0.0433 - precision: 0.9753 - recall: 0.9738 - val_accuracy: 0.6852 - val_loss: 0.0736 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 111/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.9769 - loss: 0.0409 - precision: 0.9783 - recall: 0.9753 - val_accuracy: 0.6852 - val_loss: 0.0685 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 112/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.9637 - loss: 0.0417 - precision: 0.9689 - recall: 0.9630 - val_accuracy: 0.7037 - val_loss: 0.0607 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 2.0000e-04\n",
      "Epoch 113/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.9653 - loss: 0.0448 - precision: 0.9697 - recall: 0.9622 - val_accuracy: 0.6852 - val_loss: 0.2297 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 114/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - accuracy: 0.9645 - loss: 0.0412 - precision: 0.9682 - recall: 0.9622 - val_accuracy: 0.6667 - val_loss: 0.0773 - val_precision: 0.6604 - val_recall: 0.6481 - learning_rate: 2.0000e-04\n",
      "Epoch 115/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - accuracy: 0.9537 - loss: 0.0464 - precision: 0.9543 - recall: 0.9506 - val_accuracy: 0.6667 - val_loss: 0.5785 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 116/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - accuracy: 0.9676 - loss: 0.0433 - precision: 0.9697 - recall: 0.9645 - val_accuracy: 0.6667 - val_loss: 0.1991 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 117/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - accuracy: 0.9630 - loss: 0.0410 - precision: 0.9644 - recall: 0.9614 - val_accuracy: 0.6852 - val_loss: 0.0749 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 118/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.9545 - loss: 0.0432 - precision: 0.9609 - recall: 0.9483 - val_accuracy: 0.6852 - val_loss: 0.1230 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 119/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.9769 - loss: 0.0383 - precision: 0.9775 - recall: 0.9738 - val_accuracy: 0.7037 - val_loss: 0.1199 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 2.0000e-04\n",
      "Epoch 120/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.9645 - loss: 0.0391 - precision: 0.9667 - recall: 0.9622 - val_accuracy: 0.7037 - val_loss: 0.0923 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 2.0000e-04\n",
      "Epoch 121/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.9753 - loss: 0.0359 - precision: 0.9775 - recall: 0.9707 - val_accuracy: 0.7037 - val_loss: 0.0537 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 2.0000e-04\n",
      "Epoch 122/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - accuracy: 0.9653 - loss: 0.0383 - precision: 0.9682 - recall: 0.9645 - val_accuracy: 0.6667 - val_loss: 0.1386 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 123/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.9730 - loss: 0.0357 - precision: 0.9774 - recall: 0.9684 - val_accuracy: 0.7037 - val_loss: 0.0996 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 2.0000e-04\n",
      "Epoch 124/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.9645 - loss: 0.0362 - precision: 0.9666 - recall: 0.9614 - val_accuracy: 0.7222 - val_loss: 0.0859 - val_precision: 0.7222 - val_recall: 0.7222 - learning_rate: 2.0000e-04\n",
      "Epoch 125/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.9707 - loss: 0.0361 - precision: 0.9714 - recall: 0.9691 - val_accuracy: 0.7222 - val_loss: 0.1587 - val_precision: 0.7222 - val_recall: 0.7222 - learning_rate: 2.0000e-04\n",
      "Epoch 126/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - accuracy: 0.9668 - loss: 0.0350 - precision: 0.9675 - recall: 0.9645 - val_accuracy: 0.6852 - val_loss: 0.1989 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 127/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.9668 - loss: 0.0364 - precision: 0.9697 - recall: 0.9630 - val_accuracy: 0.6667 - val_loss: 0.2667 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 128/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.9776 - loss: 0.0329 - precision: 0.9784 - recall: 0.9769 - val_accuracy: 0.6852 - val_loss: 0.1652 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 129/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - accuracy: 0.9699 - loss: 0.0358 - precision: 0.9698 - recall: 0.9668 - val_accuracy: 0.6667 - val_loss: 0.2522 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-04\n",
      "Epoch 130/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.9653 - loss: 0.0349 - precision: 0.9689 - recall: 0.9614 - val_accuracy: 0.7037 - val_loss: 0.0673 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 2.0000e-04\n",
      "Epoch 131/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.9645 - loss: 0.0327 - precision: 0.9666 - recall: 0.9614 - val_accuracy: 0.7593 - val_loss: 0.1003 - val_precision: 0.7593 - val_recall: 0.7593 - learning_rate: 2.0000e-04\n",
      "Epoch 132/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.9684 - loss: 0.0324 - precision: 0.9705 - recall: 0.9660 - val_accuracy: 0.6852 - val_loss: 0.1646 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 133/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.9653 - loss: 0.0334 - precision: 0.9674 - recall: 0.9630 - val_accuracy: 0.7407 - val_loss: 0.0545 - val_precision: 0.7547 - val_recall: 0.7407 - learning_rate: 2.0000e-04\n",
      "Epoch 134/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.9715 - loss: 0.0297 - precision: 0.9729 - recall: 0.9691 - val_accuracy: 0.6852 - val_loss: 0.1291 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 135/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.9738 - loss: 0.0279 - precision: 0.9745 - recall: 0.9722 - val_accuracy: 0.6852 - val_loss: 0.0715 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 2.0000e-04\n",
      "Epoch 136/300\n",
      "\u001b[1m53/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9767 - loss: 0.0261 - precision: 0.9767 - recall: 0.9767\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 0.00011999999696854502.\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.9738 - loss: 0.0262 - precision: 0.9738 - recall: 0.9738 - val_accuracy: 0.7037 - val_loss: 0.0976 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 2.0000e-04\n",
      "Epoch 137/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - accuracy: 0.9745 - loss: 0.0271 - precision: 0.9753 - recall: 0.9738 - val_accuracy: 0.7037 - val_loss: 0.0972 - val_precision: 0.7170 - val_recall: 0.7037 - learning_rate: 1.2000e-04\n",
      "Epoch 138/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - accuracy: 0.9784 - loss: 0.0263 - precision: 0.9791 - recall: 0.9753 - val_accuracy: 0.6852 - val_loss: 0.1082 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 1.2000e-04\n",
      "Epoch 139/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - accuracy: 0.9861 - loss: 0.0241 - precision: 0.9868 - recall: 0.9838 - val_accuracy: 0.6852 - val_loss: 0.1193 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 1.2000e-04\n",
      "Epoch 140/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.9761 - loss: 0.0259 - precision: 0.9806 - recall: 0.9738 - val_accuracy: 0.6852 - val_loss: 0.2104 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 1.2000e-04\n",
      "Epoch 141/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.9722 - loss: 0.0314 - precision: 0.9736 - recall: 0.9684 - val_accuracy: 0.7037 - val_loss: 0.1563 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 1.2000e-04\n",
      "Epoch 142/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.9877 - loss: 0.0257 - precision: 0.9876 - recall: 0.9869 - val_accuracy: 0.7222 - val_loss: 0.0679 - val_precision: 0.7222 - val_recall: 0.7222 - learning_rate: 1.2000e-04\n",
      "Epoch 143/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.9869 - loss: 0.0239 - precision: 0.9884 - recall: 0.9853 - val_accuracy: 0.7037 - val_loss: 0.0991 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 1.2000e-04\n",
      "Epoch 144/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.9830 - loss: 0.0241 - precision: 0.9845 - recall: 0.9815 - val_accuracy: 0.7593 - val_loss: 0.0604 - val_precision: 0.7593 - val_recall: 0.7593 - learning_rate: 1.2000e-04\n",
      "Epoch 145/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.9838 - loss: 0.0229 - precision: 0.9845 - recall: 0.9799 - val_accuracy: 0.7037 - val_loss: 0.1426 - val_precision: 0.6981 - val_recall: 0.6852 - learning_rate: 1.2000e-04\n",
      "Epoch 146/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.9815 - loss: 0.0223 - precision: 0.9822 - recall: 0.9799 - val_accuracy: 0.7037 - val_loss: 0.1439 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 1.2000e-04\n",
      "Epoch 147/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.9907 - loss: 0.0209 - precision: 0.9907 - recall: 0.9892 - val_accuracy: 0.6852 - val_loss: 0.1090 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 1.2000e-04\n",
      "Epoch 148/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.9923 - loss: 0.0208 - precision: 0.9923 - recall: 0.9915 - val_accuracy: 0.7222 - val_loss: 0.1364 - val_precision: 0.7222 - val_recall: 0.7222 - learning_rate: 1.2000e-04\n",
      "Epoch 149/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step - accuracy: 0.9853 - loss: 0.0204 - precision: 0.9869 - recall: 0.9846 - val_accuracy: 0.7407 - val_loss: 0.0610 - val_precision: 0.7358 - val_recall: 0.7222 - learning_rate: 1.2000e-04\n",
      "Epoch 150/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.9722 - loss: 0.0245 - precision: 0.9737 - recall: 0.9707 - val_accuracy: 0.6667 - val_loss: 0.4103 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 1.2000e-04\n",
      "Epoch 151/300\n",
      "\u001b[1m53/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9621 - loss: 0.0271 - precision: 0.9684 - recall: 0.9600\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 7.199999818112701e-05.\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - accuracy: 0.9691 - loss: 0.0263 - precision: 0.9728 - recall: 0.9676 - val_accuracy: 0.6667 - val_loss: 0.2817 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 1.2000e-04\n",
      "Epoch 152/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.9838 - loss: 0.0232 - precision: 0.9838 - recall: 0.9823 - val_accuracy: 0.6852 - val_loss: 0.1583 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 7.2000e-05\n",
      "Epoch 153/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.9884 - loss: 0.0214 - precision: 0.9899 - recall: 0.9830 - val_accuracy: 0.7222 - val_loss: 0.0965 - val_precision: 0.7222 - val_recall: 0.7222 - learning_rate: 7.2000e-05\n",
      "Epoch 154/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.9907 - loss: 0.0202 - precision: 0.9923 - recall: 0.9900 - val_accuracy: 0.7222 - val_loss: 0.0889 - val_precision: 0.7222 - val_recall: 0.7222 - learning_rate: 7.2000e-05\n",
      "Epoch 155/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - accuracy: 0.9931 - loss: 0.0197 - precision: 0.9938 - recall: 0.9923 - val_accuracy: 0.7037 - val_loss: 0.1133 - val_precision: 0.7037 - val_recall: 0.7037 - learning_rate: 7.2000e-05\n",
      "Epoch 156/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.9946 - loss: 0.0189 - precision: 0.9954 - recall: 0.9938 - val_accuracy: 0.7222 - val_loss: 0.1076 - val_precision: 0.7222 - val_recall: 0.7222 - learning_rate: 7.2000e-05\n",
      "Epoch 157/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - accuracy: 0.9961 - loss: 0.0181 - precision: 0.9961 - recall: 0.9961 - val_accuracy: 0.6852 - val_loss: 0.1243 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 7.2000e-05\n",
      "Epoch 158/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.9892 - loss: 0.0188 - precision: 0.9892 - recall: 0.9877 - val_accuracy: 0.7222 - val_loss: 0.0868 - val_precision: 0.7222 - val_recall: 0.7222 - learning_rate: 7.2000e-05\n",
      "Epoch 159/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.9946 - loss: 0.0175 - precision: 0.9961 - recall: 0.9938 - val_accuracy: 0.6852 - val_loss: 0.0960 - val_precision: 0.6852 - val_recall: 0.6852 - learning_rate: 7.2000e-05\n",
      "Epoch 160/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.9931 - loss: 0.0169 - precision: 0.9931 - recall: 0.9931 - val_accuracy: 0.7222 - val_loss: 0.0928 - val_precision: 0.7222 - val_recall: 0.7222 - learning_rate: 7.2000e-05\n",
      "Epoch 161/300\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - accuracy: 0.9861 - loss: 0.0179 - precision: 0.9876 - recall: 0.9861 - val_accuracy: 0.7222 - val_loss: 0.1212 - val_precision: 0.7222 - val_recall: 0.7222 - learning_rate: 7.2000e-05\n",
      "Epoch 161: early stopping\n",
      "Restoring model weights from the end of the best epoch: 121.\n",
      "\n",
      "âœ“ Hybrid model training complete with improvements!\n"
     ]
    }
   ],
   "source": [
    "# Train Hybrid model with ANTI-OVERFITTING strategy\n",
    "print(\"\\nğŸš€ Training IMPROVED Hybrid CNN-GRU model...\")\n",
    "print(\"ANTI-OVERFITTING IMPROVEMENTS:\")\n",
    "print(\"  â€¢ INCREASED L2 regularization (0.001 â†’ 0.01) to prevent memorization\")\n",
    "print(\"  â€¢ INCREASED dropout (0.2-0.35 â†’ 0.3-0.45) for better generalization\")\n",
    "print(\"  â€¢ Added residual connections for gradient flow\")\n",
    "print(\"  â€¢ Stronger focal loss (gamma=3.0) for hard examples\")\n",
    "print(\"  â€¢ Lower learning rate (0.0002) for stability\")\n",
    "print(\"  â€¢ Enhanced class weights for Low/High yield detection\")\n",
    "\n",
    "# Ensure models directory exists and clean up any locked files\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "Path('models').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Remove existing checkpoint if it exists and might be locked\n",
    "checkpoint_path = Path('models') / 'hybrid_best.weights.h5'\n",
    "if checkpoint_path.exists():\n",
    "    try:\n",
    "        os.remove(checkpoint_path)\n",
    "        gc.collect()  # Force garbage collection\n",
    "        time.sleep(2.0)  # Longer delay for Windows file system\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not remove checkpoint file: {e}\")\n",
    "\n",
    "# Custom callback to handle file locking issues\n",
    "class SafeModelCheckpoint(callbacks.Callback):\n",
    "    def __init__(self, filepath, monitor='val_accuracy'):\n",
    "        super().__init__()\n",
    "        self.filepath = filepath\n",
    "        self.monitor = monitor\n",
    "        self.best = -float('inf')\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            return\n",
    "            \n",
    "        if current > self.best:\n",
    "            print(f\"\\nEpoch {epoch + 1}: {self.monitor} improved from {self.best:.5f} to {current:.5f}\")\n",
    "            self.best = current\n",
    "            # Close any existing file handles\n",
    "            gc.collect()\n",
    "            time.sleep(0.1)\n",
    "            # Save with retry logic\n",
    "            for attempt in range(3):\n",
    "                try:\n",
    "                    # Remove old file first\n",
    "                    if os.path.exists(self.filepath):\n",
    "                        os.remove(self.filepath)\n",
    "                        time.sleep(0.1)\n",
    "                    self.model.save_weights(self.filepath)\n",
    "                    print(f\"  Saved model to {self.filepath}\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt < 2:\n",
    "                        print(f\"  Save attempt {attempt + 1} failed, retrying...\")\n",
    "                        gc.collect()\n",
    "                        time.sleep(0.5)\n",
    "                    else:\n",
    "                        print(f\"  Warning: Could not save checkpoint: {e}\")\n",
    "\n",
    "hybrid_callbacks = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=40,  # Increased patience for slower learning rate\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.6,  # Less aggressive reduction\n",
    "        patience=15,  # More patience\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    SafeModelCheckpoint(\n",
    "        filepath=str(checkpoint_path),\n",
    "        monitor='val_accuracy'\n",
    "    )\n",
    "]\n",
    "\n",
    "# CRITICAL FIX: Amplify class weights for better Low/High yield detection\n",
    "# The original weights were too balanced, causing Medium bias\n",
    "hybrid_class_weight_amplified = {}\n",
    "for cls, weight in hybrid_class_weight_dict.items():\n",
    "    if cls == 0 or cls == 2:  # Low and High classes\n",
    "        hybrid_class_weight_amplified[cls] = weight * 1.5  # 50% boost\n",
    "    else:  # Medium class\n",
    "        hybrid_class_weight_amplified[cls] = weight * 0.7  # Reduce medium weight\n",
    "\n",
    "print(\"\\nAmplified Class Weights (to combat Medium bias):\")\n",
    "for cls, weight in hybrid_class_weight_amplified.items():\n",
    "    class_names = ['Low', 'Medium', 'High']\n",
    "    print(f\"  Class {cls} ({class_names[cls]}): {weight:.4f}\")\n",
    "\n",
    "history_hybrid = hybrid_model.fit(\n",
    "    [X_hybrid_temp_aug, X_hybrid_stat_aug],\n",
    "    y_hybrid_train_aug,\n",
    "    validation_data=([X_hybrid_temp_val_scaled, X_hybrid_stat_val_scaled], y_hybrid_val_onehot),\n",
    "    epochs=300,  # Keep high epochs due to early stopping\n",
    "    batch_size=24,  # Smaller batch for better gradient estimation\n",
    "    class_weight=hybrid_class_weight_amplified,  # Use amplified weights\n",
    "    callbacks=hybrid_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Hybrid model training complete with improvements!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7081f994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Evaluating Hybrid model...\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 344ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\n",
      "================================================================================\n",
      "HYBRID CNN-GRU MODEL RESULTS\n",
      "================================================================================\n",
      "\n",
      "Train Set:\n",
      "  Accuracy:  0.9877\n",
      "  Precision: 0.9881\n",
      "  Recall:    0.9877\n",
      "  F1-Score:  0.9877\n",
      "\n",
      "Validation Set:\n",
      "  Accuracy:  0.7037\n",
      "  Precision: 0.7604\n",
      "  Recall:    0.7037\n",
      "  F1-Score:  0.6509\n",
      "\n",
      "Test Set:\n",
      "  Accuracy:  0.7963\n",
      "  Precision: 0.8071\n",
      "  Recall:    0.7963\n",
      "  F1-Score:  0.7923\n",
      "\n",
      "\n",
      "Test Set Confusion Matrix:\n",
      "[[18  0  0]\n",
      " [ 0 15  3]\n",
      " [ 0  8 10]]\n",
      "\n",
      "\n",
      "Test Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low       1.00      1.00      1.00        18\n",
      "      Medium       0.65      0.83      0.73        18\n",
      "        High       0.77      0.56      0.65        18\n",
      "\n",
      "    accuracy                           0.80        54\n",
      "   macro avg       0.81      0.80      0.79        54\n",
      "weighted avg       0.81      0.80      0.79        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Hybrid model\n",
    "print(\"\\nğŸ“Š Evaluating Hybrid model...\")\n",
    "\n",
    "y_hybrid_pred_train_probs = hybrid_model.predict([X_hybrid_temp_train_scaled, X_hybrid_stat_train_scaled])\n",
    "y_hybrid_pred_val_probs = hybrid_model.predict([X_hybrid_temp_val_scaled, X_hybrid_stat_val_scaled])\n",
    "y_hybrid_pred_test_probs = hybrid_model.predict([X_hybrid_temp_test_scaled, X_hybrid_stat_test_scaled])\n",
    "\n",
    "y_hybrid_pred_train = np.argmax(y_hybrid_pred_train_probs, axis=1)\n",
    "y_hybrid_pred_val = np.argmax(y_hybrid_pred_val_probs, axis=1)\n",
    "y_hybrid_pred_test = np.argmax(y_hybrid_pred_test_probs, axis=1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYBRID CNN-GRU MODEL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for split_name, y_true, y_pred in [\n",
    "    ('Train', y_hybrid_train_cat, y_hybrid_pred_train),\n",
    "    ('Validation', y_hybrid_val_cat, y_hybrid_pred_val),\n",
    "    ('Test', y_hybrid_test_cat, y_hybrid_pred_test)\n",
    "]:\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"\\n{split_name} Set:\")\n",
    "    print(f\"  Accuracy:  {acc:.4f}\")\n",
    "    print(f\"  Precision: {prec:.4f}\")\n",
    "    print(f\"  Recall:    {rec:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "\n",
    "print(\"\\n\\nTest Set Confusion Matrix:\")\n",
    "print(confusion_matrix(y_hybrid_test_cat, y_hybrid_pred_test))\n",
    "\n",
    "print(\"\\n\\nTest Set Classification Report:\")\n",
    "print(classification_report(y_hybrid_test_cat, y_hybrid_pred_test,\n",
    "                          target_names=['Low', 'Medium', 'High']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f545db9",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2ceb519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL MODEL COMPARISON (Test Set)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "         Model  Accuracy  Precision  Recall  F1-Score\n",
      "           CNN    0.7407     0.7831  0.7407    0.7285\n",
      "           GRU    0.6852     0.8381  0.6852    0.5948\n",
      "Hybrid CNN-GRU    0.7963     0.8071  0.7963    0.7923\n",
      "\n",
      "\n",
      "Best Model by Metric:\n",
      "  Accuracy: Hybrid CNN-GRU (0.7963)\n",
      "  Precision: GRU (0.8381)\n",
      "  Recall: Hybrid CNN-GRU (0.7963)\n",
      "  F1-Score: Hybrid CNN-GRU (0.7923)\n"
     ]
    }
   ],
   "source": [
    "# Compare all models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL MODEL COMPARISON (Test Set)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_data = {\n",
    "    'Model': ['CNN', 'GRU', 'Hybrid CNN-GRU'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_cnn_test_cat, y_cnn_pred_test),\n",
    "        accuracy_score(y_gru_test_cat, y_gru_pred_test),\n",
    "        accuracy_score(y_hybrid_test_cat, y_hybrid_pred_test)\n",
    "    ],\n",
    "    'Precision': [\n",
    "        precision_score(y_cnn_test_cat, y_cnn_pred_test, average='weighted', zero_division=0),\n",
    "        precision_score(y_gru_test_cat, y_gru_pred_test, average='weighted', zero_division=0),\n",
    "        precision_score(y_hybrid_test_cat, y_hybrid_pred_test, average='weighted', zero_division=0)\n",
    "    ],\n",
    "    'Recall': [\n",
    "        recall_score(y_cnn_test_cat, y_cnn_pred_test, average='weighted', zero_division=0),\n",
    "        recall_score(y_gru_test_cat, y_gru_pred_test, average='weighted', zero_division=0),\n",
    "        recall_score(y_hybrid_test_cat, y_hybrid_pred_test, average='weighted', zero_division=0)\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f1_score(y_cnn_test_cat, y_cnn_pred_test, average='weighted', zero_division=0),\n",
    "        f1_score(y_gru_test_cat, y_gru_pred_test, average='weighted', zero_division=0),\n",
    "        f1_score(y_hybrid_test_cat, y_hybrid_pred_test, average='weighted', zero_division=0)\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.round(4)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nBest Model by Metric:\")\n",
    "for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score']:\n",
    "    best_idx = comparison_df[metric].idxmax()\n",
    "    best_model = comparison_df.loc[best_idx, 'Model']\n",
    "    best_score = comparison_df.loc[best_idx, metric]\n",
    "    print(f\"  {metric}: {best_model} ({best_score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bbdb0b",
   "metadata": {},
   "source": [
    "---\n",
    "## Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b9dc176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ Saving models...\n",
      "âœ“ Models saved:\n",
      "  - models/cnn_model.keras\n",
      "  - models/gru_model.keras\n",
      "  - models/hybrid_model.keras\n",
      "  - models/cnn_best.weights.h5\n",
      "  - models/gru_best.weights.h5\n",
      "  - models/hybrid_best.weights.h5\n",
      "\n",
      "ğŸ’¾ Saving scalers...\n",
      "âœ“ Scalers saved:\n",
      "  - models/cnn_scaler.pkl\n",
      "  - models/gru_scaler.pkl\n",
      "  - models/hybrid_temp_scaler.pkl\n",
      "  - models/hybrid_stat_scaler.pkl\n",
      "\n",
      "ğŸ’¾ Saving encoders...\n",
      "âœ“ Encoders saved:\n",
      "  - models/crop_encoder.pkl\n",
      "  - models/region_encoder.pkl\n",
      "\n",
      "âœ… All models, scalers, and encoders saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save complete models\n",
    "print(\"\\nğŸ’¾ Saving models...\")\n",
    "\n",
    "cnn_model.save('models/cnn_model.keras')\n",
    "gru_model.save('models/gru_model.keras')\n",
    "hybrid_model.save('models/hybrid_model.keras')\n",
    "\n",
    "print(\"âœ“ Models saved:\")\n",
    "print(\"  - models/cnn_model.keras\")\n",
    "print(\"  - models/gru_model.keras\")\n",
    "print(\"  - models/hybrid_model.keras\")\n",
    "print(\"  - models/cnn_best.weights.h5\")\n",
    "print(\"  - models/gru_best.weights.h5\")\n",
    "print(\"  - models/hybrid_best.weights.h5\")\n",
    "\n",
    "# Save scalers and encoders\n",
    "import joblib\n",
    "\n",
    "print(\"\\nğŸ’¾ Saving scalers...\")\n",
    "joblib.dump(scaler_cnn, 'models/cnn_scaler.pkl')\n",
    "joblib.dump(scaler_gru, 'models/gru_scaler.pkl')\n",
    "joblib.dump(scaler_hybrid_temp, 'models/hybrid_temp_scaler.pkl')\n",
    "joblib.dump(scaler_hybrid_stat, 'models/hybrid_stat_scaler.pkl')\n",
    "\n",
    "print(\"âœ“ Scalers saved:\")\n",
    "print(\"  - models/cnn_scaler.pkl\")\n",
    "print(\"  - models/gru_scaler.pkl\")\n",
    "print(\"  - models/hybrid_temp_scaler.pkl\")\n",
    "print(\"  - models/hybrid_stat_scaler.pkl\")\n",
    "\n",
    "print(\"\\nğŸ’¾ Saving encoders...\")\n",
    "joblib.dump(crop_encoder, 'models/crop_encoder.pkl')\n",
    "joblib.dump(region_encoder, 'models/region_encoder.pkl')\n",
    "\n",
    "print(\"âœ“ Encoders saved:\")\n",
    "print(\"  - models/crop_encoder.pkl\")\n",
    "print(\"  - models/region_encoder.pkl\")\n",
    "\n",
    "print(\"\\nâœ… All models, scalers, and encoders saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f9796c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAVING TRAINED MODELS\n",
      "================================================================================\n",
      "\n",
      "âœ“ Models directory created/verified\n",
      "\n",
      "ğŸ“Š Models in memory:\n",
      "  CNN model: Sequential\n",
      "  GRU model: Sequential\n",
      "  Hybrid model: Functional\n",
      "\n",
      "ğŸ’¾ Saving models...\n",
      "  âœ“ models/cnn_model.keras\n",
      "  âœ“ models/gru_model.keras\n",
      "  âœ“ models/hybrid_model.keras\n",
      "\n",
      "ğŸ’¾ Saving scalers...\n",
      "  âœ“ All 4 scalers saved\n",
      "\n",
      "ğŸ’¾ Saving encoders...\n",
      "  âœ“ All 2 encoders saved\n",
      "\n",
      "ğŸ“ Verifying files...\n",
      "  cnn_best.weights.h5                     2.07 MB\n",
      "  cnn_model.keras                         2.08 MB\n",
      "  cnn_scaler.pkl                          0.00 MB\n",
      "  crop_encoder.pkl                        0.00 MB\n",
      "  gru_best.weights.h5                     2.50 MB\n",
      "  gru_model.keras                         2.52 MB\n",
      "  gru_scaler.pkl                          0.00 MB\n",
      "  hybrid_best.weights.h5                  6.26 MB\n",
      "  hybrid_model.keras                      6.30 MB\n",
      "  hybrid_stat_scaler.pkl                  0.00 MB\n",
      "  hybrid_temp_scaler.pkl                  0.00 MB\n",
      "  region_encoder.pkl                      0.00 MB\n",
      "\n",
      "================================================================================\n",
      "âœ… ALL MODELS, SCALERS, AND ENCODERS SAVED SUCCESSFULLY!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# VERIFY AND SAVE MODELS\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SAVING TRAINED MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure models directory exists\n",
    "Path('models').mkdir(exist_ok=True)\n",
    "print(\"\\nâœ“ Models directory created/verified\")\n",
    "\n",
    "# Check models are in memory\n",
    "print(\"\\nğŸ“Š Models in memory:\")\n",
    "print(f\"  CNN model: {type(cnn_model).__name__}\")\n",
    "print(f\"  GRU model: {type(gru_model).__name__}\")\n",
    "print(f\"  Hybrid model: {type(hybrid_model).__name__}\")\n",
    "\n",
    "print(\"\\nğŸ’¾ Saving models...\")\n",
    "cnn_model.save('models/cnn_model.keras')\n",
    "print(\"  âœ“ models/cnn_model.keras\")\n",
    "\n",
    "gru_model.save('models/gru_model.keras')\n",
    "print(\"  âœ“ models/gru_model.keras\")\n",
    "\n",
    "hybrid_model.save('models/hybrid_model.keras')\n",
    "print(\"  âœ“ models/hybrid_model.keras\")\n",
    "\n",
    "print(\"\\nğŸ’¾ Saving scalers...\")\n",
    "joblib.dump(scaler_cnn, 'models/cnn_scaler.pkl')\n",
    "joblib.dump(scaler_gru, 'models/gru_scaler.pkl')\n",
    "joblib.dump(scaler_hybrid_temp, 'models/hybrid_temp_scaler.pkl')\n",
    "joblib.dump(scaler_hybrid_stat, 'models/hybrid_stat_scaler.pkl')\n",
    "print(\"  âœ“ All 4 scalers saved\")\n",
    "\n",
    "print(\"\\nğŸ’¾ Saving encoders...\")\n",
    "joblib.dump(crop_encoder, 'models/crop_encoder.pkl')\n",
    "joblib.dump(region_encoder, 'models/region_encoder.pkl')\n",
    "print(\"  âœ“ All 2 encoders saved\")\n",
    "\n",
    "print(\"\\nğŸ“ Verifying files...\")\n",
    "files = sorted(os.listdir('models'))\n",
    "for f in files:\n",
    "    size = os.path.getsize(f'models/{f}') / (1024*1024)\n",
    "    print(f\"  {f:<35} {size:>8.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… ALL MODELS, SCALERS, AND ENCODERS SAVED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce09ce4f",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "âœ… **Three models trained successfully:**\n",
    "- **CNN**: 1D convolutions for temporal pattern extraction\n",
    "- **GRU**: Bidirectional recurrent network for sequence modeling\n",
    "- **Hybrid CNN-GRU**: Combined architecture with CNNâ†’GRU pipeline + static features\n",
    "\n",
    "âœ… **Improvements implemented:**\n",
    "- Lag features (previous 1-3 years' yields)\n",
    "- Data augmentation (2x training data)\n",
    "- Class weights for balanced learning\n",
    "- Bidirectional processing for temporal context\n",
    "\n",
    "âœ… **Next Steps:**\n",
    "Proceed to phase4_validation.ipynb for detailed analysis and ensemble modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7709ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAVING MODELS\n",
      "================================================================================\n",
      "\n",
      "ğŸ’¾ Saving CNN model...\n",
      "  âœ“ models/cnn_model.keras\n",
      "\n",
      "ğŸ’¾ Saving GRU model...\n",
      "  âœ“ models/gru_model.keras\n",
      "\n",
      "ğŸ’¾ Saving Hybrid model...\n",
      "  âœ“ models/hybrid_model.keras\n",
      "\n",
      "ğŸ’¾ Saving scalers...\n",
      "  âœ“ All scalers saved\n",
      "\n",
      "ğŸ’¾ Saving encoders...\n",
      "  âœ“ All encoders saved\n",
      "\n",
      "ğŸ“ Files created:\n",
      "  cnn_best.weights.h5                     2.07 MB\n",
      "  cnn_model.keras                         2.08 MB\n",
      "  cnn_scaler.pkl                          0.00 MB\n",
      "  crop_encoder.pkl                        0.00 MB\n",
      "  gru_best.weights.h5                     2.50 MB\n",
      "  gru_model.keras                         2.52 MB\n",
      "  gru_scaler.pkl                          0.00 MB\n",
      "  hybrid_best.weights.h5                  6.26 MB\n",
      "  hybrid_model.keras                      6.30 MB\n",
      "  hybrid_stat_scaler.pkl                  0.00 MB\n",
      "  hybrid_temp_scaler.pkl                  0.00 MB\n",
      "  region_encoder.pkl                      0.00 MB\n",
      "\n",
      "================================================================================\n",
      "âœ… ALL FILES SAVED!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SAVE MODELS FROM KERNEL\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SAVING MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "Path('models').mkdir(exist_ok=True)\n",
    "\n",
    "print(\"\\nğŸ’¾ Saving CNN model...\")\n",
    "cnn_model.save('models/cnn_model.keras')\n",
    "print(\"  âœ“ models/cnn_model.keras\")\n",
    "\n",
    "print(\"\\nğŸ’¾ Saving GRU model...\")\n",
    "gru_model.save('models/gru_model.keras')\n",
    "print(\"  âœ“ models/gru_model.keras\")\n",
    "\n",
    "print(\"\\nğŸ’¾ Saving Hybrid model...\")\n",
    "hybrid_model.save('models/hybrid_model.keras')\n",
    "print(\"  âœ“ models/hybrid_model.keras\")\n",
    "\n",
    "print(\"\\nğŸ’¾ Saving scalers...\")\n",
    "joblib.dump(scaler_cnn, 'models/cnn_scaler.pkl')\n",
    "joblib.dump(scaler_gru, 'models/gru_scaler.pkl')\n",
    "joblib.dump(scaler_hybrid_temp, 'models/hybrid_temp_scaler.pkl')\n",
    "joblib.dump(scaler_hybrid_stat, 'models/hybrid_stat_scaler.pkl')\n",
    "print(\"  âœ“ All scalers saved\")\n",
    "\n",
    "print(\"\\nğŸ’¾ Saving encoders...\")\n",
    "joblib.dump(crop_encoder, 'models/crop_encoder.pkl')\n",
    "joblib.dump(region_encoder, 'models/region_encoder.pkl')\n",
    "print(\"  âœ“ All encoders saved\")\n",
    "\n",
    "import os\n",
    "print(\"\\nğŸ“ Files created:\")\n",
    "for f in sorted(os.listdir('models')):\n",
    "    size = os.path.getsize(f'models/{f}') / (1024*1024)\n",
    "    print(f\"  {f:<35} {size:>8.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… ALL FILES SAVED!\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
