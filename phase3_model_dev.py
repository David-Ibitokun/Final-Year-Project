{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate-Food Security Modeling Pipeline\n",
    "## Phase 3: Model Development (FNN, LSTM, Hybrid)\n",
    "\n",
    "This notebook implements three deep learning architectures:\n",
    "1. **FNN (Feedforward Neural Network)** - Annual aggregated data\n",
    "2. **LSTM (Long Short-Term Memory)** - Monthly time-series data\n",
    "3. **Hybrid Model** - Combines temporal and static features\n",
    "\n",
    "**Splits:** Train (1990-2016), Validation (2017-2019), Test (2020-2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "data_path = Path('project_data')\n",
    "splits_path = data_path / 'train_test_split'\n",
    "\n",
    "print(\"Loading train/validation/test splits...\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: FNN (Feedforward Neural Network) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load FNN Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä LOADING FNN DATA...\")\n",
    "\n",
    "# Load splits\n",
    "fnn_train = pd.read_csv(splits_path / 'fnn' / 'train.csv')\n",
    "fnn_val = pd.read_csv(splits_path / 'fnn' / 'val.csv')\n",
    "fnn_test = pd.read_csv(splits_path / 'fnn' / 'test.csv')\n",
    "\n",
    "print(f\"  Train: {fnn_train.shape}\")\n",
    "print(f\"  Val:   {fnn_val.shape}\")\n",
    "print(f\"  Test:  {fnn_test.shape}\")\n",
    "\n",
    "# Remove rows with missing yields\n",
    "fnn_train = fnn_train.dropna(subset=['Yield_tonnes_per_ha'])\n",
    "fnn_val = fnn_val.dropna(subset=['Yield_tonnes_per_ha'])\n",
    "fnn_test = fnn_test.dropna(subset=['Yield_tonnes_per_ha'])\n",
    "\n",
    "print(f\"\\nAfter removing missing yields:\")\n",
    "print(f\"  Train: {fnn_train.shape}\")\n",
    "print(f\"  Val:   {fnn_val.shape}\")\n",
    "print(f\"  Test:  {fnn_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Prepare FNN Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns for FNN\n",
    "fnn_feature_cols = [\n",
    "    # Climate features (growing season aggregates)\n",
    "    'Avg_Temp_C', 'Min_Temp_C', 'Max_Temp_C', 'Temp_Range_C',\n",
    "    'Rainfall_mm', 'Rainy_Days', 'Max_Daily_Rainfall_mm', 'Rainfall_Intensity',\n",
    "    'Avg_Humidity_Percent', 'Min_Humidity_Percent', 'Max_Humidity_Percent',\n",
    "    'CO2_ppm', 'CO2_Growth_Rate_ppm_per_year',\n",
    "    \n",
    "    # Stress indicators\n",
    "    'Heat_Stress_Days', 'Cold_Stress_Days', 'Drought_Index', 'Flood_Risk_Index',\n",
    "    \n",
    "    # Soil properties\n",
    "    'Soil_pH', 'Organic_Matter_Percent', 'Nitrogen_ppm', 'Phosphorus_ppm', \n",
    "    'Potassium_ppm', 'Cation_Exchange_Capacity', 'Bulk_Density', \n",
    "    'Water_Holding_Capacity_Percent'\n",
    "]\n",
    "\n",
    "target_col = 'Yield_tonnes_per_ha'\n",
    "\n",
    "# Encode categorical variables\n",
    "le_crop = LabelEncoder()\n",
    "le_zone = LabelEncoder()\n",
    "le_state = LabelEncoder()\n",
    "\n",
    "# Fit on train, transform all\n",
    "fnn_train['Crop_encoded'] = le_crop.fit_transform(fnn_train['Crop'])\n",
    "fnn_val['Crop_encoded'] = le_crop.transform(fnn_val['Crop'])\n",
    "fnn_test['Crop_encoded'] = le_crop.transform(fnn_test['Crop'])\n",
    "\n",
    "fnn_train['Zone_encoded'] = le_zone.fit_transform(fnn_train['Geopolitical_Zone'])\n",
    "fnn_val['Zone_encoded'] = le_zone.transform(fnn_val['Geopolitical_Zone'])\n",
    "fnn_test['Zone_encoded'] = le_zone.transform(fnn_test['Geopolitical_Zone'])\n",
    "\n",
    "fnn_train['State_encoded'] = le_state.fit_transform(fnn_train['State'])\n",
    "fnn_val['State_encoded'] = le_state.transform(fnn_val['State'])\n",
    "fnn_test['State_encoded'] = le_state.transform(fnn_test['State'])\n",
    "\n",
    "# Add encoded features\n",
    "fnn_feature_cols.extend(['Crop_encoded', 'Zone_encoded', 'State_encoded'])\n",
    "\n",
    "print(f\"\\nTotal FNN features: {len(fnn_feature_cols)}\")\n",
    "print(f\"Feature columns: {fnn_feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "X_fnn_train = fnn_train[fnn_feature_cols].values\n",
    "y_fnn_train = fnn_train[target_col].values\n",
    "\n",
    "X_fnn_val = fnn_val[fnn_feature_cols].values\n",
    "y_fnn_val = fnn_val[target_col].values\n",
    "\n",
    "X_fnn_test = fnn_test[fnn_feature_cols].values\n",
    "y_fnn_test = fnn_test[target_col].values\n",
    "\n",
    "print(f\"\\nFNN arrays:\")\n",
    "print(f\"  X_train: {X_fnn_train.shape}, y_train: {y_fnn_train.shape}\")\n",
    "print(f\"  X_val:   {X_fnn_val.shape}, y_val:   {y_fnn_val.shape}\")\n",
    "print(f\"  X_test:  {X_fnn_test.shape}, y_test:  {y_fnn_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "fnn_scaler = StandardScaler()\n",
    "X_fnn_train_scaled = fnn_scaler.fit_transform(X_fnn_train)\n",
    "X_fnn_val_scaled = fnn_scaler.transform(X_fnn_val)\n",
    "X_fnn_test_scaled = fnn_scaler.transform(X_fnn_test)\n",
    "\n",
    "print(\"\\n‚úì Features scaled using StandardScaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Build FNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fnn_model(input_dim, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Build Feedforward Neural Network for crop yield prediction\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        \n",
    "        # Hidden layer 1\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Hidden layer 2\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # Hidden layer 3\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.1),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='mse',\n",
    "        metrics=['mae', tf.keras.metrics.RootMeanSquaredError(name='rmse')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "fnn_model = build_fnn_model(input_dim=X_fnn_train_scaled.shape[1])\n",
    "fnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Train FNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining FNN model...\")\n",
    "history_fnn = fnn_model.fit(\n",
    "    X_fnn_train_scaled, y_fnn_train,\n",
    "    validation_data=(X_fnn_val_scaled, y_fnn_val),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì FNN model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Evaluate FNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_fnn_pred_train = fnn_model.predict(X_fnn_train_scaled).flatten()\n",
    "y_fnn_pred_val = fnn_model.predict(X_fnn_val_scaled).flatten()\n",
    "y_fnn_pred_test = fnn_model.predict(X_fnn_test_scaled).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "def calculate_metrics(y_true, y_pred, set_name):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "    \n",
    "    print(f\"\\n{set_name} Set Performance:\")\n",
    "    print(f\"  RMSE:  {rmse:.4f} tonnes/ha\")\n",
    "    print(f\"  MAE:   {mae:.4f} tonnes/ha\")\n",
    "    print(f\"  R¬≤:    {r2:.4f}\")\n",
    "    print(f\"  MAPE:  {mape:.2f}%\")\n",
    "    \n",
    "    return {'RMSE': rmse, 'MAE': mae, 'R2': r2, 'MAPE': mape}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FNN MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fnn_metrics_train = calculate_metrics(y_fnn_train, y_fnn_pred_train, \"Train\")\n",
    "fnn_metrics_val = calculate_metrics(y_fnn_val, y_fnn_pred_val, \"Validation\")\n",
    "fnn_metrics_test = calculate_metrics(y_fnn_test, y_fnn_pred_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('FNN Training History', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history_fnn.history['loss'], label='Train Loss')\n",
    "axes[0].plot(history_fnn.history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].set_title('Model Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "axes[1].plot(history_fnn.history['mae'], label='Train MAE')\n",
    "axes[1].plot(history_fnn.history['val_mae'], label='Val MAE')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('Mean Absolute Error')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: LSTM (Long Short-Term Memory) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load LSTM Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä LOADING LSTM DATA...\")\n",
    "\n",
    "# Load splits\n",
    "lstm_train = pd.read_csv(splits_path / 'lstm' / 'train.csv')\n",
    "lstm_val = pd.read_csv(splits_path / 'lstm' / 'val.csv')\n",
    "lstm_test = pd.read_csv(splits_path / 'lstm' / 'test.csv')\n",
    "\n",
    "print(f\"  Train: {lstm_train.shape}\")\n",
    "print(f\"  Val:   {lstm_val.shape}\")\n",
    "print(f\"  Test:  {lstm_test.shape}\")\n",
    "\n",
    "# Remove rows with missing yields\n",
    "lstm_train = lstm_train.dropna(subset=['Yield_tonnes_per_ha'])\n",
    "lstm_val = lstm_val.dropna(subset=['Yield_tonnes_per_ha'])\n",
    "lstm_test = lstm_test.dropna(subset=['Yield_tonnes_per_ha'])\n",
    "\n",
    "print(f\"\\nAfter removing missing yields:\")\n",
    "print(f\"  Train: {lstm_train.shape}\")\n",
    "print(f\"  Val:   {lstm_val.shape}\")\n",
    "print(f\"  Test:  {lstm_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Prepare LSTM Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM feature columns (temporal features only)\n",
    "lstm_feature_cols = [\n",
    "    'Avg_Temp_C', 'Min_Temp_C', 'Max_Temp_C',\n",
    "    'Rainfall_mm', 'Rainy_Days', 'Rainfall_Intensity',\n",
    "    'Avg_Humidity_Percent',\n",
    "    'CO2_ppm',\n",
    "    'Heat_Stress_Days', 'Drought_Index', 'Flood_Risk_Index',\n",
    "    'Is_Growing_Season'\n",
    "]\n",
    "\n",
    "# Add static features as encodings\n",
    "lstm_train['Crop_encoded'] = le_crop.transform(lstm_train['Crop'])\n",
    "lstm_val['Crop_encoded'] = le_crop.transform(lstm_val['Crop'])\n",
    "lstm_test['Crop_encoded'] = le_crop.transform(lstm_test['Crop'])\n",
    "\n",
    "lstm_feature_cols.append('Crop_encoded')\n",
    "\n",
    "print(f\"\\nLSTM features: {len(lstm_feature_cols)}\")\n",
    "print(f\"Feature columns: {lstm_feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, feature_cols, target_col, sequence_length=12):\n",
    "    \"\"\"\n",
    "    Create sequences for LSTM\n",
    "    Each sequence contains 'sequence_length' months of data\n",
    "    \"\"\"\n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "    \n",
    "    # Group by Year, State, Crop\n",
    "    grouped = data.groupby(['Year', 'State', 'Crop'])\n",
    "    \n",
    "    for name, group in grouped:\n",
    "        group = group.sort_values('Month')\n",
    "        \n",
    "        if len(group) >= sequence_length:\n",
    "            # Get features and target\n",
    "            features = group[feature_cols].values\n",
    "            target = group[target_col].iloc[0]  # Yield is same for all months\n",
    "            \n",
    "            # Create sequence (use first 'sequence_length' months)\n",
    "            X_sequences.append(features[:sequence_length])\n",
    "            y_sequences.append(target)\n",
    "    \n",
    "    return np.array(X_sequences), np.array(y_sequences)\n",
    "\n",
    "# Create sequences\n",
    "print(\"\\nCreating LSTM sequences (12 months)...\")\n",
    "sequence_length = 12\n",
    "\n",
    "X_lstm_train, y_lstm_train = create_sequences(lstm_train, lstm_feature_cols, target_col, sequence_length)\n",
    "X_lstm_val, y_lstm_val = create_sequences(lstm_val, lstm_feature_cols, target_col, sequence_length)\n",
    "X_lstm_test, y_lstm_test = create_sequences(lstm_test, lstm_feature_cols, target_col, sequence_length)\n",
    "\n",
    "print(f\"\\nLSTM sequences:\")\n",
    "print(f\"  X_train: {X_lstm_train.shape}, y_train: {y_lstm_train.shape}\")\n",
    "print(f\"  X_val:   {X_lstm_val.shape}, y_val:   {y_lstm_val.shape}\")\n",
    "print(f\"  X_test:  {X_lstm_test.shape}, y_test:  {y_lstm_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale LSTM features\n",
    "lstm_scaler = StandardScaler()\n",
    "\n",
    "# Reshape for scaling\n",
    "n_train, seq_len, n_features = X_lstm_train.shape\n",
    "X_lstm_train_reshaped = X_lstm_train.reshape(-1, n_features)\n",
    "X_lstm_train_scaled = lstm_scaler.fit_transform(X_lstm_train_reshaped).reshape(n_train, seq_len, n_features)\n",
    "\n",
    "n_val = X_lstm_val.shape[0]\n",
    "X_lstm_val_reshaped = X_lstm_val.reshape(-1, n_features)\n",
    "X_lstm_val_scaled = lstm_scaler.transform(X_lstm_val_reshaped).reshape(n_val, seq_len, n_features)\n",
    "\n",
    "n_test = X_lstm_test.shape[0]\n",
    "X_lstm_test_reshaped = X_lstm_test.reshape(-1, n_features)\n",
    "X_lstm_test_scaled = lstm_scaler.transform(X_lstm_test_reshaped).reshape(n_test, seq_len, n_features)\n",
    "\n",
    "print(\"\\n‚úì LSTM sequences scaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Build LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(sequence_length, n_features, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Build LSTM model for time-series crop yield prediction\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(sequence_length, n_features)),\n",
    "        \n",
    "        # LSTM layers\n",
    "        layers.LSTM(128, return_sequences=True),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.LSTM(64, return_sequences=False),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # Dense layers\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.1),\n",
    "        \n",
    "        # Output\n",
    "        layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='mse',\n",
    "        metrics=['mae', tf.keras.metrics.RootMeanSquaredError(name='rmse')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build LSTM model\n",
    "lstm_model = build_lstm_model(sequence_length=sequence_length, n_features=n_features)\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stop_lstm = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=30,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr_lstm = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=15,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train LSTM model\n",
    "print(\"\\nTraining LSTM model...\")\n",
    "history_lstm = lstm_model.fit(\n",
    "    X_lstm_train_scaled, y_lstm_train,\n",
    "    validation_data=(X_lstm_val_scaled, y_lstm_val),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop_lstm, reduce_lr_lstm],\n",
    "    verbose=1\n",
    ")"
    "\n",
    "print(\"\\n‚úì LSTM model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Evaluate LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_lstm_pred_train = lstm_model.predict(X_lstm_train_scaled).flatten()\n",
    "y_lstm_pred_val = lstm_model.predict(X_lstm_val_scaled).flatten()\n",
    "y_lstm_pred_test = lstm_model.predict(X_lstm_test_scaled).flatten()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LSTM MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "lstm_metrics_train = calculate_metrics(y_lstm_train, y_lstm_pred_train, \"Train\")\n",
    "lstm_metrics_val = calculate_metrics(y_lstm_val, y_lstm_pred_val, \"Validation\")\n",
    "lstm_metrics_test = calculate_metrics(y_lstm_test, y_lstm_pred_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LSTM training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('LSTM Training History', fontsize=16, fontweight='bold')\n",
    "\n",
    "axes[0].plot(history_lstm.history['loss'], label='Train Loss')\n",
    "axes[0].plot(history_lstm.history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].set_title('Model Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history_lstm.history['mae'], label='Train MAE')\n",
    "axes[1].plot(history_lstm.history['val_mae'], label='Val MAE')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('Mean Absolute Error')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Hybrid Model (LSTM + Static Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load Hybrid Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä LOADING HYBRID DATA...\")\n",
    "\n",
    "# Load splits\n",
    "hybrid_train = pd.read_csv(splits_path / 'hybrid' / 'train.csv')\n",
    "hybrid_val = pd.read_csv(splits_path / 'hybrid' / 'val.csv')\n",
    "hybrid_test = pd.read_csv(splits_path / 'hybrid' / 'test.csv')\n",
    "\n",
    "print(f\"  Train: {hybrid_train.shape}\")\n",
    "print(f\"  Val:   {hybrid_val.shape}\")\n",
    "print(f\"  Test:  {hybrid_test.shape}\")\n",
    "\n",
    "# Remove rows with missing yields\n",
    "hybrid_train = hybrid_train.dropna(subset=['Yield_tonnes_per_ha'])\n",
    "hybrid_val = hybrid_val.dropna(subset=['Yield_tonnes_per_ha'])\n",
    "hybrid_test = hybrid_test.dropna(subset=['Yield_tonnes_per_ha'])\n",
    "\n",
    "print(f\"\\nAfter removing missing yields:\")\n",
    "print(f\"  Train: {hybrid_train.shape}\")\n",
    "print(f\"  Val:   {hybrid_val.shape}\")\n",
    "print(f\"  Test:  {hybrid_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Prepare Hybrid Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define temporal and static features separately\n",
    "temporal_features = [\n",
    "    'Avg_Temp_C', 'Min_Temp_C', 'Max_Temp_C',\n",
    "    'Rainfall_mm', 'Rainy_Days', 'Rainfall_Intensity',\n",
    "    'Avg_Humidity_Percent',\n",
    "    'CO2_ppm',\n",
    "    'Heat_Stress_Days', 'Drought_Index', 'Flood_Risk_Index',\n",
    "    'Growing_Degree_Days', 'Cumulative_Rainfall', 'Days_Into_Season'\n",
    "]\n",
    "\n",
    "static_features = [\n",
    "    'Soil_pH', 'Organic_Matter_Percent', 'Nitrogen_ppm', \n",
    "    'Phosphorus_ppm', 'Potassium_ppm', 'Water_Holding_Capacity_Percent'\n",
    "]\n",
    "\n",
    "# Add encodings\n",
    "hybrid_train['Crop_encoded'] = le_crop.transform(hybrid_train['Crop'])\n",
    "hybrid_val['Crop_encoded'] = le_crop.transform(hybrid_val['Crop'])\n",
    "hybrid_test['Crop_encoded'] = le_crop.transform(hybrid_test['Crop'])\n",
    "\n",
    "static_features.append('Crop_encoded')\n",
    "\n",
    "print(f\"\\nTemporal features: {len(temporal_features)}\")\n",
    "print(f\"Static features: {len(static_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hybrid_sequences(data, temporal_cols, static_cols, target_col, sequence_length=12):\n",
    "    \"\"\"\n",
    "    Create sequences for Hybrid model\n",
    "    Returns: temporal sequences, static features, targets\n",
    "    \"\"\"\n",
    "    X_temporal = []\n",
    "    X_static = []\n",
    "    y_targets = []\n",
    "    \n",
    "    # Group by Year, State, Crop\n",
    "    grouped = data.groupby(['Year', 'State', 'Crop'])\n",
    "    \n",
    "    for name, group in grouped:\n",
    "        group = group.sort_values('Month')\n",
    "        \n",
    "        if len(group) >= sequence_length:\n",
    "            # Temporal features (sequence)\n",
    "            temporal = group[temporal_cols].values[:sequence_length]\n",
    "            \n",
    "            # Static features (same for all timesteps, take first)\n",
    "            static = group[static_cols].iloc[0].values\n",
    "            \n",
    "            # Target\n",
    "            target = group[target_col].iloc[0]\n",
    "            \n",
    "            X_temporal.append(temporal)\n",
    "            X_static.append(static)\n",
    "            y_targets.append(target)\n",
    "    \n",
    "    return np.array(X_temporal), np.array(X_static), np.array(y_targets)\n",
    "\n",
    "# Create hybrid sequences\n",
    "print(\"\\nCreating Hybrid model sequences...\")\n",
    "X_hybrid_temporal_train, X_hybrid_static_train, y_hybrid_train = create_hybrid_sequences(\n",
    "    hybrid_train, temporal_features, static_features, target_col, sequence_length\n",
    ")\n",
    "X_hybrid_temporal_val, X_hybrid_static_val, y_hybrid_val = create_hybrid_sequences(\n",
    "    hybrid_val, temporal_features, static_features, target_col, sequence_length\n",
    ")\n",
    "X_hybrid_temporal_test, X_hybrid_static_test, y_hybrid_test = create_hybrid_sequences(\n",
    "    hybrid_test, temporal_features, static_features, target_col, sequence_length\n",
    ")\n",
    "\n",
    "print(f\"\\nHybrid sequences:\")\n",
    "print(f\"  Temporal train: {X_hybrid_temporal_train.shape}\")\n",
    "print(f\"  Static train: {X_hybrid_static_train.shape}\")\n",
    "print(f\"  y_train: {y_hybrid_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale temporal features\n",
    "temporal_scaler = StandardScaler()\n",
    "n_train_h, seq_len_h, n_temp_feat = X_hybrid_temporal_train.shape\n",
    "\n",
    "X_hybrid_temporal_train_scaled = temporal_scaler.fit_transform(\n",
    "    X_hybrid_temporal_train.reshape(-1, n_temp_feat)\n",
    ").reshape(n_train_h, seq_len_h, n_temp_feat)\n",
    "\n",
    "n_val_h = X_hybrid_temporal_val.shape[0]\n",
    "X_hybrid_temporal_val_scaled = temporal_scaler.transform(\n",
    "    X_hybrid_temporal_val.reshape(-1, n_temp_feat)\n",
    ").reshape(n_val_h, seq_len_h, n_temp_feat)\n",
    "\n",
    "n_test_h = X_hybrid_temporal_test.shape[0]\n",
    "X_hybrid_temporal_test_scaled = temporal_scaler.transform(\n",
    "    X_hybrid_temporal_test.reshape(-1, n_temp_feat)\n",
    ").reshape(n_test_h, seq_len_h, n_temp_feat)\n",
    "\n",
    "# Scale static features\n",
    "static_scaler = StandardScaler()\n",
    "X_hybrid_static_train_scaled = static_scaler.fit_transform(X_hybrid_static_train)\n",
    "X_hybrid_static_val_scaled = static_scaler.transform(X_hybrid_static_val)\n",
    "X_hybrid_static_test_scaled = static_scaler.transform(X_hybrid_static_test)\n",
    "\n",
    "print(\"\\n‚úì Hybrid features scaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Build Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hybrid_model(sequence_length, n_temporal_features, n_static_features, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Build Hybrid model combining LSTM for temporal data and Dense layers for static data\n",
    "    \"\"\"\n",
    "    # Temporal input branch (LSTM)\n",
    "    temporal_input = layers.Input(shape=(sequence_length, n_temporal_features), name='temporal_input')\n",
    "    lstm_out = layers.LSTM(64, return_sequences=True)(temporal_input)\n",
    "    lstm_out = layers.Dropout(0.3)(lstm_out)\n",
    "    lstm_out = layers.LSTM(32, return_sequences=False)(lstm_out)\n",
    "    lstm_out = layers.Dropout(0.2)(lstm_out)\n",
    "    \n",
    "    # Static input branch (Dense)\n",
    "    static_input = layers.Input(shape=(n_static_features,), name='static_input')\n",
    "    static_out = layers.Dense(32, activation='relu')(static_input)\n",
    "    static_out = layers.Dropout(0.2)(static_out)\n",
    "    static_out = layers.Dense(16, activation='relu')(static_out)\n",
    "    \n",
    "    # Concatenate both branches\n",
    "    combined = layers.concatenate([lstm_out, static_out])\n",
    "    \n",
    "    # Final dense layers\n",
    "    combined = layers.Dense(32, activation='relu')(combined)\n",
    "    combined = layers.Dropout(0.1)(combined)\n",
    "    combined = layers.Dense(16, activation='relu')(combined)\n",
    "    \n",
    "    # Output\n",
    "    output = layers.Dense(1, activation='linear', name='output')(combined)\n",
    "    \n",
    "    # Build model\n",
    "    model = models.Model(inputs=[temporal_input, static_input], outputs=output)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='mse',\n",
    "        metrics=['mae', tf.keras.metrics.RootMeanSquaredError(name='rmse')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build hybrid model\n",
    "hybrid_model = build_hybrid_model(\n",
    "    sequence_length=sequence_length,\n",
    "    n_temporal_features=n_temp_feat,\n",
    "    n_static_features=X_hybrid_static_train_scaled.shape[1]\n",
    ")\n",
    "\n",
    "hybrid_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Train Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stop_hybrid = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=30,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr_hybrid = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=15,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train hybrid model\n",
    "print(\"\\nTraining Hybrid model...\")\n",
    "history_hybrid = hybrid_model.fit(\n",
    "    [X_hybrid_temporal_train_scaled, X_hybrid_static_train_scaled],\n",
    "    y_hybrid_train,\n",
    "    validation_data=(\n",
    "        [X_hybrid_temporal_val_scaled, X_hybrid_static_val_scaled],\n",
    "        y_hybrid_val\n",
    "    ),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop_hybrid, reduce_lr_hybrid],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Hybrid model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Evaluate Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_hybrid_pred_train = hybrid_model.predict(\n",
    "    [X_hybrid_temporal_train_scaled, X_hybrid_static_train_scaled]\n",
    ").flatten()\n",
    "\n",
    "y_hybrid_pred_val = hybrid_model.predict(\n",
    "    [X_hybrid_temporal_val_scaled, X_hybrid_static_val_scaled]\n",
    ").flatten()\n",
    "\n",
    "y_hybrid_pred_test = hybrid_model.predict(\n",
    "    [X_hybrid_temporal_test_scaled, X_hybrid_static_test_scaled]\n",
    ").flatten()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYBRID MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "hybrid_metrics_train = calculate_metrics(y_hybrid_train, y_hybrid_pred_train, \"Train\")\n",
    "hybrid_metrics_val = calculate_metrics(y_hybrid_val, y_hybrid_pred_val, \"Validation\")\n",
    "hybrid_metrics_test = calculate_metrics(y_hybrid_test, y_hybrid_pred_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Hybrid training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('Hybrid Model Training History', fontsize=16, fontweight='bold')\n",
    "\n",
    "axes[0].plot(history_hybrid.history['loss'], label='Train Loss')\n",
    "axes[0].plot(history_hybrid.history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].set_title('Model Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history_hybrid.history['mae'], label='Train MAE')\n",
    "axes[1].plot(history_hybrid.history['val_mae'], label='Val MAE')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('Mean Absolute Error')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['FNN', 'LSTM', 'Hybrid'] * 3,\n",
    "    'Dataset': ['Train']*3 + ['Validation']*3 + ['Test']*3,\n",
    "    'RMSE': [\n",
    "        fnn_metrics_train['RMSE'], lstm_metrics_train['RMSE'], hybrid_metrics_train['RMSE'],\n",
    "        fnn_metrics_val['RMSE'], lstm_metrics_val['RMSE'], hybrid_metrics_val['RMSE'],\n",
    "        fnn_metrics_test['RMSE'], lstm_metrics_test['RMSE'], hybrid_metrics_test['RMSE']\n",
    "    ],\n",
    "    'MAE': [\n",
    "        fnn_metrics_train['MAE'], lstm_metrics_train['MAE'], hybrid_metrics_train['MAE'],\n",
    "        fnn_metrics_val['MAE'], lstm_metrics_val['MAE'], hybrid_metrics_val['MAE'],\n",
    "        fnn_metrics_test['MAE'], lstm_metrics_test['MAE'], hybrid_metrics_test['MAE']\n",
    "    ],\n",
    "    'R2': [\n",
    "        fnn_metrics_train['R2'], lstm_metrics_train['R2'], hybrid_metrics_train['R2'],\n",
    "        fnn_metrics_val['R2'], lstm_metrics_val['R2'], hybrid_metrics_val['R2'],\n",
    "        fnn_metrics_test['R2'], lstm_metrics_test['R2'], hybrid_metrics_test['R2']\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON - ALL DATASETS\")\n",
    "print(\"=\"*80)\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Model Comparison on Test Set', fontsize=16, fontweight='bold')\n",
    "\n",
    "test_comparison = comparison[comparison['Dataset'] == 'Test']\n",
    "\n",
    "# RMSE\n",
    "axes[0].bar(test_comparison['Model'], test_comparison['RMSE'], color=['steelblue', 'coral', 'green'])\n",
    "axes[0].set_ylabel('RMSE (tonnes/ha)')\n",
    "axes[0].set_title('Root Mean Squared Error')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# MAE\n",
    "axes[1].bar(test_comparison['Model'], test_comparison['MAE'], color=['steelblue', 'coral', 'green'])\n",
    "axes[1].set_ylabel('MAE (tonnes/ha)')\n",
    "axes[1].set_title('Mean Absolute Error')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# R¬≤\n",
    "axes[2].bar(test_comparison['Model'], test_comparison['R2'], color=['steelblue', 'coral', 'green'])\n",
    "axes[2].set_ylabel('R¬≤ Score')\n",
    "axes[2].set_title('R¬≤ Score (Higher is Better)')\n",
    "axes[2].axhline(y=0.8, color='red', linestyle='--', alpha=0.5, label='0.8 threshold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual vs Predicted for all models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Actual vs Predicted Yields (Test Set)', fontsize=16, fontweight='bold')\n",
    "\n",
    "models_data = [\n",
    "    ('FNN', y_fnn_test, y_fnn_pred_test),\n",
    "    ('LSTM', y_lstm_test, y_lstm_pred_test),\n",
    "    ('Hybrid', y_hybrid_test, y_hybrid_pred_test)\n",
    "]\n",
    "\n",
    "for idx, (name, y_true, y_pred) in enumerate(models_data):\n",
    "    axes[idx].scatter(y_true, y_pred, alpha=0.6, s=50)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_true.min(), y_pred.min())\n",
    "    max_val = max(y_true.max(), y_pred.max())\n",
    "    axes[idx].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "    \n",
    "    # Calculate R¬≤\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    axes[idx].set_xlabel('Actual Yield (tonnes/ha)')\n",
    "    axes[idx].set_ylabel('Predicted Yield (tonnes/ha)')\n",
    "    axes[idx].set_title(f'{name}\\nR¬≤ = {r2:.4f}')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory\n",
    "models_dir = Path('models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Saving models and artifacts...\")\n",
    "\n",
    "# Save models\n",
    "fnn_model.save(models_dir / 'fnn_model.keras')\n",
    "lstm_model.save(models_dir / 'lstm_model.keras')\n",
    "hybrid_model.save(models_dir / 'hybrid_model.keras')\n",
    "print(\"  ‚úì Models saved\")\n",
    "\n",
    "# Save scalers\n",
    "import joblib\n",
    "joblib.dump(fnn_scaler, models_dir / 'fnn_scaler.pkl')\n",
    "joblib.dump(lstm_scaler, models_dir / 'lstm_scaler.pkl')\n",
    "joblib.dump(temporal_scaler, models_dir / 'temporal_scaler.pkl')\n",
    "joblib.dump(static_scaler, models_dir / 'static_scaler.pkl')\n",
    "print(\"  ‚úì Scalers saved\")\n",
    "\n",
    "# Save encoders\n",
    "joblib.dump(le_crop, models_dir / 'crop_encoder.pkl')\n",
    "joblib.dump(le_zone, models_dir / 'zone_encoder.pkl')\n",
    "joblib.dump(le_state, models_dir / 'state_encoder.pkl')\n",
    "print(\"  ‚úì Encoders saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results comparison\n",
    "comparison.to_csv(models_dir / 'model_comparison.csv', index=False)\n",
    "print(\"  ‚úì Comparison results saved\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'data_splits': {\n",
    "        'train': '1990-2016',\n",
    "        'validation': '2017-2019',\n",
    "        'test': '2020-2023'\n",
    "    },\n",
    "    'models': {\n",
    "        'fnn': {\n",
    "            'architecture': 'Feedforward Neural Network',\n",
    "            'test_rmse': float(fnn_metrics_test['RMSE']),\n",
    "            'test_r2': float(fnn_metrics_test['R2']),\n",
    "            'n_features': len(fnn_feature_cols)\n",
    "        },\n",
    "        'lstm': {\n",
    "            'architecture': 'Long Short-Term Memory',\n",
    "            'test_rmse': float(lstm_metrics_test['RMSE']),\n",
    "            'test_r2': float(lstm_metrics_test['R2']),\n",
    "            'sequence_length': sequence_length,\n",
    "            'n_temporal_features': n_temp_feat\n",
    "        },\n",
    "        'hybrid': {\n",
    "            'architecture': 'Hybrid LSTM + Dense',\n",
    "            'test_rmse': float(hybrid_metrics_test['RMSE']),\n",
    "            'test_r2': float(hybrid_metrics_test['R2']),\n",
    "            'n_temporal_features': n_temp_feat,\n",
    "            'n_static_features': X_hybrid_static_train_scaled.shape[1]\n",
    "        }\n",
    "
    }
}

with open(models_dir / 'training_metadata.json', 'w') as f:
    json.dump(metadata, f, indent=2)

print(\"  ‚úì Metadata saved\")\nprint(f\"\n‚úì All artifacts saved to: {models_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL DEVELOPMENT COMPLETE - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nü§ñ MODELS TRAINED:\")\n",
    "print(\"  1. FNN (Feedforward Neural Network)\")\n",
    "print(\"     - Architecture: 3 hidden layers (128‚Üí64‚Üí32)\")\n",
    "print(\"     - Input: Annual aggregated features\")\n",
    "print(f\"     - Test RMSE: {fnn_metrics_test['RMSE']:.4f} tonnes/ha\")\n",
    "print(f\"     - Test R¬≤: {fnn_metrics_test['R2']:.4f}\")\n",
    "\n",
    "print(\"\\n  2. LSTM (Long Short-Term Memory)\")\n",
    "print(\"     - Architecture: 2 LSTM layers (128‚Üí64)\")\n",
    "print(\"     - Input: Monthly time-series (12 months)\")\n",
    "print(f\"     - Test RMSE: {lstm_metrics_test['RMSE']:.4f} tonnes/ha\")\n",
    "print(f\"     - Test R¬≤: {lstm_metrics_test['R2']:.4f}\")\n",
    "\n",
    "print(\"\\n  3. Hybrid (LSTM + Dense for Static)\")\n",
    "print(\"     - Architecture: LSTM branch + Dense branch\")\n",
    "print(\"     - Input: Monthly temporal + static features\")\n",
    "print(f\"     - Test RMSE: {hybrid_metrics_test['RMSE']:.4f} tonnes/ha\")\n",
    "print(f\"     - Test R¬≤: {hybrid_metrics_test['R2']:.4f}\")\n",
    "\n",
    "# Identify best model\n",
    "test_r2_scores = {\n",
    "    'FNN': fnn_metrics_test['R2'],\n",
    "    'LSTM': lstm_metrics_test['R2'],\n",
    "    'Hybrid': hybrid_metrics_test['R2']\n",
    "}\n",
    "best_model = max(test_r2_scores, key=test_r2_scores.get)\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model}\")\n",
    "print(f\"   R¬≤ Score: {test_r2_scores[best_model]:.4f}\")\n",
    "\n",
    "print(\"\\nüìä DATA SPLITS:\")\n",
    "print(\"  Train: 1990-2016 (27 years)\")\n",
    "print(\"  Validation: 2017-2019 (3 years)\")\n",
    "print(\"  Test: 2020-2023 (4 years)\")\n",
    "\n",
    "print(\"\\nüíæ SAVED ARTIFACTS:\")\n",
    "print(f\"  Location: {models_dir}\")\n",
    "print(\"  Files:\")\n",
    "print(\"    - fnn_model.keras\")\n",
    "print(\"    - lstm_model.keras\")\n",
    "print(\"    - hybrid_model.keras\")\n",
    "print(\"    - fnn_scaler.pkl, lstm_scaler.pkl\")\n",
    "print(\"    - temporal_scaler.pkl, static_scaler.pkl\")\n",
    "print(\"    - crop_encoder.pkl, zone_encoder.pkl, state_encoder.pkl\")\n",
    "print(\"    - model_comparison.csv\")\n",
    "print(\"    - training_metadata.json\")\n",
    "\n",
    "print(\"\\n‚úÖ NEXT STEPS:\")\n",
    "print(\"  1. Proceed to Phase 4: Model Validation & Interpretation\")\n",
    "print(\"  2. Analyze predictions by crop type and geopolitical zone\")\n",
    "print(\"  3. Test climate change scenarios (temperature +2¬∞C, etc.)\")\n",
    "print(\"  4. Generate SHAP values for feature importance\")\n",
    "print(\"  5. Create interactive dashboards for predictions\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate-Food Security Modeling Pipeline\n",
    "## Phase 3: Model Development (FNN, LSTM, Hybrid)\n",
    "\n",
    "This notebook implements three deep learning architectures:\n",
    "1. **FNN (Feedforward Neural Network)** - Annual aggregated data\n",
    "2. **LSTM (Long Short-Term Memory)** - Monthly time-series data\n",
    "3. **Hybrid Model** - Combines temporal and static features\n",
    "\n",
    "**Splits:** Train (1990-2016), Validation (2017-2019), Test (2020-2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "data_path = Path('project_data')\n",
    "splits_path = data_path / 'train_test_split'\n",
    "\n",
    "print(\"Loading train/validation/test splits...\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: FNN (Feedforward Neural Network) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load FNN Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä LOADING FNN DATA...\")\n",
    "\n",
    "# Load splits\n",
    "fnn_train = pd.read_csv(splits_path / 'fnn' / 'train.csv')\n",
    "fnn_val = pd.read_csv(splits_path / 'fnn' / 'val.csv')\n",
    "fnn_test = pd.read_csv(splits_path / 'fnn' / 'test.csv')\n",
    "\n",
    "print(f\"  Train: {fnn_train.shape}\")\n",
    "print(f\"  Val:   {fnn_val.shape}\")\n",
    "print(f\"  Test:  {fnn_test.shape}\")\n",
    "\n",
    "# Remove rows with missing yields\n",
    "fnn_train = fnn_train.dropna(subset=['Yield_tonnes_per_ha'])\n",
    "fnn_val = fnn_val.dropna(subset=['Yield_tonnes_per_ha'])\n",
    "fnn_test = fnn_test.dropna(subset=['Yield_tonnes_per_ha'])\n",
    "\n",
    "print(f\"\\nAfter removing missing yields:\")\n",
    "print(f\"  Train: {fnn_train.shape}\")\n",
    "print(f\"  Val:   {fnn_val.shape}\")\n",
    "print(f\"  Test:  {fnn_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Prepare FNN Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns for FNN\n",
    "fnn_feature_cols = [\n",
    "    # Climate features (growing season aggregates)\n",
    "    'Avg_Temp_C', 'Min_Temp_C', 'Max_Temp_C', 'Temp_Range_C',\n",
    "    'Rainfall_mm', 'Rainy_Days', 'Max_Daily_Rainfall_mm', 'Rainfall_Intensity',\n",
    "    'Avg_Humidity_Percent', 'Min_Humidity_Percent', 'Max_Humidity_Percent',\n",
    "    'CO2_ppm', 'CO2_Growth_Rate_ppm_per_year',\n",
    "    \n",
    "    # Stress indicators\n",
    "    'Heat_Stress_Days', 'Cold_Stress_Days', 'Drought_Index', 'Flood_Risk_Index',\n",
    "    \n",
    "    # Soil properties\n",
    "    'Soil_pH', 'Organic_Matter_Percent', 'Nitrogen_ppm', 'Phosphorus_ppm', \n",
    "    'Potassium_ppm', 'Cation_Exchange_Capacity', 'Bulk_Density', \n",
    "    'Water_Holding_Capacity_Percent'\n",
    "]\n",
    "\n",
    "target_col = 'Yield_tonnes_per_ha'\n",
    "\n",
    "# Encode categorical variables\n",
    "le_crop = LabelEncoder()\n",
    "le_zone = LabelEncoder()\n",
    "le_state = LabelEncoder()\n",
    "\n",
    "# Fit on train, transform all\n",
    "fnn_train['Crop_encoded'] = le_crop.fit_transform(fnn_train['Crop'])\n",
    "fnn_val['Crop_encoded'] = le_crop.transform(fnn_val['Crop'])\n",
    "fnn_test['Crop_encoded'] = le_crop.transform(fnn_test['Crop'])\n",
    "\n",
    "fnn_train['Zone_encoded'] = le_zone.fit_transform(fnn_train['Geopolitical_Zone'])\n",
    "fnn_val['Zone_encoded'] = le_zone.transform(fnn_val['Geopolitical_Zone'])\n",
    "fnn_test['Zone_encoded'] = le_zone.transform(fnn_test['Geopolitical_Zone'])\n",
    "\n",
    "fnn_train['State_encoded'] = le_state.fit_transform(fnn_train['State'])\n",
    "fnn_val['State_encoded'] = le_state.transform(fnn_val['State'])\n",
    "fnn_test['State_encoded'] = le_state.transform(fnn_test['State'])\n",
    "\n",
    "# Add encoded features\n",
    "fnn_feature_cols.extend(['Crop_encoded', 'Zone_encoded', 'State_encoded'])\n",
    "\n",
    "print(f\"\\nTotal FNN features: {len(fnn_feature_cols)}\")\n",
    "print(f\"Feature columns: {fnn_feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "X_fnn_train = fnn_train[fnn_feature_cols].values\n",
    "y_fnn_train = fnn_train[target_col].values\n",
    "\n",
    "X_fnn_val = fnn_val[fnn_feature_cols].values\n",
    "y_fnn_val = fnn_val[target_col].values\n",
    "\n",
    "X_fnn_test = fnn_test[fnn_feature_cols].values\n",
    "y_fnn_test = fnn_test[target_col].values\n",
    "\n",
    "print(f\"\\nFNN arrays:\")\n",
    "print(f\"  X_train: {X_fnn_train.shape}, y_train: {y_fnn_train.shape}\")\n",
    "print(f\"  X_val:   {X_fnn_val.shape}, y_val:   {y_fnn_val.shape}\")\n",
    "print(f\"  X_test:  {X_fnn_test.shape}, y_test:  {y_fnn_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "fnn_scaler = StandardScaler()\n",
    "X_fnn_train_scaled = fnn_scaler.fit_transform(X_fnn_train)\n",
    "X_fnn_val_scaled = fnn_scaler.transform(X_fnn_val)\n",
    "X_fnn_test_scaled = fnn_scaler.transform(X_fnn_test)\n",
    "\n",
    "print(\"\\n‚úì Features scaled using StandardScaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Build FNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fnn_model(input_dim, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Build Feedforward Neural Network for crop yield prediction\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        \n",
    "        # Hidden layer 1\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Hidden layer 2\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # Hidden layer 3\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.1),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='mse',\n",
    "        metrics=['mae', tf.keras.metrics.RootMeanSquaredError(name='rmse')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "fnn_model = build_fnn_model(input_dim=X_fnn_train_scaled.shape[1])\n",
    "fnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Train FNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining FNN model...\")\n",
    "history_fnn = fnn_model.fit(\n",
    "    X_fnn_train_scaled, y_fnn_train,\n",
    "    validation_data=(X_fnn_val_scaled, y_fnn_val),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì FNN model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Evaluate FNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_fnn_pred_train = fnn_model.predict(X_fnn_train_scaled).flatten()\n",
    "y_fnn_pred_val = fnn_model.predict(X_fnn_val_scaled).flatten()\n",
    "y_fnn_pred_test = fnn_model.predict(X_fnn_test_scaled).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "def calculate_metrics(y_true, y_pred, set_name):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "    \n",
    "    print(f\"\\n{set_name} Set Performance:\")\n",
    "    print(f\"  RMSE:  {rmse:.4f} tonnes/ha\")\n",
    "    print(f\"  MAE:   {mae:.4f} tonnes/ha\")\n",
    "    print(f\"  R¬≤:    {r2:.4f}\")\n",
    "    print(f\"  MAPE:  {mape:.2f}%\")\n",
    "    \n",
    "    return {'RMSE': rmse, 'MAE': mae, 'R2': r2, 'MAPE': mape}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FNN MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fnn_metrics_train = calculate_metrics(y_fnn_train, y_fnn_pred_train, \"Train\")\n",
    "fnn_metrics_val = calculate_metrics(y_fnn_val, y_fnn_pred_val, \"Validation\")\n",
    "fnn_metrics_test = calculate_metrics(y_fnn_test, y_fnn_pred_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('FNN Training History', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history_fnn.history['loss'], label='Train Loss')\n",
    "axes[0].plot(history_fnn.history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].set_title('Model Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "axes[1].plot(history_fnn.history['mae'], label='Train MAE')\n",
    "axes[1].plot(history_fnn.history['val_mae'], label='Val MAE')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('Mean Absolute Error')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: LSTM (Long Short-Term Memory) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load LSTM Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä LOADING LSTM DATA...\")\n",
    "\n",
    "# Load splits\n",
    "lstm_train = pd.read_csv(splits_path / 'lstm' / 'train.csv')\n",
    "lstm_val = pd.read_csv(splits_path / 'lstm' / 'val.csv')\n",
    "lstm_test = pd.read_csv(splits_path / 'lstm' / 'test.csv')\n",
    "\n",
    "print(f\"  Train: {lstm_train.shape}\")\n",
    "print(f\"  Val:   {lstm_val.shape}\")\n",
    "print(f\"  Test:  {lstm_test.shape}\")\n",
    "\n",
    "# Remove rows with missing yields\n",
    "lstm_train = lstm_train.dropna(subset=['Yield_tonnes_per_ha'])\n",
    "lstm_val = lstm_val.dropna(subset=['Yield_tonnes_per_ha'])\n",
    "lstm_test = lstm_test.dropna(subset=['Yield_tonnes_per_ha'])\n",
    "\n",
    "print(f\"\\nAfter removing missing yields:\")\n",
    "print(f\"  Train: {lstm_train.shape}\")\n",
    "print(f\"  Val:   {lstm_val.shape}\")\n",
    "print(f\"  Test:  {lstm_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Prepare LSTM Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM feature columns (temporal features only)\n",
    "lstm_feature_cols = [\n",
    "    'Avg_Temp_C', 'Min_Temp_C', 'Max_Temp_C',\n",
    "    'Rainfall_mm', 'Rainy_Days', 'Rainfall_Intensity',\n",
    "    'Avg_Humidity_Percent',\n",
    "    'CO2_ppm',\n",
    "    'Heat_Stress_Days', 'Drought_Index', 'Flood_Risk_Index',\n",
    "    'Is_Growing_Season'\n",
    "]\n",
    "\n",
    "# Add static features as encodings\n",
    "lstm_train['Crop_encoded'] = le_crop.transform(lstm_train['Crop'])\n",
    "lstm_val['Crop_encoded'] = le_crop.transform(lstm_val['Crop'])\n",
    "lstm_test['Crop_encoded'] = le_crop.transform(lstm_test['Crop'])\n",
    "\n",
    "lstm_feature_cols.append('Crop_encoded')\n",
    "\n",
    "print(f\"\\nLSTM features: {len(lstm_feature_cols)}\")\n",
    "print(f\"Feature columns: {lstm_feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, feature_cols, target_col, sequence_length=12):\n",
    "    \"\"\"\n",
    "    Create sequences for LSTM\n",
    "    Each sequence contains 'sequence_length' months of data\n",
    "    \"\"\"\n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "    \n",
    "    # Group by Year, State, Crop\n",
    "    grouped = data.groupby(['Year', 'State', 'Crop'])\n",
    "    \n",
    "    for name, group in grouped:\n",
    "        group = group.sort_values('Month')\n",
    "        \n",
    "        if len(group) >= sequence_length:\n",
    "            # Get features and target\n",
    "            features = group[feature_cols].values\n",
    "            target = group[target_col].iloc[0]  # Yield is same for all months\n",
    "            \n",
    "            # Create sequence (use first 'sequence_length' months)\n",
    "            X_sequences.append(features[:sequence_length])\n",
    "            y_sequences.append(target)\n",
    "    \n",
    "    return np.array(X_sequences), np.array(y_sequences)\n",
    "\n",
    "# Create sequences\n",
    "print(\"\\nCreating LSTM sequences (12 months)...\")\n",
    "sequence_length = 12\n",
    "\n",
    "X_lstm_train, y_lstm_train = create_sequences(lstm_train, lstm_feature_cols, target_col, sequence_length)\n",
    "X_lstm_val, y_lstm_val = create_sequences(lstm_val, lstm_feature_cols, target_col, sequence_length)\n",
    "X_lstm_test, y_lstm_test = create_sequences(lstm_test, lstm_feature_cols, target_col, sequence_length)\n",
    "\n",
    "print(f\"\\nLSTM sequences:\")\n",
    "print(f\"  X_train: {X_lstm_train.shape}, y_train: {y_lstm_train.shape}\")\n",
    "print(f\"  X_val:   {X_lstm_val.shape}, y_val:   {y_lstm_val.shape}\")\n",
    "print(f\"  X_test:  {X_lstm_test.shape}, y_test:  {y_lstm_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale LSTM features\n",
    "lstm_scaler = StandardScaler()\n",
    "\n",
    "# Reshape for scaling\n",
    "n_train, seq_len, n_features = X_lstm_train.shape\n",
    "X_lstm_train_reshaped = X_lstm_train.reshape(-1, n_features)\n",
    "X_lstm_train_scaled = lstm_scaler.fit_transform(X_lstm_train_reshaped).reshape(n_train, seq_len, n_features)\n",
    "\n",
    "n_val = X_lstm_val.shape[0]\n",
    "X_lstm_val_reshaped = X_lstm_val.reshape(-1, n_features)\n",
    "X_lstm_val_scaled = lstm_scaler.transform(X_lstm_val_reshaped).reshape(n_val, seq_len, n_features)\n",
    "\n",
    "n_test = X_lstm_test.shape[0]\n",
    "X_lstm_test_reshaped = X_lstm_test.reshape(-1, n_features)\n",
    "X_lstm_test_scaled = lstm_scaler.transform(X_lstm_test_reshaped).reshape(n_test, seq_len, n_features)\n",
    "\n",
    "print(\"\\n‚úì LSTM sequences scaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Build LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(sequence_length, n_features, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Build LSTM model for time-series crop yield prediction\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(sequence_length, n_features)),\n",
    "        \n",
    "        # LSTM layers\n",
    "        layers.LSTM(128, return_sequences=True),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.LSTM(64, return_sequences=False),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # Dense layers\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.1),\n",
    "        \n",
    "        # Output\n",
    "        layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='mse',\n",
    "        metrics=['mae', tf.keras.metrics.RootMeanSquaredError(name='rmse')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build LSTM model\n",
    "lstm_model = build_lstm_model(sequence_length=sequence_length, n_features=n_features)\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stop_lstm = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=30,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr_lstm = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=15,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train LSTM model\n",
    "print(\"\\nTraining LSTM model...\")\n",
    "history_lstm = lstm_model.fit(\n",
    "    X_lstm_train_scaled, y_lstm_train,\n",
    "    validation_data=(X_lstm_val_scaled, y_lstm_val),
    epochs=200,
    batch_size=32,
    callbacks=[early_stop_lstm, reduce_lr_lstm],
    verbose=1
)

print("\n‚úì LSTM model training complete!")